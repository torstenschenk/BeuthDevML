{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pixel CNN \n",
    "\n",
    "Alternative approach to Pixel RNN from [Pixel Recurrent Neural Networks](https://arxiv.org/pdf/1601.06759.pdf). \n",
    "You can find many online resources to this topic - this notebook is based on the following resources:\n",
    " * See for an existing PyTorch implementation https://github.com/jzbontar/pixelcnn-pytorch/blob/master/main.py\n",
    " * http://sergeiturukin.com/2017/02/22/pixelcnn.html for a nice walk-through\n",
    " * http://tinyclouds.org/residency/\n",
    " * https://github.com/pilipolio/learn-pytorch\n",
    "\n",
    "The core idea of this [autoregressive model](https://en.wikipedia.org/wiki/Autoregressive_model) is the following:\n",
    "\n",
    "### Joint distribution of an image $\\mathbf{x}$ modelled as an autoregressive process\n",
    "\n",
    "Same model for PixelRNN and PixelCNN:\n",
    "\n",
    "$$p(\\mathbf{x}) = \\prod_{i=1}^{n^2} p(x_i|x_{1}, \\dots, x_{i-1})$$\n",
    " \n",
    "![](http://sergeiturukin.com/assets/2017-02-22-183010_479x494_scrot.png)\n",
    "\n",
    "That means you model the dependencies so that the first pixel is independent, the second depends on first, third depends on first and second and so on. The image is modelled as a sequence of points comparable to a time series where each point depends linearly on previous ones.\n",
    "\n",
    "The consequence of this idea is that the generation is performed sequentially: one generates the first pixel with one forward pass, then generates the second one given the first and continue the process until you have whole image generated. That is in contrast with convolutional neural network where the whole process is sort of parallel (kernel working on an image).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def show_as_image(binary_image, figsize=(10, 5)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(binary_image, cmap='gray')\n",
    "    plt.xticks([]); plt.yticks([])\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, utils\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal or masked convolutions\n",
    "\n",
    "Masks in a kernel are a way to restrict the information flow from 'future' pixels into the one we’re predicting. \n",
    "One way  is to use masked convolutions: all we need is just zero out some weights in convolution filters, like illustrated in the image. One can see, that information from pixels below won’t reach the target (center) pixel as well as from pixels on the same line to the right of target.\n",
    "\n",
    "![](http://sergeiturukin.com/assets/filter_mask5x5.png)\n",
    "\n",
    "There are two different modes of masking:\n",
    "   * Type B: includes the center pixel\n",
    "   * Type A: excludes the center pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASUAAAElCAYAAACiZ/R3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAABDFJREFUeJzt17FtAkEURdEdixJw7C3C/VfAFkFOD+PUjliQVlzhc+IX/OhqZsw5F4CKj1cfAPCbKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmnR8bn83mu63rQKcA727btNuf8vLd7KErrui6Xy+X5q4B/a4xx3bPzfQNSRAlIESUgRZSAFFECUkQJSBElIEWUgBRRAlJECUgRJSBFlIAUUQJSRAlIESUgRZSAFFECUkQJSBElIEWUgBRRAlJECUgRJSBFlIAUUQJSRAlIESUgRZSAFFECUkQJSBElIEWUgBRRAlJECUgRJSBFlIAUUQJSRAlIESUgRZSAFFECUkQJSBElIEWUgBRRAlJECUgRJSBFlIAUUQJSRAlIESUgRZSAFFECUkQJSBElIEWUgBRRAlJECUgRJSBFlIAUUQJSRAlIESUgRZSAFFECUkQJSBElIEWUgBRRAlJECUgRJSBFlIAUUQJSRAlIESUgRZSAFFECUkQJSBElIEWUgBRRAlJECUgRJSBFlIAUUQJSRAlIESUgRZSAFFECUkQJSBElIEWUgBRRAlJECUgRJSBFlIAUUQJSRAlIESUgRZSAFFECUkQJSBElIEWUgBRRAlJECUg5vfoAeMYY49UncBAvJSBFlIAUUQJSRAlIESUgRZSAFFECUkQJSBElIEWUgBRRAlJECUgRJSBFlIAUUQJSRAlIESUgRZSAFFECUkQJSBElIEWUgBRRAlJECUgRJSBFlIAUUQJSRAlIESUgRZSAFFECUkQJSBElIEWUgBRRAlJECUgRJSBFlIAUUQJSRAlIESUgRZSAFFECUkQJSBElIEWUgBRRAlJECUgRJSBFlIAUUQJSRAlIESUgZcw594/H2D8G+Gubc37fG3kpASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQMrpwf1tWZbrEYcAb+9rz2jMOY8+BGA33zcgRZSAFFECUkQJSBElIEWUgBRRAlJECUgRJSDlB+3gHd7ORa3KAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASUAAAElCAYAAACiZ/R3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAABDFJREFUeJzt17Ftw0AUBUGeoRLk2CzC/VcgFqHcPZxTOxJpgNBCnolf8KPF3ZhzLgAVb88+AOAnUQJSRAlIESUgRZSAFFECUkQJSBElIEWUgBRRAlIuR8bX63Wu63rSKcAr27bta875/mh3KErrui632+3vVwH/1hjjvmfn+wakiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpByefYBnGuM8ewT4BAvJSBFlIAUUQJSRAlIESUgRZSAFFECUkQJSBElIEWUgBRRAlJECUgRJSBFlIAUUQJSRAlIESUgRZSAFFECUkQJSBElIEWUgBRRAlJECUgRJSBFlIAUUQJSRAlIESUgRZSAFFECUkQJSBElIEWUgBRRAlJECUgRJSBFlIAUUQJSRAlIESUgRZSAFFECUkQJSBElIEWUgBRRAlJECUgRJSBFlIAUUQJSRAlIESUgZcw594/H2D8G+G2bc34+GnkpASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQIooASmiBKSIEpAiSkCKKAEpogSkiBKQIkpAiigBKaIEpIgSkCJKQMrl4P5rWZb7GYcAL+9jz2jMOc8+BGA33zcgRZSAFFECUkQJSBElIEWUgBRRAlJECUgRJSDlG/BoHd7iaQCCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def causal_mask(width, height, starting_point):\n",
    "    row_grid, col_grid = np.meshgrid(np.arange(width), np.arange(height), indexing='ij')\n",
    "    mask = np.logical_or(\n",
    "        row_grid < starting_point[0],\n",
    "        np.logical_and(row_grid == starting_point[0], col_grid <= starting_point[1]))\n",
    "    return mask\n",
    "\n",
    "def conv_mask(width, height, include_center=False):\n",
    "    return 1.0 * causal_mask(width, height, starting_point=(width//2, height//2 + include_center - 1))\n",
    "\n",
    "show_as_image(conv_mask(5, 5, include_center=True))\n",
    "\n",
    "show_as_image(conv_mask(5, 5, include_center=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-ing all inputs weights after center point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function conv_mask at 0x11b78e488>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[[ 1.,  2.,  3.],\n",
       "         [ 4.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]],\n",
       "\n",
       "        [[10., 11., 12.],\n",
       "         [13.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "       [[[19., 20., 21.],\n",
       "         [22.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]],\n",
       "\n",
       "        [[28., 29., 30.],\n",
       "         [31.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_channels, in_channels, width, height = 2, 2, 3, 3\n",
    "\n",
    "conv_weights = 1 + np.arange(out_channels * in_channels * width * height).reshape((out_channels, in_channels, width, height))\n",
    "\n",
    "print(conv_mask)\n",
    "\n",
    "masked_weights = conv_weights * conv_mask(width, height, False)\n",
    "\n",
    "masked_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedConv2d(nn.Conv2d):\n",
    "    def __init__(self, mask_type, *args, **kwargs):\n",
    "        super(MaskedConv2d, self).__init__(*args, **kwargs)\n",
    "        _, n_channels, width, height = self.weight.size()\n",
    "\n",
    "        mask = conv_mask(width, height, include_center=mask_type=='B')\n",
    "        self.register_buffer('mask', torch.from_numpy(mask).float())\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.weight.data *= self.mask\n",
    "        return super(MaskedConv2d, self).forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully convolutional network preserving spatial resolution\n",
    "\n",
    "Input to output map      |  Output distribution\n",
    ":-------------------------:|:-------------------------:\n",
    "![](https://tensorflowkorea.files.wordpress.com/2016/11/pixel-cnn1.png)  |  ![](http://tinyclouds.org/residency/pixelcnn.png)\n",
    "\n",
    "Quite a counter-intuitive model:\n",
    "\n",
    " * Convolutional layers bottom to top!\n",
    " * Last layer with `kernel_size=1` and outputs $ n_W \\times n_H \\times n_{pixels}$ logits, inferring $p(\\mathbf{x})$ in one forward pass (during training)\n",
    " * Representation of dimension `n_channels` output by each layer anologous to RNN's internal state vector $\\mathbf{h}$\n",
    " * Necessary to stack enough layers (and/or dillatations) to augment the \"receptive field\" so that output pixels can be influenced by the whole image\n",
    " \n",
    "\n",
    "Below is a minimalistic implementation for 0/1 pixels without many of the bells and whistles of the original paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "    n_channels = 4\n",
    "    kernel_size = 7\n",
    "    padding = 3\n",
    "    n_pixels_out = 2 # binary 0/1 pixels\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(PixelCNN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            MaskedConv2d('A', in_channels=1, out_channels=self.n_channels, kernel_size=self.kernel_size, padding=self.padding, bias=False), nn.BatchNorm2d(self.n_channels), nn.ReLU(True),\n",
    "            MaskedConv2d('B', self.n_channels, self.n_channels, kernel_size=self.kernel_size, padding=self.padding, bias=False), nn.BatchNorm2d(self.n_channels), nn.ReLU(True),\n",
    "            MaskedConv2d('B', self.n_channels, self.n_channels, kernel_size=self.kernel_size, padding=self.padding, bias=False), nn.BatchNorm2d(self.n_channels), nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=self.n_channels, out_channels=self.n_pixels_out, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pixel_logits = self.layers(x)\n",
    "        return pixel_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application on a simple generative model of LCD digits\n",
    "\n",
    "From https://gist.github.com/benjaminwilson/b25a321f292f98d74269b83d4ed2b9a8#file-lcd-digits-dataset-nmf-ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALwAAAElCAYAAABeV4iUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAA2lJREFUeJzt3ctpQzEQQNE3wX2k/w7STvbpYVJCbPx5JvectRCzuAgtBJrdPaDi4+wB4JUET4rgSRE8KYInRfCkCJ4UwZMieFJuCn5mvp41CNzj2jbnlqcFM+MdAm9rd+evNa40pAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ+Vy9gC7+/A9Z+bhe/I/OOFJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgidF8KQInhTBkyJ4UgRPiuBJETwpgiflcvYAM3P2CIQ44UkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4EkRPCmCJ0XwpAieFMGTInhSBE+K4Em59Sfun+M4vp8xCNzp85pFs7vPHgTehisNKYInRfCkCJ4UwZMieFIET4rgSRE8Kb+l/RWqZUaedQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cell_length = 4\n",
    "image_width, image_height = 2 * cell_length + 5, cell_length + 4\n",
    "\n",
    "def vertical_stroke(rightness, downness):\n",
    "    \"\"\"\n",
    "    Return a 2d numpy array representing an image with a single vertical stroke in it.\n",
    "    `rightness` and `downness` are values from [0, 1] and define the position of the vertical stroke.\n",
    "    \"\"\"\n",
    "    i = (downness * (cell_length + 1)) + 2\n",
    "    j = rightness * (cell_length + 1) + 1\n",
    "    x = np.zeros(shape=(image_width, image_height), dtype=np.float64)\n",
    "    x[i + np.arange(cell_length), j] = 1.\n",
    "    return x\n",
    "\n",
    "def horizontal_stroke(downness):\n",
    "    \"\"\"\n",
    "    Analogue to vertical_stroke, but it returns horizontal strokes.\n",
    "    `downness` is here a value in [0, 1, 2].\n",
    "    \"\"\"\n",
    "    i = (downness * (cell_length + 1)) + 1\n",
    "    x = np.zeros(shape=(image_width, image_height), dtype=np.float64)\n",
    "    x[i, 2 + np.arange(cell_length)] = 1.\n",
    "    return x\n",
    "\n",
    "show_as_image(vertical_stroke(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output array for number '5':  [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BASE_STROKES = np.asarray(\n",
    "    [horizontal_stroke(k) for k in range(3)] + [vertical_stroke(k, l) for k in range(2) for l in range(2)])\n",
    "\n",
    "DIGITS_STROKES = np.array([[0, 2, 3, 4, 5, 6], [5, 6], [0, 1, 2, 4, 5], [0, 1, 2, 5, 6], [1, 3, 5, 6], [0, 1, 2, 3, 6], [0, 1, 2, 3, 4, 6], [0, 5, 6], np.arange(7), [0, 1, 2, 3, 5, 6]])\n",
    "\n",
    "def random_digits(strokes=BASE_STROKES, digit_as_strokes=DIGITS_STROKES, fixed_label=None):\n",
    "    label = fixed_label if fixed_label is not None else np.random.choice(len(digit_as_strokes))\n",
    "    combined_strokes = strokes[digit_as_strokes[label], :, :].sum(axis=0)\n",
    "    return combined_strokes, label\n",
    "\n",
    "def batch_images_to_one(batches_images):\n",
    "    n_square_elements = int(np.sqrt(batches_images.shape[0]))\n",
    "    rows_images = np.split(np.squeeze(batches_images), n_square_elements)\n",
    "    return np.vstack([np.hstack(row_images) for row_images in rows_images])\n",
    "\n",
    "print(\"output array for number '5': \",BASE_STROKES[DIGITS_STROKES[5], :, :].sum(axis=0))\n",
    "show_as_image(batch_images_to_one(np.stack([random_digits()[0] for _ in range(25)])) , figsize=(9, 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 1., 1., 1., 1., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 1., 1., 1., 1., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "           [0., 0., 1., 1., 1., 1., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "           [0., 0., 1., 1., 1., 1., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "           [0., 0., 1., 1., 1., 1., 0., 0.],\n",
       "           [0., 1., 0., 0., 0., 0., 1., 0.],\n",
       "           [0., 1., 0., 0., 0., 0., 1., 0.],\n",
       "           [0., 1., 0., 0., 0., 0., 1., 0.],\n",
       "           [0., 1., 0., 0., 0., 0., 1., 0.],\n",
       "           [0., 0., 1., 1., 1., 1., 0., 0.],\n",
       "           [0., 1., 0., 0., 0., 0., 1., 0.],\n",
       "           [0., 1., 0., 0., 0., 0., 1., 0.],\n",
       "           [0., 1., 0., 0., 0., 0., 1., 0.],\n",
       "           [0., 1., 0., 0., 0., 0., 1., 0.],\n",
       "           [0., 0., 1., 1., 1., 1., 0., 0.],\n",
       "           [0., 0., 0., 0., 0., 0., 0., 0.]]]]), tensor([[7],\n",
       "         [3],\n",
       "         [8]])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class LCDDigits(Dataset):\n",
    "\n",
    "    def __init__(self, n_examples):\n",
    "        digits, labels = zip(*[random_digits() for _ in range(n_examples)])\n",
    "        self.digits = np.asarray(digits, dtype=np.float64)\n",
    "        self.labels = np.asarray(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        digit_with_channel = self.digits[idx][np.newaxis, :, :]\n",
    "        \n",
    "        return torch.from_numpy(digit_with_channel).float(), torch.from_numpy(np.array([self.labels[idx]]))\n",
    "\n",
    "next(b for b in DataLoader(LCDDigits(128), batch_size=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the PixelCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Loss: 0.5472\n",
      "Epoch [2/25], Loss: 0.1990\n",
      "Epoch [3/25], Loss: 0.0892\n",
      "Epoch [4/25], Loss: 0.0523\n",
      "Epoch [5/25], Loss: 0.0337\n",
      "Epoch [6/25], Loss: 0.0298\n",
      "Epoch [7/25], Loss: 0.0284\n",
      "Epoch [8/25], Loss: 0.0275\n",
      "Epoch [9/25], Loss: 0.0250\n",
      "Epoch [10/25], Loss: 0.0268\n",
      "Epoch [11/25], Loss: 0.0243\n",
      "Epoch [12/25], Loss: 0.0255\n",
      "Epoch [13/25], Loss: 0.0250\n",
      "Epoch [14/25], Loss: 0.0247\n",
      "Epoch [15/25], Loss: 0.0233\n",
      "Epoch [16/25], Loss: 0.0231\n",
      "Epoch [17/25], Loss: 0.0234\n",
      "Epoch [18/25], Loss: 0.0226\n",
      "Epoch [19/25], Loss: 0.0222\n",
      "Epoch [20/25], Loss: 0.0225\n",
      "Epoch [21/25], Loss: 0.0220\n",
      "Epoch [22/25], Loss: 0.0219\n",
      "Epoch [23/25], Loss: 0.0211\n",
      "Epoch [24/25], Loss: 0.0213\n",
      "Epoch [25/25], Loss: 0.0209\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "num_epochs = 25\n",
    "batch_size = 10\n",
    "learning_rate = 0.005\n",
    "\n",
    "cnn = PixelCNN()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate )\n",
    "\n",
    "train_dataset = LCDDigits(batch_size * 50)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, _) in enumerate(train_loader):\n",
    "        images = Variable(images)\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.cross_entropy(input=cnn(images), target=torch.squeeze(images).long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequentially generating new samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_samples(n_samples, starting_image=None):\n",
    "\n",
    "    samples = torch.from_numpy(\n",
    "        starting_image if starting_image is not None else np.zeros((n_samples * n_samples, 1, image_width , image_height))).float()\n",
    "    # set training to false\n",
    "    cnn.train(False)\n",
    "    \n",
    "    # go over each pixel and predict the next one based on the previous\n",
    "    for i in range(image_width):\n",
    "        for j in range(image_height):\n",
    "            with torch.no_grad():\n",
    "                out = cnn(Variable(samples))\n",
    "            probs = F.softmax(out[:, :, i, j], dim=1).data\n",
    "#             print(out[:, :, i, j].shape, i, j, torch.multinomial(probs, 1).float())\n",
    "            samples[:, :, i, j] = torch.multinomial(probs, 1).float()\n",
    "    \n",
    "    return samples.numpy()\n",
    "\n",
    "samples = generate_samples(n_samples=2)\n",
    "show_as_image(batch_images_to_one(samples), figsize=(10, 10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
