{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source code inspireed by\n",
    "# https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html#model-training-and-validation-code\n",
    "\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# show plot first run\n",
    "%matplotlib inline \n",
    "# test autocompletion with tab or tab+shift\n",
    "%config IPCompleter.greedy=True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vgg26 Inspiration\n",
    "<img src=\"vgg16-1-e1542731207177.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.3.1\n",
      "Torchvision Version:  0.4.2\n",
      "cuda_available: True\n",
      "Loading dataset\n",
      "Datasets loaded\n",
      "MyNeuralNetwork(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=5184, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (fc3): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('cuda_available:',use_cuda)\n",
    "\n",
    "root = './data'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    # you can add other transformations in this list\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "print('Loading dataset')\n",
    "train_set = datasets.FashionMNIST(root=root, train=True, transform=transform, download=True)\n",
    "test_set = datasets.FashionMNIST(root=root, train=False, transform=transform, download=True)\n",
    "print('Datasets loaded')\n",
    "\n",
    "# hyperparameter\n",
    "# TODO Find good hyperparameters\n",
    "batch_size = 4\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "\n",
    "# Load train and test data as dictionary!!!\n",
    "data_loaders = {}\n",
    "data_loaders['train'] = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "data_loaders['test'] = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)\n",
    "\n",
    "\n",
    "# implement your own NNs\n",
    "\n",
    "class MyNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNeuralNetwork, self).__init__()\n",
    "        # TODO YOUR CODE HERE\n",
    "        #https://neurohive.io/en/popular-networks/vgg16/\n",
    "        #input size=1x28x28\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5,stride=1)\n",
    "        #conv1: 28x28x1 -> 24x24x64\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5,stride=1)\n",
    "        self.drop = nn.Dropout(0.25)\n",
    "        #conv2: 24x24x16 -> 20x20x32\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3,stride=1)\n",
    "        #conv3: 20x20x64 -> 18x18x64\n",
    "        #pooling: 18x18x64 -> 9x9x64\n",
    "        self.fc1   = nn.Linear(9*9*64, 2048)\n",
    "        self.fc2   = nn.Linear(2048, 512)\n",
    "        self.fc3   = nn.Linear(512, 10)\n",
    "    def forward(self, x):\n",
    "        # TODO YOUR CODE HERE\n",
    "        conv1_relu = F.relu(self.conv1(x))                          #conv1: 28x28x1 -> 24x24x64\n",
    "        x = self.drop(x)\n",
    "        conv2_relu = F.relu(self.conv2(conv1_relu))                 #conv2: 24x24x64 -> 20x20x128\n",
    "        conv3_relu = F.max_pool2d(F.relu(self.conv3(conv2_relu)),2) #conv3: 20x20x128 -> 16x16x256\n",
    "        x = self.drop(x)                                            #pooling: 16x16x256 -> 8x8x256  \n",
    "        x = conv3_relu.view(-1, 9*9*64)                             # fc 8*8*256, 4096\n",
    "        x = F.relu(self.fc1(x))                                     # fc 4096, 1000\n",
    "        x = F.relu(self.fc2(x))                                     # fc 1000, 10\n",
    "        x = F.softmax(self.fc3(x),dim=1)                                             # fc 10 out\n",
    "        return x\n",
    "    \n",
    "    def name(self):\n",
    "        return \"MyNeuralNetwork\"\n",
    "\n",
    "## training\n",
    "model = MyNeuralNetwork()\n",
    "print(model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read nn model\n",
    "#PATH = './bak-'+ model.name() +'.pth'\n",
    "#model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4\n",
      "----------\n",
      "train Batch: 0 of 15000\n",
      "train Batch: 500 of 15000\n",
      "train Batch: 1000 of 15000\n",
      "train Batch: 1500 of 15000\n",
      "train Batch: 2000 of 15000\n",
      "train Batch: 2500 of 15000\n",
      "train Batch: 3000 of 15000\n",
      "train Batch: 3500 of 15000\n",
      "train Batch: 4000 of 15000\n",
      "train Batch: 4500 of 15000\n",
      "train Batch: 5000 of 15000\n",
      "train Batch: 5500 of 15000\n",
      "train Batch: 6000 of 15000\n",
      "train Batch: 6500 of 15000\n",
      "train Batch: 7000 of 15000\n",
      "train Batch: 7500 of 15000\n",
      "train Batch: 8000 of 15000\n",
      "train Batch: 8500 of 15000\n",
      "train Batch: 9000 of 15000\n",
      "train Batch: 9500 of 15000\n",
      "train Batch: 10000 of 15000\n",
      "train Batch: 10500 of 15000\n",
      "train Batch: 11000 of 15000\n",
      "train Batch: 11500 of 15000\n",
      "train Batch: 12000 of 15000\n",
      "train Batch: 12500 of 15000\n",
      "train Batch: 13000 of 15000\n",
      "train Batch: 13500 of 15000\n",
      "train Batch: 14000 of 15000\n",
      "train Batch: 14500 of 15000\n",
      "train Loss: 2.0231 Acc: 0.4354\n",
      "test Batch: 0 of 2500\n",
      "test Batch: 500 of 2500\n",
      "test Batch: 1000 of 2500\n",
      "test Batch: 1500 of 2500\n",
      "test Batch: 2000 of 2500\n",
      "test Loss: 1.7660 Acc: 0.6945\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Batch: 0 of 15000\n",
      "train Batch: 500 of 15000\n",
      "train Batch: 1000 of 15000\n",
      "train Batch: 1500 of 15000\n",
      "train Batch: 2000 of 15000\n",
      "train Batch: 2500 of 15000\n",
      "train Batch: 3000 of 15000\n",
      "train Batch: 3500 of 15000\n",
      "train Batch: 4000 of 15000\n",
      "train Batch: 4500 of 15000\n",
      "train Batch: 5000 of 15000\n",
      "train Batch: 5500 of 15000\n",
      "train Batch: 6000 of 15000\n",
      "train Batch: 6500 of 15000\n",
      "train Batch: 7000 of 15000\n",
      "train Batch: 7500 of 15000\n",
      "train Batch: 8000 of 15000\n",
      "train Batch: 8500 of 15000\n",
      "train Batch: 9000 of 15000\n",
      "train Batch: 9500 of 15000\n",
      "train Batch: 10000 of 15000\n",
      "train Batch: 10500 of 15000\n",
      "train Batch: 11000 of 15000\n",
      "train Batch: 11500 of 15000\n",
      "train Batch: 12000 of 15000\n",
      "train Batch: 12500 of 15000\n",
      "train Batch: 13000 of 15000\n",
      "train Batch: 13500 of 15000\n",
      "train Batch: 14000 of 15000\n",
      "train Batch: 14500 of 15000\n",
      "train Loss: 1.7177 Acc: 0.7440\n",
      "test Batch: 0 of 2500\n",
      "test Batch: 500 of 2500\n",
      "test Batch: 1000 of 2500\n",
      "test Batch: 1500 of 2500\n",
      "test Batch: 2000 of 2500\n",
      "test Loss: 1.6958 Acc: 0.7650\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Batch: 0 of 15000\n",
      "train Batch: 500 of 15000\n",
      "train Batch: 1000 of 15000\n",
      "train Batch: 1500 of 15000\n",
      "train Batch: 2000 of 15000\n",
      "train Batch: 2500 of 15000\n",
      "train Batch: 3000 of 15000\n",
      "train Batch: 3500 of 15000\n",
      "train Batch: 4000 of 15000\n",
      "train Batch: 4500 of 15000\n",
      "train Batch: 5000 of 15000\n",
      "train Batch: 5500 of 15000\n",
      "train Batch: 6000 of 15000\n",
      "train Batch: 6500 of 15000\n",
      "train Batch: 7000 of 15000\n",
      "train Batch: 7500 of 15000\n",
      "train Batch: 8000 of 15000\n",
      "train Batch: 8500 of 15000\n",
      "train Batch: 9000 of 15000\n",
      "train Batch: 9500 of 15000\n",
      "train Batch: 10000 of 15000\n",
      "train Batch: 10500 of 15000\n",
      "train Batch: 11000 of 15000\n",
      "train Batch: 11500 of 15000\n",
      "train Batch: 12000 of 15000\n",
      "train Batch: 12500 of 15000\n",
      "train Batch: 13000 of 15000\n",
      "train Batch: 13500 of 15000\n",
      "train Batch: 14000 of 15000\n",
      "train Batch: 14500 of 15000\n",
      "train Loss: 1.6685 Acc: 0.7933\n",
      "test Batch: 0 of 2500\n",
      "test Batch: 500 of 2500\n",
      "test Batch: 1000 of 2500\n",
      "test Batch: 1500 of 2500\n",
      "test Batch: 2000 of 2500\n",
      "test Loss: 1.6586 Acc: 0.8043\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Batch: 0 of 15000\n",
      "train Batch: 500 of 15000\n",
      "train Batch: 1000 of 15000\n",
      "train Batch: 1500 of 15000\n",
      "train Batch: 2000 of 15000\n",
      "train Batch: 2500 of 15000\n"
     ]
    }
   ],
   "source": [
    "#optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_acc_history = []\n",
    "test_acc_history = []\n",
    "\n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "\n",
    "\n",
    "best_acc = 0.0\n",
    "since = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "    print('-' * 10)\n",
    "\n",
    "    # Each epoch has a training and validation phase\n",
    "    for phase in ['train', 'test']:\n",
    "        if phase == 'train':\n",
    "            model.train()  # Set model to training mode\n",
    "        else:\n",
    "            model.eval()  # Set model to evaluate mode\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(data_loaders[phase]):\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #print(labels.shape)\n",
    "\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = model(inputs)\n",
    "                #print('in',inputs.shape)\n",
    "                #print('out',outputs.shape)\n",
    "                #print('labels',labels.shape)\n",
    "                outputs.reshape(-1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if batch_idx % 500 == 0:\n",
    "                print('{} Batch: {} of {}'.format(phase, batch_idx, len(data_loaders[phase])))\n",
    "                \n",
    "            #if batch_idx == 2000:\n",
    "            #    break\n",
    "                \n",
    "\n",
    "        epoch_loss = running_loss / len(data_loaders[phase].dataset)\n",
    "        epoch_acc = running_corrects.double() / len(data_loaders[phase].dataset)\n",
    "\n",
    "        print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "        # deep copy the model\n",
    "        if phase == 'test' and epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        if phase == 'test':\n",
    "            test_acc_history.append(epoch_acc)\n",
    "            test_loss_history.append(epoch_loss)\n",
    "        if phase == 'train':\n",
    "            train_acc_history.append(epoch_acc)\n",
    "            train_loss_history.append(epoch_loss)\n",
    "\n",
    "    #print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "#print('Saving model',PATH)\n",
    "#torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_elapsed = time.time() - since\n",
    "print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "acc_train_hist = []\n",
    "acc_test_hist = []\n",
    "\n",
    "acc_train_hist = [h.cpu().numpy() for h in train_acc_history]\n",
    "acc_test_hist = [h.cpu().numpy() for h in test_acc_history]\n",
    "\n",
    "plt.title(\"Validation/Test Accuracy vs. Number of Training Epochs\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation/Test Accuracy\")\n",
    "plt.plot(range(1,num_epochs+1),acc_train_hist,label=\"Train\")\n",
    "plt.plot(range(1,num_epochs+1),acc_test_hist,label=\"Test\")\n",
    "plt.ylim((0,1.))\n",
    "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.savefig('ex3_3_acc_vgg16.png')\n",
    "plt.show()\n",
    "\n",
    "#print(np.arange(1,num_epochs+1),train_loss_history)\n",
    "#print(np.arange(1,num_epochs+1),test_loss_history)\n",
    "\n",
    "plt.title(\"Validation/Test Loss vs. Number of Training Epochs\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation/Test Loss\")\n",
    "plt.plot(range(1,num_epochs+1),train_loss_history,label=\"Train\")\n",
    "plt.plot(range(1,num_epochs+1),test_loss_history,label=\"Test\")\n",
    "plt.ylim((0,2.3))\n",
    "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.savefig('ex3_3_loss_vgg16.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "examples = enumerate(data_loaders['test'])\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "with torch.no_grad():\n",
    "    output = model(example_data)\n",
    "\n",
    "categories = {\n",
    "    0:\t'T-shirt/top',\n",
    "    1:\t'Trouser',\n",
    "    2:\t'Pullover',\n",
    "    3:\t'Dress',\n",
    "    4:\t'Coat',\n",
    "    5:\t'Sandal',\n",
    "    6:\t'Shirt',\n",
    "    7:\t'Sneaker',\n",
    "    8:\t'Bag',\n",
    "    9:\t'Ankle boot'\n",
    "}\n",
    "\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Pred: {}\".format(\n",
    "      categories[output.data.max(1, keepdim=True)[1][i].item()]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
