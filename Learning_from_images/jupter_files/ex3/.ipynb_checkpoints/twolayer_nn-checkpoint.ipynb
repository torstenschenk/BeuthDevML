{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "nn_img_size = 32\n",
    "num_classes = 3\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 500\n",
    "batch_size = 4\n",
    "\n",
    "loss_mode = 'crossentropy' \n",
    "\n",
    "loss_train_hist = []\n",
    "\n",
    "##################################################\n",
    "## Please implement a two layer neural network  ##\n",
    "##################################################\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU activation function\"\"\"\n",
    "    return torch.clamp(x, min=0.0)\n",
    "\n",
    "def relu_derivative(output):\n",
    "    \"\"\"derivative of the ReLU activation function\"\"\"\n",
    "    output[output <= 0] = 0\n",
    "    output[output>0] = 1\n",
    "    return output\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"softmax function to transform values to probabilities\"\"\"\n",
    "    z -= z.max()\n",
    "    z = torch.exp(z)\n",
    "    sum_z = z.sum(1, keepdim=True)\n",
    "    return z / sum_z \n",
    "\n",
    "def loss_mse(activation, y_batch):\n",
    "    \"\"\"mean squared loss function\"\"\"\n",
    "    # use MSE error as loss function \n",
    "    # Hint: the computed error needs to get normalized over the number of samples\n",
    "    loss = (activation - y_batch).pow(2).sum() \n",
    "    mse = 1.0 / activation.shape[0] * loss\n",
    "    return mse\n",
    "\n",
    "def loss_crossentropy(activation, y_batch):\n",
    "    \"\"\"cross entropy loss function\"\"\"\n",
    "    batch_size = y_batch.shape[0]\n",
    "    loss = ( - y_batch * activation.log()).sum() / batch_size\n",
    "    return loss\n",
    "\n",
    "def loss_deriv_mse(activation, y_batch):\n",
    "    \"\"\"derivative of the mean squared loss function\"\"\"\n",
    "    dCda2 = (1 / activation.shape[0]) * (activation - y_batch)\n",
    "    return dCda2\n",
    "\n",
    "def loss_deriv_crossentropy(activation, y_batch):\n",
    "    \"\"\"derivative of the mean cross entropy loss function\"\"\"\n",
    "    batch_size = y_batch.shape[0]\n",
    "    dCda2 = activation\n",
    "    dCda2[range(batch_size), np.argmax(y_batch, axis=1)] -= 1\n",
    "    dCda2 /= batch_size\n",
    "    return dCda2\n",
    "\n",
    "def setup_train():\n",
    "    \"\"\"train function\"\"\"\n",
    "    # load and resize train images in three categories\n",
    "    # cars = 0, flowers = 1, faces = 2 ( true_ids )\n",
    "    train_images_cars = glob.glob('./images/db/train/cars/*.jpg')\n",
    "    train_images_flowers = glob.glob('./images/db/train/flowers/*.jpg')\n",
    "    train_images_faces = glob.glob('./images/db/train/faces/*.jpg')\n",
    "    train_images = [train_images_cars, train_images_flowers, train_images_faces]\n",
    "    num_rows = len(train_images_cars)+len(train_images_flowers) +len(train_images_faces)\n",
    "    X_train = torch.zeros((num_rows, nn_img_size*nn_img_size))\n",
    "    y_train = torch.zeros((num_rows, num_classes))\n",
    "\n",
    "    counter = 0\n",
    "    for (label, fnames) in enumerate(train_images):\n",
    "        for fname in fnames:\n",
    "            print(label, fname)\n",
    "            img = cv2.imread(fname, cv2.IMREAD_GRAYSCALE)\n",
    "            img = cv2.resize(img, (nn_img_size, nn_img_size) , interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            # print( label, \" -- \", fname, img.shape)\n",
    "\n",
    "            # fill matrices X_train - each row is an image vector\n",
    "            # y_train - one-hot encoded, put only a 1 where the label is correct for the row in X_train\n",
    "            y_train[counter, label] = 1\n",
    "            X_train[counter] = torch.from_numpy(img.flatten().astype(np.float32))\n",
    "            \n",
    "            counter += 1\n",
    "\n",
    "    # print(y_train)\n",
    "    return X_train, y_train\n",
    "\n",
    "def forward(X_batch, y_batch, W1, W2, b1, b2):\n",
    "    \"\"\"forward pass in the neural network \"\"\"\n",
    "    ### YOUR CODE ####\n",
    "    # please implement the forward pass\n",
    "    # \n",
    "    \n",
    "    # the function should return the loss and both intermediate activations\n",
    "    #return loss, a2, a1\n",
    "\n",
    "def backward(a2, a1, X_batch, y_batch, W2):\n",
    "    \"\"\"backward pass in the neural network \"\"\"\n",
    "    # Implement the backward pass by computing\n",
    "    # the derivative of the complete function\n",
    "    # using the chain rule as discussed in the lecture\n",
    "\n",
    "    # please use the appropriate loss functions \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # function should return 4 derivatives with respect to\n",
    "    # W1, W2, b1, b2\n",
    "    # return dCdW1, dCdW2, dCdb1, dCdb2\n",
    "\n",
    "def train(X_train, y_train):\n",
    "    \"\"\" train procedure \"\"\"\n",
    "    # for simplicity of this execise you don't need to find useful hyperparameter\n",
    "    # I've done this for you already and every test image should work for the\n",
    "    # given very small trainings database and the following parameters.\n",
    "    h = 1500\n",
    "    std = 0.001\n",
    "    # YOUR CODE HERE\n",
    "    # initialize W1, W2, b1, b2 randomly\n",
    "    # Note: W1, W2 should be scaled by variable std\n",
    "    \n",
    "    # run for num_epochs\n",
    "    for i in range(num_epochs):\n",
    "\n",
    "        X_batch = None\n",
    "        y_batch = None\n",
    "\n",
    "        # use only a batch of batch_size of the training images in each run\n",
    "        # sample the batch images randomly from the training set\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # forward pass for two-layer neural network using ReLU as activation function\n",
    "        \n",
    "        # add loss to loss_train_hist for plotting\n",
    "        \n",
    "        #if i % 10 == 0:\n",
    "        #    print(\"iteration %d: loss %f\" % (i, loss))\n",
    "\n",
    "        # backward pass \n",
    "        \n",
    "        # print(\"dCdb2.shape:\", dCdb2.shape, dCdb1.shape)\n",
    "\n",
    "        # depending on the derivatives of W1, and W2 regaring the cost/loss\n",
    "        # we need to adapt the values in the negative direction of the \n",
    "        # gradient decreasing towards the minimum\n",
    "        # we weight the gradient by a learning rate\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "    # return W1, W2, b1, b2\n",
    "\n",
    "X_train, y_train = setup_train()\n",
    "W1, W2, b1, b2 = train(X_train, y_train)\n",
    "\n",
    "# predict the test images, load all test images and \n",
    "# run prediction by computing the forward pass\n",
    "test_images = []\n",
    "test_images.append( (cv2.imread('./images/db/test/flower.jpg', cv2.IMREAD_GRAYSCALE), 1) )\n",
    "test_images.append( (cv2.imread('./images/db/test/car.jpg', cv2.IMREAD_GRAYSCALE), 0) )\n",
    "test_images.append( (cv2.imread('./images/db/test/face.jpg', cv2.IMREAD_GRAYSCALE), 2) )\n",
    "\n",
    "for ti in test_images:\n",
    "    resized_ti = cv2.resize(ti[0], (nn_img_size, nn_img_size) , interpolation=cv2.INTER_AREA)\n",
    "    x_test = resized_ti.reshape(1,-1)\n",
    "    # YOUR CODE HERE \n",
    "    # convert test images to pytorch\n",
    "    # do forward pass depending mse or softmax\n",
    "    # print(\"Test output (values / pred_id / true_id):\", a2_test, np.argmax(a2_test), ti[1])\n",
    "\n",
    "# print(\"------------------------------------\")\n",
    "# print(\"Test model output Weights:\", W1, W2)\n",
    "# print(\"Test model output bias:\", b1, b2)\n",
    "\n",
    "\n",
    "plt.title(\"Training Loss vs. Number of Training Epochs\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.plot(range(1,num_epochs +1),loss_train_hist,label=\"Train\")\n",
    "plt.ylim((0,3.))\n",
    "plt.xticks(np.arange(1, num_epochs+1, 50.0))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig(\"simple_nn_train.png\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
