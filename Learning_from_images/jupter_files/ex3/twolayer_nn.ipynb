{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 13.167477\n",
      "a2 torch.Size([4, 3])\n",
      "a1 torch.Size([4, 10])\n",
      "x torch.Size([4, 1024])\n",
      "y torch.Size([4, 3])\n",
      "W2 torch.Size([10, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (10) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-fa867353b35b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m \u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;31m# predict the test images, load all test images and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-fa867353b35b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(X_train, y_train)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;31m# backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0mdCdW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdCdW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdCdb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdCdb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dCdb2.shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdCdb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdCdb1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-fa867353b35b>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(a2, a1, X_batch, y_batch, W2)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0mdCdb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrelu_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0mdCdW2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdCdb2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mdCdb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdCdb2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mW2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrelu_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (10) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import random\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "nn_img_size = 32\n",
    "num_classes = 3\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 500\n",
    "batch_size = 4\n",
    "\n",
    "loss_mode = 'crossentropy' \n",
    "\n",
    "loss_train_hist = []\n",
    "\n",
    "##################################################\n",
    "## Please implement a two layer neural network  ##\n",
    "##################################################\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU activation function\"\"\"\n",
    "    return torch.clamp(x, min=0.0)\n",
    "\n",
    "def relu_derivative(output):\n",
    "    \"\"\"derivative of the ReLU activation function\"\"\"\n",
    "    output[output <= 0] = 0\n",
    "    output[output>0] = 1\n",
    "    return output\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"softmax function to transform values to probabilities\"\"\"\n",
    "    z -= z.max()\n",
    "    z = torch.exp(z)\n",
    "    sum_z = z.sum(1, keepdim=True)\n",
    "    return z / sum_z \n",
    "\n",
    "def loss_mse(activation, y_batch):\n",
    "    \"\"\"mean squared loss function\"\"\"\n",
    "    # use MSE error as loss function \n",
    "    # Hint: the computed error needs to get normalized over the number of samples\n",
    "    loss = (activation - y_batch).pow(2).sum() \n",
    "    mse = 1.0 / activation.shape[0] * loss\n",
    "    return mse\n",
    "\n",
    "def loss_crossentropy(activation, y_batch):\n",
    "    \"\"\"cross entropy loss function\"\"\"\n",
    "    batch_size = y_batch.shape[0]\n",
    "    loss = ( - y_batch * activation.log()).sum() / batch_size\n",
    "    return loss\n",
    "\n",
    "def loss_deriv_mse(activation, y_batch):\n",
    "    \"\"\"derivative of the mean squared loss function\"\"\"\n",
    "    dCda2 = (1 / activation.shape[0]) * (activation - y_batch)\n",
    "    return dCda2\n",
    "\n",
    "def loss_deriv_crossentropy(activation, y_batch):\n",
    "    \"\"\"derivative of the mean cross entropy loss function\"\"\"\n",
    "    batch_size = y_batch.shape[0]\n",
    "    dCda2 = activation\n",
    "    dCda2[range(batch_size), np.argmax(y_batch, axis=1)] -= 1\n",
    "    dCda2 /= batch_size\n",
    "    return dCda2\n",
    "\n",
    "def setup_train():\n",
    "    \"\"\"train function\"\"\"\n",
    "    # load and resize train images in three categories\n",
    "    # cars = 0, flowers = 1, faces = 2 ( true_ids )\n",
    "    train_images_cars = glob.glob('./images/db/train/cars/*.jpg')\n",
    "    train_images_flowers = glob.glob('./images/db/train/flowers/*.jpg')\n",
    "    train_images_faces = glob.glob('./images/db/train/faces/*.jpg')\n",
    "    train_images = [train_images_cars, train_images_flowers, train_images_faces]\n",
    "    num_rows = len(train_images_cars)+len(train_images_flowers) +len(train_images_faces)\n",
    "    X_train = torch.zeros((num_rows, nn_img_size*nn_img_size))\n",
    "    y_train = torch.zeros((num_rows, num_classes))\n",
    "\n",
    "    counter = 0\n",
    "    for (label, fnames) in enumerate(train_images):\n",
    "        for fname in fnames:\n",
    "            #print(label, fname)\n",
    "            img = cv2.imread(fname, cv2.IMREAD_GRAYSCALE)\n",
    "            img = cv2.resize(img, (nn_img_size, nn_img_size) , interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            # print( label, \" -- \", fname, img.shape)\n",
    "\n",
    "            # fill matrices X_train - each row is an image vector\n",
    "            # y_train - one-hot encoded, put only a 1 where the label is correct for the row in X_train\n",
    "            y_train[counter, label] = 1\n",
    "            X_train[counter] = torch.from_numpy(img.flatten().astype(np.float32))\n",
    "            \n",
    "            counter += 1\n",
    "\n",
    "    # print(y_train)\n",
    "    return X_train, y_train\n",
    "\n",
    "def forward(X_batch, y_batch, W1, W2, b1, b2):\n",
    "    \"\"\"forward pass in the neural network \"\"\"\n",
    "    ### YOUR CODE ####\n",
    "    # please implement the forward pass\n",
    "    # \n",
    "    #print(type(X_batch))\n",
    "    #print(type(W1))\n",
    "    #print(type(b1))\n",
    "    \n",
    "    m1 = torch.mm(X_batch, W1)+b1\n",
    "    #print(tmp.shape)\n",
    "    #m1 = torch.add(tmp,b1)\n",
    "    #print(m1.shape)\n",
    "    a1 = relu(m1)\n",
    "    #print(a1.shape)\n",
    "    \n",
    "    m2 = torch.mm(a1, W2)+b2\n",
    "    #print(m2.shape)\n",
    "    a2 = relu(m2)\n",
    "    \n",
    "    loss = torch.sum((a2 - y_batch)**2)\n",
    "\n",
    "    # the function should return the loss and both intermediate activations\n",
    "    return loss.numpy(), a2, a1\n",
    "\n",
    "def backward(a2, a1, X_batch, y_batch, W2):\n",
    "    \"\"\"backward pass in the neural network \"\"\"\n",
    "    # Implement the backward pass by computing\n",
    "    # the derivative of the complete function\n",
    "    # using the chain rule as discussed in the lecture\n",
    "    print('a2',a2.shape)\n",
    "    print('a1',a1.shape)\n",
    "    print('x',X_batch.shape)\n",
    "    print('y',y_batch.shape)\n",
    "    print('W2',W2.shape)\n",
    "    \n",
    "    # please use the appropriate loss functions \n",
    "    # YOUR CODE HERE\n",
    "    dCdb2 = 2*(a2-y_batch) * relu_derivative(a2) \n",
    "    dCdW2 = dCdb2 * a1\n",
    "    \n",
    "    dCdb1 = dCdb2 * W2 * relu_derivative(a1)\n",
    "    dCdW1 = dCdb1 * X_batch\n",
    "    \n",
    "    # function should return 4 derivatives with respect to\n",
    "    # W1, W2, b1, b2\n",
    "    return dCdW1, dCdW2, dCdb1, dCdb2\n",
    "\n",
    "def train(X_train, y_train):\n",
    "    \"\"\" train procedure \"\"\"\n",
    "    # for simplicity of this execise you don't need to find useful hyperparameter\n",
    "    # I've done this for you already and every test image should work for the\n",
    "    # given very small trainings database and the following parameters.\n",
    "    #h = 1500\n",
    "    h = 10\n",
    "    std = 0.001\n",
    "    # YOUR CODE HERE\n",
    "    # initialize W1, W2, b1, b2 randomly\n",
    "    # Note: W1, W2 should be scaled by variable std\n",
    "    #W1 =  np.random.normal(0, 1.0,  h * nn_img_size**2).reshape((nn_img_size**2,h))\n",
    "    #W2 =  np.random.normal(0, 1.0,  h * num_classes).reshape((h,num_classes))\n",
    "    #W1 *= 1/(h * nn_img_size**2)\n",
    "    #W2 *= 1/(h * num_classes)\n",
    "    #print(1/(h * nn_img_size**2))\n",
    "    #print(1/(h * num_classes))\n",
    "    \n",
    "    W1 = torch.normal(torch.zeros(nn_img_size**2,h), torch.ones(nn_img_size**2,h))\n",
    "    W2 = torch.normal(torch.zeros(h,num_classes), torch.ones(h,num_classes))\n",
    "    W1 *=std\n",
    "    W2 *=std\n",
    "    \n",
    "    #b1 = np.random.normal(0, 0.2, h)\n",
    "    #b2 = np.random.normal(0, 0.2, num_classes)\n",
    "    b1 = torch.normal(torch.zeros(h), torch.ones(h))\n",
    "    b2 = torch.normal(torch.zeros(num_classes), torch.ones(num_classes))\n",
    "    \n",
    "    #print(b1)\n",
    "    #print(b2)\n",
    "    \n",
    "    lenY = y_train.shape[0]\n",
    "    \n",
    "    # run for num_epochs\n",
    "    for i in range(num_epochs):\n",
    "\n",
    "        X_batch = None\n",
    "        y_batch = None\n",
    "\n",
    "        # use only a batch of batch_size of the training images in each run\n",
    "        # sample the batch images randomly from the training set\n",
    "        # YOUR CODE HERE\n",
    "        idx = random.sample(range(0, lenY), batch_size)\n",
    "        X_batch = X_train[idx]\n",
    "        y_batch = y_train[idx]\n",
    "\n",
    "        # forward pass for two-layer neural network using ReLU as activation function\n",
    "        loss, a2, a1 = forward(X_batch, y_batch, W1, W2, b1, b2)\n",
    "\n",
    "        # add loss to loss_train_hist for plotting\n",
    "        loss_train_hist.append(loss)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(\"iteration %d: loss %f\" % (i, loss))\n",
    "\n",
    "        # backward pass \n",
    "        dCdW1, dCdW2, dCdb1, dCdb2 = backward(a2, a1, X_batch, y_batch, W2)\n",
    "        \n",
    "        print(\"dCdb2.shape:\", dCdb2.shape, dCdb1.shape)\n",
    "\n",
    "        # depending on the derivatives of W1, and W2 regaring the cost/loss\n",
    "        # we need to adapt the values in the negative direction of the \n",
    "        # gradient decreasing towards the minimum\n",
    "        # we weight the gradient by a learning rate\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        \n",
    "        \n",
    "    return W1, W2, b1, b2\n",
    "\n",
    "X_train, y_train = setup_train()\n",
    "W1, W2, b1, b2 = train(X_train, y_train)\n",
    "\n",
    "# predict the test images, load all test images and \n",
    "# run prediction by computing the forward pass\n",
    "test_images = []\n",
    "test_images.append( (cv2.imread('./images/db/test/flower.jpg', cv2.IMREAD_GRAYSCALE), 1) )\n",
    "test_images.append( (cv2.imread('./images/db/test/car.jpg', cv2.IMREAD_GRAYSCALE), 0) )\n",
    "test_images.append( (cv2.imread('./images/db/test/face.jpg', cv2.IMREAD_GRAYSCALE), 2) )\n",
    "\n",
    "for ti in test_images:\n",
    "    resized_ti = cv2.resize(ti[0], (nn_img_size, nn_img_size) , interpolation=cv2.INTER_AREA)\n",
    "    x_test = resized_ti.reshape(1,-1)\n",
    "    # YOUR CODE HERE \n",
    "    # convert test images to pytorch\n",
    "    # do forward pass depending mse or softmax\n",
    "    # print(\"Test output (values / pred_id / true_id):\", a2_test, np.argmax(a2_test), ti[1])\n",
    "\n",
    "# print(\"------------------------------------\")\n",
    "# print(\"Test model output Weights:\", W1, W2)\n",
    "# print(\"Test model output bias:\", b1, b2)\n",
    "\n",
    "#print(np.asarray(loss_train_hist))\n",
    "\n",
    "plt.title(\"Training Loss vs. Number of Training Epochs\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.plot(range(1,num_epochs +1),loss_train_hist,label=\"Train\")\n",
    "plt.ylim((0,6))\n",
    "plt.xticks(np.arange(1, num_epochs+1, 50.0))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig(\"simple_nn_train.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6115,  0.3719,  1.3543],\n",
       "        [ 1.2641, -0.5581,  0.5403],\n",
       "        [ 0.0199,  0.5632,  0.0536],\n",
       "        [-0.0604,  0.1405, -0.2728],\n",
       "        [ 0.1770, -0.2019, -1.1390],\n",
       "        [ 1.0944, -0.4882,  0.9378]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.normal(torch.zeros(6,3), torch.ones(6,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]\n",
      " [7 8]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[ 3,  4],\n",
       "        [ 5,  6],\n",
       "        [ 7,  8],\n",
       "        [ 9, 10]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.matrix('1 2 3 4 5 6 7 8').reshape((4,2))\n",
    "print(a)\n",
    "b = np.matrix('2,2')\n",
    "a+b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
