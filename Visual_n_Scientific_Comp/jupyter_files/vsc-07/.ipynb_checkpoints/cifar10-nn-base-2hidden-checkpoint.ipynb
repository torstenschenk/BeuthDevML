{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3072)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3072)\n",
      "Validation labels shape:  (1000,)\n",
      "W0 shape: (3072, 200)\n",
      "b0 shape: (1, 200)\n",
      "W1 shape: (200, 50)\n",
      "b1 shape: (1, 50)\n",
      "W2 shape: (50, 10)\n",
      "b2 shape: (1, 10)\n",
      "iteration 0 / 3000: loss 2.302582\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 200 is different from 3072)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0fb8ce50e06a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    354\u001b[0m     stats = net.train(X_train, y_train, X_val, y_val,\n\u001b[1;32m    355\u001b[0m                         \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate_decay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                       num_iters=num_iter, batch_size=batch_size, verbose=True)\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Final training loss: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss_history'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-0fb8ce50e06a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, X_val, y_val, learning_rate, learning_rate_decay, num_iters, batch_size, verbose)\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mit\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0miterations_per_epoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m                 \u001b[0;31m# Überprüfung der Klassifikationsgenauigkeit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m                 \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m                 \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0mtrain_acc_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-0fb8ce50e06a>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \"\"\"\n\u001b[1;32m    299\u001b[0m         \u001b[0;31m#y_pred = None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0mm1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0;31m#print('forward m1.shape:',m1.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 200 is different from 3072)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import helper\n",
    "import os\n",
    "import random\n",
    "\n",
    "# show plot first run\n",
    "%matplotlib inline \n",
    "# test autocompletion with tab or tab+shift\n",
    "%config IPCompleter.greedy=True \n",
    "\n",
    "random.seed(30)\n",
    "\n",
    "class TwoLayerNeuralNetwork:\n",
    "    \"\"\"\n",
    "    Ein 2-Layer 'fully-connected' neural network, d.h. alle Neuronen sind mit allen anderen\n",
    "    verbunden. Die Anzahl der Eingabevektoren ist N mit einer Dimension D, einem 'Hidden'-Layer mit\n",
    "    H Neuronen. Es soll eine Klassifikation über 10 Klassen (C) durchgeführt werden.\n",
    "    Wir trainieren das Netzwerk mit einer 'Softmax'-Loss Funktion und einer L2 Regularisierung\n",
    "    auf den Gewichtungsmatrizen (W1 und W2). Das Netzwerk nutzt ReLU Aktivierungsfunktionen nach\n",
    "    dem ersten Layer.\n",
    "    Die Architektur des Netzwerkes läßt sich abstrakt so darstellen:\n",
    "    Eingabe - 'fully connected'-Layer - ReLU - 'fully connected'-Layer - Softmax\n",
    "\n",
    "    Die Ausgabe aus dem 2.Layer sind die 'Scores' (Wahrscheinlichkeiten) für jede Klasse.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size0, hidden_size1, output_size, std=1e-4):\n",
    "        \"\"\"\n",
    "        Intitialisierung des Netzes - Die Gewichtungsmatrizen und die Bias-Vektoren werden mit\n",
    "        Zufallswerten initialisiert.\n",
    "        W1: 1.Layer Gewichte (D, H)\n",
    "        b1: 1.Layer Bias (H,)\n",
    "        W2: 2.Layer Gewichte (H, C)\n",
    "        b2: 2.Layer Bias (C,)\n",
    "\n",
    "        :param input_size: Die CIFAR-10 Bilder haben die Dimension D (32*32*3).\n",
    "        :param hidden_size: Anzahl der Neuronen im Hidden-Layer H.\n",
    "        :param output_size: Anzahl der Klassen C.\n",
    "        :param std: Skalierungsfaktoren für die Initialisierung (muss klein sein)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        \n",
    "        self.W0 = std * np.random.randn(input_size, hidden_size0)\n",
    "        self.b0 = std * np.random.randn(1, hidden_size0)\n",
    "        self.W1 = std * np.random.randn(hidden_size0, hidden_size1)\n",
    "        #self.W1 = std * np.random.randn(input_size, hidden_size1)\n",
    "        self.b1 = std * np.random.randn(1, hidden_size1)\n",
    "        self.W2 = std * np.random.randn(hidden_size1, output_size)\n",
    "        self.b2 = std * np.random.randn(1, output_size)\n",
    "        print('W0 shape:', self.W0.shape)\n",
    "        print('b0 shape:', self.b0.shape)\n",
    "        print('W1 shape:', self.W1.shape)\n",
    "        print('b1 shape:', self.b1.shape)\n",
    "        print('W2 shape:', self.W2.shape)\n",
    "        print('b2 shape:', self.b2.shape)\n",
    "\n",
    "    def softmax(self, z):\n",
    "        z -= np.max(z)\n",
    "        return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0.0, x)\n",
    "\n",
    "    def relu_derivative(self, output):\n",
    "        output[output <= 0] = 0\n",
    "        output[output>0] = 1\n",
    "        return output\n",
    "\n",
    "    def loss_deriv_softmax(self, activation, y_batch):\n",
    "        batch_size = y_batch.shape[0]\n",
    "        dCda2 = activation\n",
    "        dCda2[range(batch_size), y_batch] -= 1\n",
    "        dCda2 /= batch_size\n",
    "        return dCda2\n",
    "\n",
    "    def loss_crossentropy(self, activation, y_batch):\n",
    "        \"\"\"\n",
    "        Berechnet den loss (Fehler) des 2-Layer-Netzes\n",
    "\n",
    "        :param batch_size: Anzahl der Eingabebilder in einem Batch über den der Fehler normalisiert werden muss\n",
    "        :param y: Vektor mit den Trainingslabels y[i] enthält ein Label aus X[i] und jedes y[i] ist ein\n",
    "                  Integer zwischen 0 <= y[i] < C (Anzahl der Klassen)\n",
    "        :param reg: Regulasierungsstärke\n",
    "        :return: loss - normalisierter Fehler des Batches\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = y_batch.shape[0]\n",
    "        correct_logprobs = -np.log(activation[range(batch_size), y_batch])\n",
    "        loss = np.sum(correct_logprobs) / batch_size\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def forward(self, X, y):\n",
    "        \"\"\"\n",
    "        Führt den gesamten Forward Prozess durch und berechnet den Fehler (loss) und die Aktivierungen a1 und\n",
    "        a2 und gibt diese Werte zuruück\n",
    "        :param X: Trainings bzw. Testset\n",
    "        :param y: Labels des Trainings- bzw. Testsets\n",
    "        :return: loss, a1, a2\n",
    "        \"\"\"\n",
    "\n",
    "        # Berechen Sie den score\n",
    "        #N, D = X.shape\n",
    "        # TODO: Berechnen Sie den Forward-Schritt und geben Sie den Vektor mit Scores zurueck\n",
    "        # Nutzen Sie die ReLU Aktivierungsfunktion im ersten Layer\n",
    "        # Berechnen Sie die Klassenwahrscheinlichkeiten unter Nutzung der softmax Funktion\n",
    "        m0 = (X @ self.W0) + self.b0\n",
    "        #print('forward m0.shape:',m0.shape)\n",
    "        a0 = self.relu(m0)\n",
    "        \n",
    "        m1 = (a0 @ self.W1) + self.b1\n",
    "        #print('forward m1.shape:',m1.shape)\n",
    "        a1 = self.relu(m1)\n",
    "        \n",
    "        m2 = (a1 @ self.W2) + self.b2\n",
    "        #print('forward m2.shape:',m2.shape)\n",
    "        a2 = self.softmax(m2)\n",
    "        \n",
    "        # TODO: Berechnen Sie den Fehler mit der cross-entropy Funktion\n",
    "        loss = self.loss_crossentropy(a2, y)\n",
    "        \n",
    "        return loss, a0, a1, a2\n",
    "\n",
    "    def backward(self, a0, a1, a2, X, y):\n",
    "        \"\"\"\n",
    "        Backward pass- dabei wird der Gradient der Gewichte W0, W1, W2 und der Biases b0, b1, b2 aus den \n",
    "        Ausgaben des Netzes berechnet und die Gradienten der einzelnen Layer als ein Dictionary zurückgegeben.\n",
    "        Zum Beispiel sollte grads['W0'] die Gradienten von self.W0 enthalten \n",
    "        (das ist eine Matrix der gleichen Größe wie self.W0.)\n",
    "        :param a0: Aktivierung aus dem 1.Layer\n",
    "        :param a1: Aktivierung aus dem 1.Layer\n",
    "        :param a2: Aktivierung aus dem 2.Layer -> Output des Netzes\n",
    "        :param X:\n",
    "        :param y:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Nutzen Sie dabei die Notizen aus der Vorlesung und die gegebenen Ableitungsfunktionen\n",
    "        # please use the appropriate loss functions \n",
    "        # YOUR CODE HERE\n",
    "        # Backward pass: Berechnen Sie die Gradienten\n",
    "        #N, D = X.shape\n",
    "        \n",
    "        # W1' = X.T * ( ( (2*(a2-yc)*relu(a2)) * W2.T ) * relu(a1) ) ## use relu(a2) or not??? no neg val can pass through\n",
    "        # b1' = 1 * ( ( (2*(a2-yc)*relu(a2)) * W2.T ) * relu(a1) )\n",
    "        # W2' = a1.T * 2*(a2 - yc)\n",
    "        # v2' = 1 * 2*(a2 - yc)\n",
    "\n",
    "        # activations reverse\n",
    "        dCda2  = self.loss_deriv_softmax(a2,y) # for W2\n",
    "        da1dm1 = self.relu_derivative(a1) #for W1\n",
    "        da0dm0 = self.relu_derivative(a0) # for W0\n",
    "        \n",
    "        # implementation\n",
    "        tmp2 = dCda2 @ self.W2.T\n",
    "        tmp3 = tmp2 * da1dm1 # *relu_deriv element wise mult\n",
    "       \n",
    "        tmp4 = tmp3 @ self.W1.T\n",
    "        tmp5 = tmp4 * da0dm0 # *relu_deriv element wise mult\n",
    "    \n",
    "    \n",
    "    \n",
    "        dCdW2 = a1.T @ dCda2\n",
    "        dCdb2 = np.mean( dCda2, axis=0 )\n",
    "    \n",
    "        dCdW1 = a0.T @ tmp3\n",
    "        dCdb1 = np.mean( tmp3, axis=0 )\n",
    "\n",
    "        dCdW0 = X.T @ tmp5\n",
    "        dCdb0 = np.mean( tmp5, axis=0 )\n",
    "\n",
    "        # function should return 4 derivatives with respect to\n",
    "        # W1, W2, b1, b2        \n",
    "        # Füllen Sie das Dictionary grads['W2'], grads['b2'], grads['W1'], grads['b1']\n",
    "        grads = {}\n",
    "        grads['W0'] = dCdW0\n",
    "        grads['b0'] = dCdb0\n",
    "        grads['W1'] = dCdW1\n",
    "        grads['b1'] = dCdb1\n",
    "        grads['W2'] = dCdW2\n",
    "        grads['b2'] = dCdb2\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def train(self, X, y, X_val, y_val,\n",
    "              learning_rate=1e-3, learning_rate_decay=0.95, num_iters=100,\n",
    "              batch_size=200, verbose=False):\n",
    "        \"\"\"\n",
    "        Training des Neuronalen Netzwerkes unter der Nutzung des iterativen\n",
    "        Optimierungsverfahrens Stochastic Gradient Descent\n",
    "        Train this neural network using stochastic gradient descent.\n",
    "\n",
    "        :param X: Numpy Array der Größe (N,D)\n",
    "        :param y: Numpy Array der Größe (N,) mit den jeweiligen Labels y[i] = c. Das bedeutet, dass X[i] das label c hat\n",
    "                  mit 0 <= c < C\n",
    "        :param X_val: Numpy Array der Größe (N_val,D) mit den Validierungs-/Testdaten\n",
    "        :param y_val: Numpy Array der Größe (N_val,) mit den Labels für die Validierungs-/Testdaten\n",
    "        :param learning_rate: Faktor der Lernrate für den Optimierungsprozess\n",
    "        :param learning_rate_decay: gibt an, inwieweit die Lernrate in jeder Epoche angepasst werden soll\n",
    "        :param reg: Stärke der Regularisierung\n",
    "        :param num_iters: Anzahl der Iterationen der Optimierung\n",
    "        :param batch_size: Anzahl der Trainingseingabebilder, die in jedem forward-Schritt mitgegeben werden sollen\n",
    "        :param verbose: boolean, ob etwas ausgegeben werden soll\n",
    "        :return: dict (fuer die Auswertung) - enthält Fehler und Genauigkeit der Klassifizierung für jede Iteration bzw. Epoche\n",
    "        \"\"\"\n",
    "        num_train = X.shape[0]\n",
    "        iterations_per_epoch = int(max(num_train / batch_size, 1))\n",
    "\n",
    "        # Wir nutzen einen Stochastischen Gradient Decent (SGD) Optimierer um unsere\n",
    "        # Parameter W1,W2,b1,b2 zu optimieren\n",
    "        loss_history = []\n",
    "        loss_val_history = []\n",
    "        train_acc_history = []\n",
    "        val_acc_history = []\n",
    "\n",
    "        sample_propabilities = np.ones(X.shape[0])\n",
    "        for it in range(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "\n",
    "            ############################\n",
    "            # TODO: Erzeugen Sie einen zufälligen Batch der Größe batch_size\n",
    "            # aus den Trainingsdaten und speichern diese in X_batch und y_batch\n",
    "            # X_batch\n",
    "            # y_batch \n",
    "            idx = random.sample(range(0, num_train), batch_size)\n",
    "            X_batch = X[idx]\n",
    "            y_batch = y[idx]\n",
    "            \n",
    "            ############################\n",
    "\n",
    "            # TODO: Berechnung von loss und gradient für den aktuellen Batch\n",
    "            #print('X_batch shape:',X_batch.shape)\n",
    "            #print('y_batch shape:',y_batch.shape)\n",
    "            loss, a0, a1, a2 = self.forward(X_batch, y_batch)\n",
    "            # Merken des Fehlers für den Plot\n",
    "            loss_history.append(loss)\n",
    "            \n",
    "            # Jetzt backward pass (Gradienten berechnen).\n",
    "            # backward pass \n",
    "            grads = self.backward(a0, a1, a2, X_batch, y_batch)\n",
    "            \n",
    "            # Berechnung des Fehlers mit den aktuellen Parametern (W, b)\n",
    "            # mit dem Testset\n",
    "            loss_val, a0_val, a1_val, a2_val = self.forward(X_val, y_val)\n",
    "            loss_val_history.append(loss_val)\n",
    "\n",
    "            ############################\n",
    "            # TODO: Nutzen Sie die Gradienten aus der Backward-Funktion und passen\n",
    "            # Sie die Parameter an (self.W1, self.b1 etc). Diese werden mit der Lernrate\n",
    "            # gewichtet\n",
    "            self.W0 -= learning_rate * grads['W0']\n",
    "            self.b0 -= learning_rate * grads['b0']\n",
    "            self.W1 -= learning_rate * grads['W1']\n",
    "            self.b1 -= learning_rate * grads['b1']\n",
    "            self.W2 -= learning_rate * grads['W2']\n",
    "            self.b2 -= learning_rate * grads['b2']\n",
    "            \n",
    "            ############################\n",
    "\n",
    "            # Ausgabe des aktuellen Fehlers. Diese sollte am Anfang erstmal nach unten gehen\n",
    "            # kann aber immer etwas schwanken.\n",
    "            if verbose and it % 100 == 0:\n",
    "                print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
    "\n",
    "            # Wir überprüfen jede Epoche die Genauigkeit (von Trainingsset und Testset)\n",
    "            # und dämpfen die Lernrate\n",
    "            if it % iterations_per_epoch == 0:\n",
    "                # Überprüfung der Klassifikationsgenauigkeit\n",
    "                train_acc = (self.predict(X_batch) == y_batch).mean()\n",
    "                val_acc = (self.predict(X_val) == y_val).mean()\n",
    "                train_acc_history.append(train_acc)\n",
    "                val_acc_history.append(val_acc)\n",
    "                print('epoch done... acc', val_acc)\n",
    "\n",
    "                # Dämpfung der Lernrate\n",
    "                learning_rate *= learning_rate_decay\n",
    "\n",
    "        # Zum Plotten der Genauigkeiten geben wir die\n",
    "        # gesammelten Daten zurück\n",
    "        return {\n",
    "            'loss_history': loss_history,\n",
    "            'loss_val_history': loss_val_history,\n",
    "            'train_acc_history': train_acc_history,\n",
    "            'val_acc_history': val_acc_history,\n",
    "        }\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Benutzen Sie die trainierten Gewichte des 2-Layer-Netzes um die Klassen für das\n",
    "        Validierungsset vorherzusagen. Dafür müssen Sie für das/die Eingabebilder X nur\n",
    "        die Scores berechnen. Der höchste Score ist die vorhergesagte Klasse. Dieser wird in y_pred\n",
    "        geschrieben und zurückgegeben.\n",
    "\n",
    "        :param X: Numpy Array der Größe (N,D)\n",
    "        :return: y_pred Numpy Array der Größe (N,) die die jeweiligen Labels für alle Elemente in X enthaelt.\n",
    "        y_pred[i] = c bedeutet, das fuer X[i] die Klasse c mit 0<=c<C vorhergesagt wurde\n",
    "        \"\"\"\n",
    "        m0 = (X @ self.W0) + self.b0\n",
    "        a0 = self.relu(m0)\n",
    "\n",
    "        m1 = (a0 @ self.W1) + self.b1\n",
    "        a1 = self.relu(m1)\n",
    "        \n",
    "        a2 = (a1 @ self.W2) + self.b2\n",
    "        #print('forward m2.shape:',m2.shape)\n",
    "        #a2 = self.softmax(m2)\n",
    "        \n",
    "        ############################\n",
    "        # TODO: Implementieren Sie die Vorhersage. D.h. für ein/mehrere Bild/er mit den gelernten\n",
    "        # Parametern den Wahrscheinlichkeit berechnen.\n",
    "        # np.argmax in dem Wahrscheinlichkeitsvektor ist die wahrscheinlichste Klasse\n",
    "        ############################\n",
    "        # Implementieren Sie nochmals den Forward pass um die Wahrscheinlichkeiten\n",
    "        # vorherzusagen\n",
    "        y_pred = np.argmax(a2, axis=1)\n",
    "        #print('y_pred:',y_pred)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    X_train, y_train, X_val, y_val = helper.prepare_CIFAR10_images()\n",
    "    # TODO: Laden der Bilder. Hinweis - wir nutzen nur die Trainigsbilder zum Trainieren und die\n",
    "    # Validierungsbilder zum Testen.\n",
    "    print('Train data shape: ', X_train.shape)\n",
    "    print('Train labels shape: ', y_train.shape)\n",
    "    print('Validation data shape: ', X_val.shape)\n",
    "    print('Validation labels shape: ', y_val.shape)\n",
    "\n",
    "    # Grösse der Bilder\n",
    "    input_size = 32 * 32 * 3\n",
    "    # Anzahl der Klassen\n",
    "    num_classes = 10\n",
    "\n",
    "    #############################################\n",
    "    # Hyperparameter0    ##1###########################################1\n",
    "\n",
    "\n",
    "    # TODO: mit diesen Parametern sollten Sie in etwa auf eine\n",
    "    # Klassifikationsgenauigkeit von 43% kommen. Optimieren Sie die\n",
    "    # Hyperparameter um die Genauigkeit zu erhöhen (bitte tun sie das\n",
    "    # systematisch und nicht einfach durch probieren - also z.B. in einem\n",
    "    # for-loop eine Reihe von Parametern testen und die Einzelbilder abspeichern)\n",
    "    \n",
    "    hidden_size0 = 200   # Anzahl der Neuronen im Hidden Layer\n",
    "    hidden_size1 = 50   # Anzahl der Neuronen im Hidden Layer\n",
    "    num_iter = 3000  # Anzahl der Optimierungsiterationen\n",
    "    batch_size = 100  # Eingabeanzahl der Bilder\n",
    "    learning_rate = 0.001  # Lernrate\n",
    "    learning_rate_decay = 0.98  # Lernratenabschwächung\n",
    "    \n",
    "    net = TwoLayerNeuralNetwork(input_size, hidden_size0,hidden_size1, num_classes)\n",
    "\n",
    "    # Train the network\n",
    "    stats = net.train(X_train, y_train, X_val, y_val,\n",
    "                        learning_rate=learning_rate, learning_rate_decay=learning_rate_decay, \n",
    "                      num_iters=num_iter, batch_size=batch_size, verbose=True)\n",
    "\n",
    "    print('Final training loss: ', stats['loss_history'][-1])\n",
    "    print('Final validation loss: ', stats['loss_val_history'][-1])\n",
    "\n",
    "    print('Final validation accuracy: ', stats['val_acc_history'][-1])\n",
    "\n",
    "    helper.plot_net_weights(net)\n",
    "    helper.plot_accuracy(stats)\n",
    "    helper.plot_loss(stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
