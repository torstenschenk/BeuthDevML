{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3072)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3072)\n",
      "Validation labels shape:  (1000,)\n",
      "iteration 0 / 14000: loss 2.302612\n",
      "epoch done... acc 0.146\n",
      "iteration 100 / 14000: loss 2.041802\n",
      "iteration 200 / 14000: loss 1.835807\n",
      "iteration 300 / 14000: loss 1.884930\n",
      "iteration 400 / 14000: loss 1.729429\n",
      "epoch done... acc 0.373\n",
      "iteration 500 / 14000: loss 1.832018\n",
      "iteration 600 / 14000: loss 1.698563\n",
      "iteration 700 / 14000: loss 1.735223\n",
      "iteration 800 / 14000: loss 1.645720\n",
      "iteration 900 / 14000: loss 1.691082\n",
      "epoch done... acc 0.417\n",
      "iteration 1000 / 14000: loss 1.724287\n",
      "iteration 1100 / 14000: loss 1.634516\n",
      "iteration 1200 / 14000: loss 1.408235\n",
      "iteration 1300 / 14000: loss 1.467351\n",
      "iteration 1400 / 14000: loss 1.711286\n",
      "epoch done... acc 0.435\n",
      "iteration 1500 / 14000: loss 1.475298\n",
      "iteration 1600 / 14000: loss 1.440514\n",
      "iteration 1700 / 14000: loss 1.559395\n",
      "iteration 1800 / 14000: loss 1.362620\n",
      "iteration 1900 / 14000: loss 1.481450\n",
      "epoch done... acc 0.447\n",
      "iteration 2000 / 14000: loss 1.576923\n",
      "iteration 2100 / 14000: loss 1.409366\n",
      "iteration 2200 / 14000: loss 1.588828\n",
      "iteration 2300 / 14000: loss 1.432269\n",
      "iteration 2400 / 14000: loss 1.554892\n",
      "epoch done... acc 0.458\n",
      "iteration 2500 / 14000: loss 1.548507\n",
      "iteration 2600 / 14000: loss 1.392591\n",
      "iteration 2700 / 14000: loss 1.553457\n",
      "iteration 2800 / 14000: loss 1.354283\n",
      "iteration 2900 / 14000: loss 1.366090\n",
      "epoch done... acc 0.463\n",
      "iteration 3000 / 14000: loss 1.478777\n",
      "iteration 3100 / 14000: loss 1.325251\n",
      "iteration 3200 / 14000: loss 1.269441\n",
      "iteration 3300 / 14000: loss 1.433623\n",
      "iteration 3400 / 14000: loss 1.458008\n",
      "epoch done... acc 0.479\n",
      "iteration 3500 / 14000: loss 1.352473\n",
      "iteration 3600 / 14000: loss 1.325084\n",
      "iteration 3700 / 14000: loss 1.355213\n",
      "iteration 3800 / 14000: loss 1.359432\n",
      "iteration 3900 / 14000: loss 1.406850\n",
      "epoch done... acc 0.482\n",
      "iteration 4000 / 14000: loss 1.408453\n",
      "iteration 4100 / 14000: loss 1.547033\n",
      "iteration 4200 / 14000: loss 1.363514\n",
      "iteration 4300 / 14000: loss 1.361326\n",
      "iteration 4400 / 14000: loss 1.306748\n",
      "epoch done... acc 0.487\n",
      "iteration 4500 / 14000: loss 1.367315\n",
      "iteration 4600 / 14000: loss 1.434467\n",
      "iteration 4700 / 14000: loss 1.356394\n",
      "iteration 4800 / 14000: loss 1.319435\n",
      "iteration 4900 / 14000: loss 1.262249\n",
      "epoch done... acc 0.487\n",
      "iteration 5000 / 14000: loss 1.095592\n",
      "iteration 5100 / 14000: loss 1.343029\n",
      "iteration 5200 / 14000: loss 1.236089\n",
      "iteration 5300 / 14000: loss 1.208284\n",
      "epoch done... acc 0.504\n",
      "iteration 5400 / 14000: loss 1.391793\n",
      "iteration 5500 / 14000: loss 1.180929\n",
      "iteration 5600 / 14000: loss 1.210210\n",
      "iteration 5700 / 14000: loss 1.205756\n",
      "iteration 5800 / 14000: loss 1.133035\n",
      "epoch done... acc 0.488\n",
      "iteration 5900 / 14000: loss 1.436759\n",
      "iteration 6000 / 14000: loss 1.257254\n",
      "iteration 6100 / 14000: loss 1.350909\n",
      "iteration 6200 / 14000: loss 1.189153\n",
      "iteration 6300 / 14000: loss 1.386729\n",
      "epoch done... acc 0.498\n",
      "iteration 6400 / 14000: loss 1.231915\n",
      "iteration 6500 / 14000: loss 1.180848\n",
      "iteration 6600 / 14000: loss 1.242776\n",
      "iteration 6700 / 14000: loss 1.211984\n",
      "iteration 6800 / 14000: loss 1.367359\n",
      "epoch done... acc 0.502\n",
      "iteration 6900 / 14000: loss 1.326809\n",
      "iteration 7000 / 14000: loss 1.411687\n",
      "iteration 7100 / 14000: loss 1.231613\n",
      "iteration 7200 / 14000: loss 1.143230\n",
      "iteration 7300 / 14000: loss 1.059581\n",
      "epoch done... acc 0.496\n",
      "iteration 7400 / 14000: loss 1.272255\n",
      "iteration 7500 / 14000: loss 1.440291\n",
      "iteration 7600 / 14000: loss 1.242700\n",
      "iteration 7700 / 14000: loss 1.234156\n",
      "iteration 7800 / 14000: loss 1.316896\n",
      "epoch done... acc 0.502\n",
      "iteration 7900 / 14000: loss 1.252900\n",
      "iteration 8000 / 14000: loss 1.354557\n",
      "iteration 8100 / 14000: loss 1.251322\n",
      "iteration 8200 / 14000: loss 1.349029\n",
      "iteration 8300 / 14000: loss 1.115806\n",
      "epoch done... acc 0.53\n",
      "iteration 8400 / 14000: loss 1.164065\n",
      "iteration 8500 / 14000: loss 1.240220\n",
      "iteration 8600 / 14000: loss 1.264273\n",
      "iteration 8700 / 14000: loss 0.993364\n",
      "iteration 8800 / 14000: loss 1.235481\n",
      "epoch done... acc 0.53\n",
      "iteration 8900 / 14000: loss 1.192674\n",
      "iteration 9000 / 14000: loss 1.086740\n",
      "iteration 9100 / 14000: loss 1.201329\n",
      "iteration 9200 / 14000: loss 1.117451\n",
      "iteration 9300 / 14000: loss 1.089273\n",
      "epoch done... acc 0.523\n",
      "iteration 9400 / 14000: loss 1.060971\n",
      "iteration 9500 / 14000: loss 0.976107\n",
      "iteration 9600 / 14000: loss 1.079104\n",
      "iteration 9700 / 14000: loss 1.208281\n",
      "iteration 9800 / 14000: loss 1.151316\n",
      "epoch done... acc 0.511\n",
      "iteration 9900 / 14000: loss 1.285663\n",
      "iteration 10000 / 14000: loss 1.209533\n",
      "iteration 10100 / 14000: loss 1.081806\n",
      "iteration 10200 / 14000: loss 1.279075\n",
      "epoch done... acc 0.515\n",
      "iteration 10300 / 14000: loss 1.209839\n",
      "iteration 10400 / 14000: loss 1.220306\n",
      "iteration 10500 / 14000: loss 1.153593\n",
      "iteration 10600 / 14000: loss 1.335863\n",
      "iteration 10700 / 14000: loss 1.186760\n",
      "epoch done... acc 0.515\n",
      "iteration 10800 / 14000: loss 1.114089\n",
      "iteration 10900 / 14000: loss 1.077158\n",
      "iteration 11000 / 14000: loss 1.163031\n",
      "iteration 11100 / 14000: loss 1.153360\n",
      "iteration 11200 / 14000: loss 1.143555\n",
      "epoch done... acc 0.508\n",
      "iteration 11300 / 14000: loss 1.073498\n",
      "iteration 11400 / 14000: loss 1.041558\n",
      "iteration 11500 / 14000: loss 1.262378\n",
      "iteration 11600 / 14000: loss 1.134289\n",
      "iteration 11700 / 14000: loss 1.042956\n",
      "epoch done... acc 0.519\n",
      "iteration 11800 / 14000: loss 1.240378\n",
      "iteration 11900 / 14000: loss 1.106617\n",
      "iteration 12000 / 14000: loss 1.163714\n",
      "iteration 12100 / 14000: loss 1.140372\n",
      "iteration 12200 / 14000: loss 1.196709\n",
      "epoch done... acc 0.524\n",
      "iteration 12300 / 14000: loss 1.099419\n",
      "iteration 12400 / 14000: loss 1.100683\n",
      "iteration 12500 / 14000: loss 1.134570\n",
      "iteration 12600 / 14000: loss 0.980197\n",
      "iteration 12700 / 14000: loss 1.081619\n",
      "epoch done... acc 0.519\n",
      "iteration 12800 / 14000: loss 1.012403\n",
      "iteration 12900 / 14000: loss 1.182476\n",
      "iteration 13000 / 14000: loss 1.099393\n",
      "iteration 13100 / 14000: loss 1.002141\n",
      "iteration 13200 / 14000: loss 0.989299\n",
      "epoch done... acc 0.512\n",
      "iteration 13300 / 14000: loss 1.117157\n",
      "iteration 13400 / 14000: loss 1.195724\n",
      "iteration 13500 / 14000: loss 1.186385\n",
      "iteration 13600 / 14000: loss 1.054457\n",
      "iteration 13700 / 14000: loss 1.012525\n",
      "epoch done... acc 0.52\n",
      "iteration 13800 / 14000: loss 1.012836\n",
      "iteration 13900 / 14000: loss 1.158943\n",
      "Final training loss:  1.183326116450456\n",
      "Final validation loss:  1.380313411823935\n",
      "Final validation accuracy:  0.52\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "0 0 0 264 14000 100 0.001 0.98 0.52\n",
      "iteration 0 / 14000: loss 2.302607\n",
      "epoch done... acc 0.182\n",
      "iteration 100 / 14000: loss 2.039863\n",
      "iteration 200 / 14000: loss 1.934444\n",
      "iteration 300 / 14000: loss 1.797750\n",
      "iteration 400 / 14000: loss 1.789135\n",
      "epoch done... acc 0.395\n",
      "iteration 500 / 14000: loss 1.755163\n",
      "iteration 600 / 14000: loss 1.763744\n",
      "iteration 700 / 14000: loss 1.646789\n",
      "iteration 800 / 14000: loss 1.728734\n",
      "iteration 900 / 14000: loss 1.513299\n",
      "epoch done... acc 0.421\n",
      "iteration 1000 / 14000: loss 1.684115\n",
      "iteration 1100 / 14000: loss 1.627874\n",
      "iteration 1200 / 14000: loss 1.497975\n",
      "iteration 1300 / 14000: loss 1.505677\n",
      "iteration 1400 / 14000: loss 1.576323\n",
      "epoch done... acc 0.444\n",
      "iteration 1500 / 14000: loss 1.501999\n",
      "iteration 1600 / 14000: loss 1.716531\n",
      "iteration 1700 / 14000: loss 1.469886\n",
      "iteration 1800 / 14000: loss 1.409619\n",
      "iteration 1900 / 14000: loss 1.508764\n",
      "epoch done... acc 0.44\n",
      "iteration 2000 / 14000: loss 1.607603\n",
      "iteration 2100 / 14000: loss 1.718506\n",
      "iteration 2200 / 14000: loss 1.544067\n",
      "iteration 2300 / 14000: loss 1.609476\n",
      "iteration 2400 / 14000: loss 1.489156\n",
      "epoch done... acc 0.458\n",
      "iteration 2500 / 14000: loss 1.391681\n",
      "iteration 2600 / 14000: loss 1.321428\n",
      "iteration 2700 / 14000: loss 1.406673\n",
      "iteration 2800 / 14000: loss 1.630039\n",
      "iteration 2900 / 14000: loss 1.357987\n",
      "epoch done... acc 0.469\n",
      "iteration 3000 / 14000: loss 1.421836\n",
      "iteration 3100 / 14000: loss 1.454217\n",
      "iteration 3200 / 14000: loss 1.420117\n",
      "iteration 3300 / 14000: loss 1.450150\n",
      "iteration 3400 / 14000: loss 1.515211\n",
      "epoch done... acc 0.488\n",
      "iteration 3500 / 14000: loss 1.411774\n",
      "iteration 3600 / 14000: loss 1.294581\n",
      "iteration 3700 / 14000: loss 1.424618\n",
      "iteration 3800 / 14000: loss 1.362263\n",
      "iteration 3900 / 14000: loss 1.400478\n",
      "epoch done... acc 0.465\n",
      "iteration 4000 / 14000: loss 1.387757\n",
      "iteration 4100 / 14000: loss 1.296954\n",
      "iteration 4200 / 14000: loss 1.340270\n",
      "iteration 4300 / 14000: loss 1.239218\n",
      "iteration 4400 / 14000: loss 1.643205\n",
      "epoch done... acc 0.472\n",
      "iteration 4500 / 14000: loss 1.211607\n",
      "iteration 4600 / 14000: loss 1.368249\n",
      "iteration 4700 / 14000: loss 1.457324\n",
      "iteration 4800 / 14000: loss 1.350710\n",
      "iteration 4900 / 14000: loss 1.263870\n",
      "epoch done... acc 0.485\n",
      "iteration 5000 / 14000: loss 1.359305\n",
      "iteration 5100 / 14000: loss 1.396709\n",
      "iteration 5200 / 14000: loss 1.201760\n",
      "iteration 5300 / 14000: loss 1.283206\n",
      "epoch done... acc 0.49\n",
      "iteration 5400 / 14000: loss 1.413159\n",
      "iteration 5500 / 14000: loss 1.200172\n",
      "iteration 5600 / 14000: loss 1.501641\n",
      "iteration 5700 / 14000: loss 1.198594\n",
      "iteration 5800 / 14000: loss 1.433779\n",
      "epoch done... acc 0.501\n",
      "iteration 5900 / 14000: loss 1.278411\n",
      "iteration 6000 / 14000: loss 1.121440\n",
      "iteration 6100 / 14000: loss 1.152179\n",
      "iteration 6200 / 14000: loss 1.270693\n",
      "iteration 6300 / 14000: loss 1.239618\n",
      "epoch done... acc 0.502\n",
      "iteration 6400 / 14000: loss 1.310250\n",
      "iteration 6500 / 14000: loss 1.194403\n",
      "iteration 6600 / 14000: loss 1.182748\n",
      "iteration 6700 / 14000: loss 1.184341\n",
      "iteration 6800 / 14000: loss 1.295895\n",
      "epoch done... acc 0.5\n",
      "iteration 6900 / 14000: loss 1.312412\n",
      "iteration 7000 / 14000: loss 1.298044\n",
      "iteration 7100 / 14000: loss 1.273755\n",
      "iteration 7200 / 14000: loss 1.157555\n",
      "iteration 7300 / 14000: loss 1.113689\n",
      "epoch done... acc 0.503\n",
      "iteration 7400 / 14000: loss 1.109348\n",
      "iteration 7500 / 14000: loss 1.320804\n",
      "iteration 7600 / 14000: loss 1.102293\n",
      "iteration 7700 / 14000: loss 1.255919\n",
      "iteration 7800 / 14000: loss 1.322720\n",
      "epoch done... acc 0.502\n",
      "iteration 7900 / 14000: loss 1.256258\n",
      "iteration 8000 / 14000: loss 1.276567\n",
      "iteration 8100 / 14000: loss 1.064355\n",
      "iteration 8200 / 14000: loss 1.244327\n",
      "iteration 8300 / 14000: loss 1.111684\n",
      "epoch done... acc 0.516\n",
      "iteration 8400 / 14000: loss 1.343397\n",
      "iteration 8500 / 14000: loss 1.229285\n",
      "iteration 8600 / 14000: loss 1.154565\n",
      "iteration 8700 / 14000: loss 1.220737\n",
      "iteration 8800 / 14000: loss 1.283158\n",
      "epoch done... acc 0.51\n",
      "iteration 8900 / 14000: loss 1.213558\n",
      "iteration 9000 / 14000: loss 1.390152\n",
      "iteration 9100 / 14000: loss 1.130723\n",
      "iteration 9200 / 14000: loss 1.105458\n",
      "iteration 9300 / 14000: loss 1.180164\n",
      "epoch done... acc 0.5\n",
      "iteration 9400 / 14000: loss 1.158383\n",
      "iteration 9500 / 14000: loss 1.297311\n",
      "iteration 9600 / 14000: loss 1.146508\n",
      "iteration 9700 / 14000: loss 1.048528\n",
      "iteration 9800 / 14000: loss 1.136602\n",
      "epoch done... acc 0.51\n",
      "iteration 9900 / 14000: loss 1.205148\n",
      "iteration 10000 / 14000: loss 1.123118\n",
      "iteration 10100 / 14000: loss 1.366634\n",
      "iteration 10200 / 14000: loss 1.136397\n",
      "epoch done... acc 0.501\n",
      "iteration 10300 / 14000: loss 1.177660\n",
      "iteration 10400 / 14000: loss 1.002179\n",
      "iteration 10500 / 14000: loss 1.109380\n",
      "iteration 10600 / 14000: loss 1.032467\n",
      "iteration 10700 / 14000: loss 1.420483\n",
      "epoch done... acc 0.507\n",
      "iteration 10800 / 14000: loss 1.149418\n",
      "iteration 10900 / 14000: loss 1.142681\n",
      "iteration 11000 / 14000: loss 1.135698\n",
      "iteration 11100 / 14000: loss 1.103936\n",
      "iteration 11200 / 14000: loss 1.136898\n",
      "epoch done... acc 0.512\n",
      "iteration 11300 / 14000: loss 1.028456\n",
      "iteration 11400 / 14000: loss 1.063244\n",
      "iteration 11500 / 14000: loss 0.932442\n",
      "iteration 11600 / 14000: loss 1.142682\n",
      "iteration 11700 / 14000: loss 1.127241\n",
      "epoch done... acc 0.512\n",
      "iteration 11800 / 14000: loss 1.025210\n",
      "iteration 11900 / 14000: loss 1.246540\n",
      "iteration 12000 / 14000: loss 1.316512\n",
      "iteration 12100 / 14000: loss 1.028373\n",
      "iteration 12200 / 14000: loss 1.063762\n",
      "epoch done... acc 0.518\n",
      "iteration 12300 / 14000: loss 1.008738\n",
      "iteration 12400 / 14000: loss 1.061644\n",
      "iteration 12500 / 14000: loss 1.070151\n",
      "iteration 12600 / 14000: loss 1.002879\n",
      "iteration 12700 / 14000: loss 1.152165\n",
      "epoch done... acc 0.516\n",
      "iteration 12800 / 14000: loss 0.870385\n",
      "iteration 12900 / 14000: loss 0.929967\n",
      "iteration 13000 / 14000: loss 1.094173\n",
      "iteration 13100 / 14000: loss 1.057307\n",
      "iteration 13200 / 14000: loss 1.111674\n",
      "epoch done... acc 0.509\n",
      "iteration 13300 / 14000: loss 0.962700\n",
      "iteration 13400 / 14000: loss 1.208498\n",
      "iteration 13500 / 14000: loss 1.075798\n",
      "iteration 13600 / 14000: loss 0.946096\n",
      "iteration 13700 / 14000: loss 1.145216\n",
      "epoch done... acc 0.522\n",
      "iteration 13800 / 14000: loss 1.025312\n",
      "iteration 13900 / 14000: loss 0.981654\n",
      "Final training loss:  0.9267122520917488\n",
      "Final validation loss:  1.3601905932766372\n",
      "Final validation accuracy:  0.522\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "1 0 0 290 14000 100 0.001 0.98 0.522\n",
      "iteration 0 / 14000: loss 2.302608\n",
      "epoch done... acc 0.164\n",
      "iteration 100 / 14000: loss 2.061033\n",
      "iteration 200 / 14000: loss 2.051310\n",
      "iteration 300 / 14000: loss 1.804495\n",
      "iteration 400 / 14000: loss 1.778525\n",
      "epoch done... acc 0.377\n",
      "iteration 500 / 14000: loss 1.898220\n",
      "iteration 600 / 14000: loss 1.650783\n",
      "iteration 700 / 14000: loss 1.755470\n",
      "iteration 800 / 14000: loss 1.632262\n",
      "iteration 900 / 14000: loss 1.557994\n",
      "epoch done... acc 0.421\n",
      "iteration 1000 / 14000: loss 1.494434\n",
      "iteration 1100 / 14000: loss 1.518044\n",
      "iteration 1200 / 14000: loss 1.661368\n",
      "iteration 1300 / 14000: loss 1.477523\n",
      "iteration 1400 / 14000: loss 1.475246\n",
      "epoch done... acc 0.438\n",
      "iteration 1500 / 14000: loss 1.617155\n",
      "iteration 1600 / 14000: loss 1.737364\n",
      "iteration 1700 / 14000: loss 1.591914\n",
      "iteration 1800 / 14000: loss 1.624551\n",
      "iteration 1900 / 14000: loss 1.393544\n",
      "epoch done... acc 0.45\n",
      "iteration 2000 / 14000: loss 1.350999\n",
      "iteration 2100 / 14000: loss 1.487709\n",
      "iteration 2200 / 14000: loss 1.457311\n",
      "iteration 2300 / 14000: loss 1.521725\n",
      "iteration 2400 / 14000: loss 1.347910\n",
      "epoch done... acc 0.455\n",
      "iteration 2500 / 14000: loss 1.370663\n",
      "iteration 2600 / 14000: loss 1.481577\n",
      "iteration 2700 / 14000: loss 1.338851\n",
      "iteration 2800 / 14000: loss 1.486381\n",
      "iteration 2900 / 14000: loss 1.558121\n",
      "epoch done... acc 0.478\n",
      "iteration 3000 / 14000: loss 1.568460\n",
      "iteration 3100 / 14000: loss 1.375433\n",
      "iteration 3200 / 14000: loss 1.475575\n",
      "iteration 3300 / 14000: loss 1.401920\n",
      "iteration 3400 / 14000: loss 1.452101\n",
      "epoch done... acc 0.493\n",
      "iteration 3500 / 14000: loss 1.418153\n",
      "iteration 3600 / 14000: loss 1.382607\n",
      "iteration 3700 / 14000: loss 1.405581\n",
      "iteration 3800 / 14000: loss 1.444060\n",
      "iteration 3900 / 14000: loss 1.533201\n",
      "epoch done... acc 0.504\n",
      "iteration 4000 / 14000: loss 1.283777\n",
      "iteration 4100 / 14000: loss 1.273612\n",
      "iteration 4200 / 14000: loss 1.405066\n",
      "iteration 4300 / 14000: loss 1.458877\n",
      "iteration 4400 / 14000: loss 1.299607\n",
      "epoch done... acc 0.482\n",
      "iteration 4500 / 14000: loss 1.307145\n",
      "iteration 4600 / 14000: loss 1.150337\n",
      "iteration 4700 / 14000: loss 1.277202\n",
      "iteration 4800 / 14000: loss 1.213167\n",
      "iteration 4900 / 14000: loss 1.247868\n",
      "epoch done... acc 0.491\n",
      "iteration 5000 / 14000: loss 1.668587\n",
      "iteration 5100 / 14000: loss 1.258191\n",
      "iteration 5200 / 14000: loss 1.238163\n",
      "iteration 5300 / 14000: loss 1.377502\n",
      "epoch done... acc 0.481\n",
      "iteration 5400 / 14000: loss 1.198019\n",
      "iteration 5500 / 14000: loss 1.397891\n",
      "iteration 5600 / 14000: loss 1.421508\n",
      "iteration 5700 / 14000: loss 1.310847\n",
      "iteration 5800 / 14000: loss 1.302033\n",
      "epoch done... acc 0.506\n",
      "iteration 5900 / 14000: loss 1.381392\n",
      "iteration 6000 / 14000: loss 1.371773\n",
      "iteration 6100 / 14000: loss 1.294629\n",
      "iteration 6200 / 14000: loss 1.172215\n",
      "iteration 6300 / 14000: loss 1.224505\n",
      "epoch done... acc 0.511\n",
      "iteration 6400 / 14000: loss 1.350015\n",
      "iteration 6500 / 14000: loss 1.316122\n",
      "iteration 6600 / 14000: loss 1.207757\n",
      "iteration 6700 / 14000: loss 1.167872\n",
      "iteration 6800 / 14000: loss 1.201680\n",
      "epoch done... acc 0.503\n",
      "iteration 6900 / 14000: loss 1.167426\n",
      "iteration 7000 / 14000: loss 1.314632\n",
      "iteration 7100 / 14000: loss 1.172623\n",
      "iteration 7200 / 14000: loss 1.111538\n",
      "iteration 7300 / 14000: loss 1.008200\n",
      "epoch done... acc 0.536\n",
      "iteration 7400 / 14000: loss 1.152117\n",
      "iteration 7500 / 14000: loss 1.071259\n",
      "iteration 7600 / 14000: loss 1.260878\n",
      "iteration 7700 / 14000: loss 1.614375\n",
      "iteration 7800 / 14000: loss 1.233855\n",
      "epoch done... acc 0.51\n",
      "iteration 7900 / 14000: loss 1.187073\n",
      "iteration 8000 / 14000: loss 1.328974\n",
      "iteration 8100 / 14000: loss 1.254277\n",
      "iteration 8200 / 14000: loss 1.231418\n",
      "iteration 8300 / 14000: loss 1.177337\n",
      "epoch done... acc 0.504\n",
      "iteration 8400 / 14000: loss 1.120265\n",
      "iteration 8500 / 14000: loss 1.308701\n",
      "iteration 8600 / 14000: loss 1.120952\n",
      "iteration 8700 / 14000: loss 1.458424\n",
      "iteration 8800 / 14000: loss 1.313808\n",
      "epoch done... acc 0.516\n",
      "iteration 8900 / 14000: loss 1.291770\n",
      "iteration 9000 / 14000: loss 1.316058\n",
      "iteration 9100 / 14000: loss 1.182790\n",
      "iteration 9200 / 14000: loss 1.218247\n",
      "iteration 9300 / 14000: loss 1.253936\n",
      "epoch done... acc 0.529\n",
      "iteration 9400 / 14000: loss 1.183104\n",
      "iteration 9500 / 14000: loss 1.039763\n",
      "iteration 9600 / 14000: loss 1.229531\n",
      "iteration 9700 / 14000: loss 1.154030\n",
      "iteration 9800 / 14000: loss 0.937938\n",
      "epoch done... acc 0.527\n",
      "iteration 9900 / 14000: loss 1.132051\n",
      "iteration 10000 / 14000: loss 1.130916\n",
      "iteration 10100 / 14000: loss 1.269471\n",
      "iteration 10200 / 14000: loss 1.355873\n",
      "epoch done... acc 0.515\n",
      "iteration 10300 / 14000: loss 1.213116\n",
      "iteration 10400 / 14000: loss 0.933920\n",
      "iteration 10500 / 14000: loss 1.141190\n",
      "iteration 10600 / 14000: loss 1.090923\n",
      "iteration 10700 / 14000: loss 1.058147\n",
      "epoch done... acc 0.51\n",
      "iteration 10800 / 14000: loss 1.107524\n",
      "iteration 10900 / 14000: loss 1.049886\n",
      "iteration 11000 / 14000: loss 1.047031\n",
      "iteration 11100 / 14000: loss 1.133899\n",
      "iteration 11200 / 14000: loss 1.229903\n",
      "epoch done... acc 0.521\n",
      "iteration 11300 / 14000: loss 0.975449\n",
      "iteration 11400 / 14000: loss 1.060647\n",
      "iteration 11500 / 14000: loss 1.048205\n",
      "iteration 11600 / 14000: loss 1.287096\n",
      "iteration 11700 / 14000: loss 1.116096\n",
      "epoch done... acc 0.509\n",
      "iteration 11800 / 14000: loss 1.135685\n",
      "iteration 11900 / 14000: loss 1.004670\n",
      "iteration 12000 / 14000: loss 1.060496\n",
      "iteration 12100 / 14000: loss 0.954331\n",
      "iteration 12200 / 14000: loss 1.081118\n",
      "epoch done... acc 0.538\n",
      "iteration 12300 / 14000: loss 1.014820\n",
      "iteration 12400 / 14000: loss 1.246960\n",
      "iteration 12500 / 14000: loss 1.057187\n",
      "iteration 12600 / 14000: loss 1.151670\n",
      "iteration 12700 / 14000: loss 1.144809\n",
      "epoch done... acc 0.51\n",
      "iteration 12800 / 14000: loss 1.050936\n",
      "iteration 12900 / 14000: loss 1.199212\n",
      "iteration 13000 / 14000: loss 1.184140\n",
      "iteration 13100 / 14000: loss 1.121927\n",
      "iteration 13200 / 14000: loss 1.100205\n",
      "epoch done... acc 0.528\n",
      "iteration 13300 / 14000: loss 1.050448\n",
      "iteration 13400 / 14000: loss 1.017341\n",
      "iteration 13500 / 14000: loss 1.068648\n",
      "iteration 13600 / 14000: loss 1.017573\n",
      "iteration 13700 / 14000: loss 1.051659\n",
      "epoch done... acc 0.538\n",
      "iteration 13800 / 14000: loss 1.200269\n",
      "iteration 13900 / 14000: loss 1.061755\n",
      "Final training loss:  1.111170134281022\n",
      "Final validation loss:  1.3727563199552133\n",
      "Final validation accuracy:  0.538\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "2 0 0 319 14000 100 0.001 0.98 0.532\n",
      "iteration 0 / 14000: loss 2.302611\n",
      "epoch done... acc 0.149\n",
      "iteration 100 / 14000: loss 2.109546\n",
      "iteration 200 / 14000: loss 1.880856\n",
      "iteration 300 / 14000: loss 1.826335\n",
      "iteration 400 / 14000: loss 1.871664\n",
      "epoch done... acc 0.377\n",
      "iteration 500 / 14000: loss 1.743419\n",
      "iteration 600 / 14000: loss 1.611734\n",
      "iteration 700 / 14000: loss 1.674267\n",
      "iteration 800 / 14000: loss 1.621188\n",
      "iteration 900 / 14000: loss 1.658144\n",
      "epoch done... acc 0.406\n",
      "iteration 1000 / 14000: loss 1.607883\n",
      "iteration 1100 / 14000: loss 1.485882\n",
      "iteration 1200 / 14000: loss 1.452193\n",
      "iteration 1300 / 14000: loss 1.528686\n",
      "iteration 1400 / 14000: loss 1.559605\n",
      "epoch done... acc 0.45\n",
      "iteration 1500 / 14000: loss 1.572868\n",
      "iteration 1600 / 14000: loss 1.593226\n",
      "iteration 1700 / 14000: loss 1.468533\n",
      "iteration 1800 / 14000: loss 1.586296\n",
      "iteration 1900 / 14000: loss 1.447369\n",
      "epoch done... acc 0.474\n",
      "iteration 2000 / 14000: loss 1.470726\n",
      "iteration 2100 / 14000: loss 1.608251\n",
      "iteration 2200 / 14000: loss 1.573594\n",
      "iteration 2300 / 14000: loss 1.458365\n",
      "iteration 2400 / 14000: loss 1.411670\n",
      "epoch done... acc 0.464\n",
      "iteration 2500 / 14000: loss 1.366827\n",
      "iteration 2600 / 14000: loss 1.308858\n",
      "iteration 2700 / 14000: loss 1.493371\n",
      "iteration 2800 / 14000: loss 1.391217\n",
      "iteration 2900 / 14000: loss 1.491055\n",
      "epoch done... acc 0.479\n",
      "iteration 3000 / 14000: loss 1.538387\n",
      "iteration 3100 / 14000: loss 1.456928\n",
      "iteration 3200 / 14000: loss 1.426431\n",
      "iteration 3300 / 14000: loss 1.204585\n",
      "iteration 3400 / 14000: loss 1.363504\n",
      "epoch done... acc 0.464\n",
      "iteration 3500 / 14000: loss 1.443043\n",
      "iteration 3600 / 14000: loss 1.379617\n",
      "iteration 3700 / 14000: loss 1.396552\n",
      "iteration 3800 / 14000: loss 1.389079\n",
      "iteration 3900 / 14000: loss 1.457574\n",
      "epoch done... acc 0.492\n",
      "iteration 4000 / 14000: loss 1.469731\n",
      "iteration 4100 / 14000: loss 1.556321\n",
      "iteration 4200 / 14000: loss 1.298374\n",
      "iteration 4300 / 14000: loss 1.195291\n",
      "iteration 4400 / 14000: loss 1.418290\n",
      "epoch done... acc 0.478\n",
      "iteration 4500 / 14000: loss 1.340960\n",
      "iteration 4600 / 14000: loss 1.469679\n",
      "iteration 4700 / 14000: loss 1.474042\n",
      "iteration 4800 / 14000: loss 1.426113\n",
      "iteration 4900 / 14000: loss 1.213115\n",
      "epoch done... acc 0.478\n",
      "iteration 5000 / 14000: loss 1.245610\n",
      "iteration 5100 / 14000: loss 1.245740\n",
      "iteration 5200 / 14000: loss 1.428769\n",
      "iteration 5300 / 14000: loss 1.326726\n",
      "epoch done... acc 0.503\n",
      "iteration 5400 / 14000: loss 1.287147\n",
      "iteration 5500 / 14000: loss 1.225797\n",
      "iteration 5600 / 14000: loss 1.287227\n",
      "iteration 5700 / 14000: loss 1.332650\n",
      "iteration 5800 / 14000: loss 1.320007\n",
      "epoch done... acc 0.516\n",
      "iteration 5900 / 14000: loss 1.291979\n",
      "iteration 6000 / 14000: loss 1.130186\n",
      "iteration 6100 / 14000: loss 1.170555\n",
      "iteration 6200 / 14000: loss 1.299024\n",
      "iteration 6300 / 14000: loss 1.203332\n",
      "epoch done... acc 0.5\n",
      "iteration 6400 / 14000: loss 1.129189\n",
      "iteration 6500 / 14000: loss 1.258866\n",
      "iteration 6600 / 14000: loss 1.120520\n",
      "iteration 6700 / 14000: loss 1.361381\n",
      "iteration 6800 / 14000: loss 1.347326\n",
      "epoch done... acc 0.508\n",
      "iteration 6900 / 14000: loss 1.411250\n",
      "iteration 7000 / 14000: loss 1.044683\n",
      "iteration 7100 / 14000: loss 1.182301\n",
      "iteration 7200 / 14000: loss 1.127158\n",
      "iteration 7300 / 14000: loss 1.136097\n",
      "epoch done... acc 0.504\n",
      "iteration 7400 / 14000: loss 1.230662\n",
      "iteration 7500 / 14000: loss 1.539480\n",
      "iteration 7600 / 14000: loss 1.144356\n",
      "iteration 7700 / 14000: loss 1.240206\n",
      "iteration 7800 / 14000: loss 1.148259\n",
      "epoch done... acc 0.513\n",
      "iteration 7900 / 14000: loss 1.275479\n",
      "iteration 8000 / 14000: loss 1.198696\n",
      "iteration 8100 / 14000: loss 1.307096\n",
      "iteration 8200 / 14000: loss 1.223347\n",
      "iteration 8300 / 14000: loss 1.100967\n",
      "epoch done... acc 0.51\n",
      "iteration 8400 / 14000: loss 1.099212\n",
      "iteration 8500 / 14000: loss 1.104772\n",
      "iteration 8600 / 14000: loss 0.999817\n",
      "iteration 8700 / 14000: loss 1.197535\n",
      "iteration 8800 / 14000: loss 1.040320\n",
      "epoch done... acc 0.52\n",
      "iteration 8900 / 14000: loss 1.258429\n",
      "iteration 9000 / 14000: loss 1.413161\n",
      "iteration 9100 / 14000: loss 1.095910\n",
      "iteration 9200 / 14000: loss 1.112133\n",
      "iteration 9300 / 14000: loss 1.009682\n",
      "epoch done... acc 0.517\n",
      "iteration 9400 / 14000: loss 1.296054\n",
      "iteration 9500 / 14000: loss 1.101958\n",
      "iteration 9600 / 14000: loss 1.213369\n",
      "iteration 9700 / 14000: loss 1.106301\n",
      "iteration 9800 / 14000: loss 1.175619\n",
      "epoch done... acc 0.514\n",
      "iteration 9900 / 14000: loss 1.142327\n",
      "iteration 10000 / 14000: loss 1.186014\n",
      "iteration 10100 / 14000: loss 1.238041\n",
      "iteration 10200 / 14000: loss 1.157818\n",
      "epoch done... acc 0.536\n",
      "iteration 10300 / 14000: loss 1.195751\n",
      "iteration 10400 / 14000: loss 1.098764\n",
      "iteration 10500 / 14000: loss 1.135855\n",
      "iteration 10600 / 14000: loss 1.170975\n",
      "iteration 10700 / 14000: loss 1.168906\n",
      "epoch done... acc 0.526\n",
      "iteration 10800 / 14000: loss 1.164449\n",
      "iteration 10900 / 14000: loss 1.014268\n",
      "iteration 11000 / 14000: loss 1.309677\n",
      "iteration 11100 / 14000: loss 1.236079\n",
      "iteration 11200 / 14000: loss 1.280820\n",
      "epoch done... acc 0.513\n",
      "iteration 11300 / 14000: loss 1.097001\n",
      "iteration 11400 / 14000: loss 1.154915\n",
      "iteration 11500 / 14000: loss 1.089597\n",
      "iteration 11600 / 14000: loss 0.995259\n",
      "iteration 11700 / 14000: loss 1.061301\n",
      "epoch done... acc 0.529\n",
      "iteration 11800 / 14000: loss 1.000060\n",
      "iteration 11900 / 14000: loss 1.108690\n",
      "iteration 12000 / 14000: loss 1.262334\n",
      "iteration 12100 / 14000: loss 1.233264\n",
      "iteration 12200 / 14000: loss 0.944437\n",
      "epoch done... acc 0.536\n",
      "iteration 12300 / 14000: loss 1.089544\n",
      "iteration 12400 / 14000: loss 1.086526\n",
      "iteration 12500 / 14000: loss 0.904472\n",
      "iteration 12600 / 14000: loss 0.974147\n",
      "iteration 12700 / 14000: loss 0.856172\n",
      "epoch done... acc 0.533\n",
      "iteration 12800 / 14000: loss 1.066750\n",
      "iteration 12900 / 14000: loss 1.047595\n",
      "iteration 13000 / 14000: loss 0.972421\n",
      "iteration 13100 / 14000: loss 1.054455\n",
      "iteration 13200 / 14000: loss 0.812675\n",
      "epoch done... acc 0.526\n",
      "iteration 13300 / 14000: loss 1.027453\n",
      "iteration 13400 / 14000: loss 0.984410\n",
      "iteration 13500 / 14000: loss 1.136559\n",
      "iteration 13600 / 14000: loss 0.896469\n",
      "iteration 13700 / 14000: loss 1.032895\n",
      "epoch done... acc 0.536\n",
      "iteration 13800 / 14000: loss 0.996694\n",
      "iteration 13900 / 14000: loss 0.991935\n",
      "Final training loss:  1.07767115777041\n",
      "Final validation loss:  1.3582875979068554\n",
      "Final validation accuracy:  0.536\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "3 0 0 350 14000 100 0.001 0.98 0.536\n",
      "iteration 0 / 14000: loss 2.302552\n",
      "epoch done... acc 0.147\n",
      "iteration 100 / 14000: loss 1.958776\n",
      "iteration 200 / 14000: loss 1.866814\n",
      "iteration 300 / 14000: loss 1.925726\n",
      "iteration 400 / 14000: loss 1.853757\n",
      "epoch done... acc 0.394\n",
      "iteration 500 / 14000: loss 1.855707\n",
      "iteration 600 / 14000: loss 1.731284\n",
      "iteration 700 / 14000: loss 1.950813\n",
      "iteration 800 / 14000: loss 1.613219\n",
      "iteration 900 / 14000: loss 1.581362\n",
      "epoch done... acc 0.426\n",
      "iteration 1000 / 14000: loss 1.643231\n",
      "iteration 1100 / 14000: loss 1.445399\n",
      "iteration 1200 / 14000: loss 1.567929\n",
      "iteration 1300 / 14000: loss 1.649630\n",
      "iteration 1400 / 14000: loss 1.510376\n",
      "epoch done... acc 0.461\n",
      "iteration 1500 / 14000: loss 1.341364\n",
      "iteration 1600 / 14000: loss 1.455954\n",
      "iteration 1700 / 14000: loss 1.495965\n",
      "iteration 1800 / 14000: loss 1.501204\n",
      "iteration 1900 / 14000: loss 1.363487\n",
      "epoch done... acc 0.471\n",
      "iteration 2000 / 14000: loss 1.535591\n",
      "iteration 2100 / 14000: loss 1.445970\n",
      "iteration 2200 / 14000: loss 1.525602\n",
      "iteration 2300 / 14000: loss 1.395513\n",
      "iteration 2400 / 14000: loss 1.492211\n",
      "epoch done... acc 0.484\n",
      "iteration 2500 / 14000: loss 1.599588\n",
      "iteration 2600 / 14000: loss 1.425445\n",
      "iteration 2700 / 14000: loss 1.478277\n",
      "iteration 2800 / 14000: loss 1.316467\n",
      "iteration 2900 / 14000: loss 1.270431\n",
      "epoch done... acc 0.477\n",
      "iteration 3000 / 14000: loss 1.591858\n",
      "iteration 3100 / 14000: loss 1.414709\n",
      "iteration 3200 / 14000: loss 1.338316\n",
      "iteration 3300 / 14000: loss 1.509288\n",
      "iteration 3400 / 14000: loss 1.550319\n",
      "epoch done... acc 0.488\n",
      "iteration 3500 / 14000: loss 1.294539\n",
      "iteration 3600 / 14000: loss 1.581025\n",
      "iteration 3700 / 14000: loss 1.429323\n",
      "iteration 3800 / 14000: loss 1.352792\n",
      "iteration 3900 / 14000: loss 1.370759\n",
      "epoch done... acc 0.491\n",
      "iteration 4000 / 14000: loss 1.472905\n",
      "iteration 4100 / 14000: loss 1.300532\n",
      "iteration 4200 / 14000: loss 1.195028\n",
      "iteration 4300 / 14000: loss 1.275899\n",
      "iteration 4400 / 14000: loss 1.256062\n",
      "epoch done... acc 0.496\n",
      "iteration 4500 / 14000: loss 1.286731\n",
      "iteration 4600 / 14000: loss 1.302407\n",
      "iteration 4700 / 14000: loss 1.274499\n",
      "iteration 4800 / 14000: loss 1.167449\n",
      "iteration 4900 / 14000: loss 1.230319\n",
      "epoch done... acc 0.495\n",
      "iteration 5000 / 14000: loss 1.301895\n",
      "iteration 5100 / 14000: loss 1.556185\n",
      "iteration 5200 / 14000: loss 1.235371\n",
      "iteration 5300 / 14000: loss 1.444401\n",
      "epoch done... acc 0.504\n",
      "iteration 5400 / 14000: loss 1.417128\n",
      "iteration 5500 / 14000: loss 1.258091\n",
      "iteration 5600 / 14000: loss 1.386369\n",
      "iteration 5700 / 14000: loss 1.213651\n",
      "iteration 5800 / 14000: loss 1.173359\n",
      "epoch done... acc 0.518\n",
      "iteration 5900 / 14000: loss 1.154512\n",
      "iteration 6000 / 14000: loss 1.204417\n",
      "iteration 6100 / 14000: loss 1.128825\n",
      "iteration 6200 / 14000: loss 1.107396\n",
      "iteration 6300 / 14000: loss 1.199454\n",
      "epoch done... acc 0.494\n",
      "iteration 6400 / 14000: loss 1.136491\n",
      "iteration 6500 / 14000: loss 1.241237\n",
      "iteration 6600 / 14000: loss 1.505855\n",
      "iteration 6700 / 14000: loss 1.174139\n",
      "iteration 6800 / 14000: loss 1.147690\n",
      "epoch done... acc 0.502\n",
      "iteration 6900 / 14000: loss 1.238340\n",
      "iteration 7000 / 14000: loss 1.243707\n",
      "iteration 7100 / 14000: loss 1.232559\n",
      "iteration 7200 / 14000: loss 1.155333\n",
      "iteration 7300 / 14000: loss 1.279400\n",
      "epoch done... acc 0.511\n",
      "iteration 7400 / 14000: loss 1.036048\n",
      "iteration 7500 / 14000: loss 1.135331\n",
      "iteration 7600 / 14000: loss 1.280521\n",
      "iteration 7700 / 14000: loss 1.390673\n",
      "iteration 7800 / 14000: loss 1.199146\n",
      "epoch done... acc 0.524\n",
      "iteration 7900 / 14000: loss 1.339763\n",
      "iteration 8000 / 14000: loss 1.184183\n",
      "iteration 8100 / 14000: loss 1.179811\n",
      "iteration 8200 / 14000: loss 1.247212\n",
      "iteration 8300 / 14000: loss 1.293205\n",
      "epoch done... acc 0.506\n",
      "iteration 8400 / 14000: loss 1.270184\n",
      "iteration 8500 / 14000: loss 1.046546\n",
      "iteration 8600 / 14000: loss 1.235460\n",
      "iteration 8700 / 14000: loss 1.178618\n",
      "iteration 8800 / 14000: loss 1.215700\n",
      "epoch done... acc 0.515\n",
      "iteration 8900 / 14000: loss 1.209860\n",
      "iteration 9000 / 14000: loss 1.225192\n",
      "iteration 9100 / 14000: loss 1.177640\n",
      "iteration 9200 / 14000: loss 1.232410\n",
      "iteration 9300 / 14000: loss 1.211682\n",
      "epoch done... acc 0.509\n",
      "iteration 9400 / 14000: loss 1.249123\n",
      "iteration 9500 / 14000: loss 1.051705\n",
      "iteration 9600 / 14000: loss 1.145535\n",
      "iteration 9700 / 14000: loss 1.150386\n",
      "iteration 9800 / 14000: loss 1.159871\n",
      "epoch done... acc 0.51\n",
      "iteration 9900 / 14000: loss 1.025885\n",
      "iteration 10000 / 14000: loss 1.311696\n",
      "iteration 10100 / 14000: loss 1.010285\n",
      "iteration 10200 / 14000: loss 1.033371\n",
      "epoch done... acc 0.512\n",
      "iteration 10300 / 14000: loss 0.970868\n",
      "iteration 10400 / 14000: loss 1.019200\n",
      "iteration 10500 / 14000: loss 1.185476\n",
      "iteration 10600 / 14000: loss 1.132250\n",
      "iteration 10700 / 14000: loss 1.070851\n",
      "epoch done... acc 0.515\n",
      "iteration 10800 / 14000: loss 1.001620\n",
      "iteration 10900 / 14000: loss 1.094357\n",
      "iteration 11000 / 14000: loss 0.998164\n",
      "iteration 11100 / 14000: loss 1.056446\n",
      "iteration 11200 / 14000: loss 0.985551\n",
      "epoch done... acc 0.521\n",
      "iteration 11300 / 14000: loss 1.287993\n",
      "iteration 11400 / 14000: loss 1.221146\n",
      "iteration 11500 / 14000: loss 1.010997\n",
      "iteration 11600 / 14000: loss 1.098623\n",
      "iteration 11700 / 14000: loss 1.127794\n",
      "epoch done... acc 0.502\n",
      "iteration 11800 / 14000: loss 1.078891\n",
      "iteration 11900 / 14000: loss 1.163495\n",
      "iteration 12000 / 14000: loss 0.991723\n",
      "iteration 12100 / 14000: loss 1.001084\n",
      "iteration 12200 / 14000: loss 1.071502\n",
      "epoch done... acc 0.535\n",
      "iteration 12300 / 14000: loss 0.953943\n",
      "iteration 12400 / 14000: loss 1.043619\n",
      "iteration 12500 / 14000: loss 0.959321\n",
      "iteration 12600 / 14000: loss 1.123258\n",
      "iteration 12700 / 14000: loss 0.970436\n",
      "epoch done... acc 0.516\n",
      "iteration 12800 / 14000: loss 1.202322\n",
      "iteration 12900 / 14000: loss 1.053392\n",
      "iteration 13000 / 14000: loss 0.938615\n",
      "iteration 13100 / 14000: loss 1.124058\n",
      "iteration 13200 / 14000: loss 1.146470\n",
      "epoch done... acc 0.515\n",
      "iteration 13300 / 14000: loss 1.015057\n",
      "iteration 13400 / 14000: loss 0.901617\n",
      "iteration 13500 / 14000: loss 1.117571\n",
      "iteration 13600 / 14000: loss 1.042313\n",
      "iteration 13700 / 14000: loss 1.049283\n",
      "epoch done... acc 0.534\n",
      "iteration 13800 / 14000: loss 0.938983\n",
      "iteration 13900 / 14000: loss 0.866881\n",
      "Final training loss:  0.920893991914646\n",
      "Final validation loss:  1.359657801306575\n",
      "Final validation accuracy:  0.534\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "4 1 0 350 14000 100 0.001 0.98 0.534\n",
      "iteration 0 / 14000: loss 2.302621\n",
      "epoch done... acc 0.111\n",
      "iteration 100 / 14000: loss 2.050714\n",
      "iteration 200 / 14000: loss 1.893366\n",
      "iteration 300 / 14000: loss 1.833821\n",
      "iteration 400 / 14000: loss 1.764110\n",
      "epoch done... acc 0.37\n",
      "iteration 500 / 14000: loss 1.679571\n",
      "iteration 600 / 14000: loss 1.752132\n",
      "iteration 700 / 14000: loss 1.731390\n",
      "iteration 800 / 14000: loss 1.745243\n",
      "iteration 900 / 14000: loss 1.605949\n",
      "epoch done... acc 0.418\n",
      "iteration 1000 / 14000: loss 1.637720\n",
      "iteration 1100 / 14000: loss 1.679462\n",
      "iteration 1200 / 14000: loss 1.673400\n",
      "iteration 1300 / 14000: loss 1.586076\n",
      "iteration 1400 / 14000: loss 1.663218\n",
      "epoch done... acc 0.438\n",
      "iteration 1500 / 14000: loss 1.677277\n",
      "iteration 1600 / 14000: loss 1.522644\n",
      "iteration 1700 / 14000: loss 1.437395\n",
      "iteration 1800 / 14000: loss 1.645694\n",
      "iteration 1900 / 14000: loss 1.486198\n",
      "epoch done... acc 0.433\n",
      "iteration 2000 / 14000: loss 1.605835\n",
      "iteration 2100 / 14000: loss 1.576611\n",
      "iteration 2200 / 14000: loss 1.524324\n",
      "iteration 2300 / 14000: loss 1.421236\n",
      "iteration 2400 / 14000: loss 1.485731\n",
      "epoch done... acc 0.455\n",
      "iteration 2500 / 14000: loss 1.506278\n",
      "iteration 2600 / 14000: loss 1.542120\n",
      "iteration 2700 / 14000: loss 1.372465\n",
      "iteration 2800 / 14000: loss 1.429892\n",
      "iteration 2900 / 14000: loss 1.508737\n",
      "epoch done... acc 0.475\n",
      "iteration 3000 / 14000: loss 1.469510\n",
      "iteration 3100 / 14000: loss 1.397505\n",
      "iteration 3200 / 14000: loss 1.335286\n",
      "iteration 3300 / 14000: loss 1.415241\n",
      "iteration 3400 / 14000: loss 1.327420\n",
      "epoch done... acc 0.481\n",
      "iteration 3500 / 14000: loss 1.330172\n",
      "iteration 3600 / 14000: loss 1.196645\n",
      "iteration 3700 / 14000: loss 1.421957\n",
      "iteration 3800 / 14000: loss 1.406856\n",
      "iteration 3900 / 14000: loss 1.444951\n",
      "epoch done... acc 0.495\n",
      "iteration 4000 / 14000: loss 1.376805\n",
      "iteration 4100 / 14000: loss 1.435572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4200 / 14000: loss 1.382765\n",
      "iteration 4300 / 14000: loss 1.174006\n",
      "iteration 4400 / 14000: loss 1.356685\n",
      "epoch done... acc 0.498\n",
      "iteration 4500 / 14000: loss 1.328545\n",
      "iteration 4600 / 14000: loss 1.350792\n",
      "iteration 4700 / 14000: loss 1.511354\n",
      "iteration 4800 / 14000: loss 1.368640\n",
      "iteration 4900 / 14000: loss 1.161819\n",
      "epoch done... acc 0.499\n",
      "iteration 5000 / 14000: loss 1.307277\n",
      "iteration 5100 / 14000: loss 1.494216\n",
      "iteration 5200 / 14000: loss 1.280199\n",
      "iteration 5300 / 14000: loss 1.406620\n",
      "epoch done... acc 0.49\n",
      "iteration 5400 / 14000: loss 1.194780\n",
      "iteration 5500 / 14000: loss 1.202245\n",
      "iteration 5600 / 14000: loss 1.279304\n",
      "iteration 5700 / 14000: loss 1.272617\n",
      "iteration 5800 / 14000: loss 1.233333\n",
      "epoch done... acc 0.502\n",
      "iteration 5900 / 14000: loss 1.486121\n",
      "iteration 6000 / 14000: loss 1.329344\n",
      "iteration 6100 / 14000: loss 1.259596\n",
      "iteration 6200 / 14000: loss 1.307251\n",
      "iteration 6300 / 14000: loss 1.225808\n",
      "epoch done... acc 0.507\n",
      "iteration 6400 / 14000: loss 1.132373\n",
      "iteration 6500 / 14000: loss 1.282039\n",
      "iteration 6600 / 14000: loss 1.364077\n",
      "iteration 6700 / 14000: loss 1.335619\n",
      "iteration 6800 / 14000: loss 1.349255\n",
      "epoch done... acc 0.505\n",
      "iteration 6900 / 14000: loss 1.234836\n",
      "iteration 7000 / 14000: loss 1.160360\n",
      "iteration 7100 / 14000: loss 1.113136\n",
      "iteration 7200 / 14000: loss 1.337312\n",
      "iteration 7300 / 14000: loss 1.139675\n",
      "epoch done... acc 0.51\n",
      "iteration 7400 / 14000: loss 1.320058\n",
      "iteration 7500 / 14000: loss 1.313852\n",
      "iteration 7600 / 14000: loss 1.209174\n",
      "iteration 7700 / 14000: loss 1.193418\n",
      "iteration 7800 / 14000: loss 1.240001\n",
      "epoch done... acc 0.518\n",
      "iteration 7900 / 14000: loss 1.264509\n",
      "iteration 8000 / 14000: loss 1.288268\n",
      "iteration 8100 / 14000: loss 1.164480\n",
      "iteration 8200 / 14000: loss 1.030641\n",
      "iteration 8300 / 14000: loss 1.197244\n",
      "epoch done... acc 0.514\n",
      "iteration 8400 / 14000: loss 1.050640\n",
      "iteration 8500 / 14000: loss 1.252684\n",
      "iteration 8600 / 14000: loss 1.234421\n",
      "iteration 8700 / 14000: loss 1.391085\n",
      "iteration 8800 / 14000: loss 1.237456\n",
      "epoch done... acc 0.525\n",
      "iteration 8900 / 14000: loss 1.238929\n",
      "iteration 9000 / 14000: loss 1.197199\n",
      "iteration 9100 / 14000: loss 1.303997\n",
      "iteration 9200 / 14000: loss 1.083262\n",
      "iteration 9300 / 14000: loss 1.217539\n",
      "epoch done... acc 0.522\n",
      "iteration 9400 / 14000: loss 1.217064\n",
      "iteration 9500 / 14000: loss 1.213592\n",
      "iteration 9600 / 14000: loss 1.189322\n",
      "iteration 9700 / 14000: loss 1.168513\n",
      "iteration 9800 / 14000: loss 1.328464\n",
      "epoch done... acc 0.522\n",
      "iteration 9900 / 14000: loss 1.061508\n",
      "iteration 10000 / 14000: loss 1.080376\n",
      "iteration 10100 / 14000: loss 1.227677\n",
      "iteration 10200 / 14000: loss 1.011854\n",
      "epoch done... acc 0.525\n",
      "iteration 10300 / 14000: loss 1.238921\n",
      "iteration 10400 / 14000: loss 1.357685\n",
      "iteration 10500 / 14000: loss 1.096942\n",
      "iteration 10600 / 14000: loss 1.153491\n",
      "iteration 10700 / 14000: loss 1.230586\n",
      "epoch done... acc 0.533\n",
      "iteration 10800 / 14000: loss 1.136858\n",
      "iteration 10900 / 14000: loss 1.219773\n",
      "iteration 11000 / 14000: loss 1.379265\n",
      "iteration 11100 / 14000: loss 1.204608\n",
      "iteration 11200 / 14000: loss 1.196444\n",
      "epoch done... acc 0.521\n",
      "iteration 11300 / 14000: loss 1.182517\n",
      "iteration 11400 / 14000: loss 1.187539\n",
      "iteration 11500 / 14000: loss 0.926309\n",
      "iteration 11600 / 14000: loss 1.031017\n",
      "iteration 11700 / 14000: loss 1.103150\n",
      "epoch done... acc 0.529\n",
      "iteration 11800 / 14000: loss 0.995763\n",
      "iteration 11900 / 14000: loss 0.964449\n",
      "iteration 12000 / 14000: loss 1.114502\n",
      "iteration 12100 / 14000: loss 1.243605\n",
      "iteration 12200 / 14000: loss 1.156602\n",
      "epoch done... acc 0.519\n",
      "iteration 12300 / 14000: loss 0.972145\n",
      "iteration 12400 / 14000: loss 1.188444\n",
      "iteration 12500 / 14000: loss 1.179728\n",
      "iteration 12600 / 14000: loss 1.062638\n",
      "iteration 12700 / 14000: loss 1.008231\n",
      "epoch done... acc 0.518\n",
      "iteration 12800 / 14000: loss 0.984047\n",
      "iteration 12900 / 14000: loss 0.947478\n",
      "iteration 13000 / 14000: loss 0.980249\n",
      "iteration 13100 / 14000: loss 1.103719\n",
      "iteration 13200 / 14000: loss 1.114651\n",
      "epoch done... acc 0.536\n",
      "iteration 13300 / 14000: loss 1.104585\n",
      "iteration 13400 / 14000: loss 1.185687\n",
      "iteration 13500 / 14000: loss 1.185674\n",
      "iteration 13600 / 14000: loss 0.967217\n",
      "iteration 13700 / 14000: loss 0.938143\n",
      "epoch done... acc 0.525\n",
      "iteration 13800 / 14000: loss 1.142560\n",
      "iteration 13900 / 14000: loss 1.035879\n",
      "Final training loss:  1.142080151009631\n",
      "Final validation loss:  1.3698784200397713\n",
      "Final validation accuracy:  0.525\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "5 2 0 350 14000 100 0.001 0.98 0.525\n",
      "iteration 0 / 15400: loss 2.302550\n",
      "epoch done... acc 0.16\n",
      "iteration 100 / 15400: loss 2.118076\n",
      "iteration 200 / 15400: loss 1.917947\n",
      "iteration 300 / 15400: loss 1.943522\n",
      "iteration 400 / 15400: loss 1.821620\n",
      "epoch done... acc 0.374\n",
      "iteration 500 / 15400: loss 1.596337\n",
      "iteration 600 / 15400: loss 1.826170\n",
      "iteration 700 / 15400: loss 1.651941\n",
      "iteration 800 / 15400: loss 1.608411\n",
      "iteration 900 / 15400: loss 1.605964\n",
      "epoch done... acc 0.424\n",
      "iteration 1000 / 15400: loss 1.765073\n",
      "iteration 1100 / 15400: loss 1.722869\n",
      "iteration 1200 / 15400: loss 1.524612\n",
      "iteration 1300 / 15400: loss 1.605861\n",
      "iteration 1400 / 15400: loss 1.600743\n",
      "epoch done... acc 0.437\n",
      "iteration 1500 / 15400: loss 1.536401\n",
      "iteration 1600 / 15400: loss 1.730374\n",
      "iteration 1700 / 15400: loss 1.396617\n",
      "iteration 1800 / 15400: loss 1.506663\n",
      "iteration 1900 / 15400: loss 1.553006\n",
      "epoch done... acc 0.469\n",
      "iteration 2000 / 15400: loss 1.687189\n",
      "iteration 2100 / 15400: loss 1.491658\n",
      "iteration 2200 / 15400: loss 1.626805\n",
      "iteration 2300 / 15400: loss 1.396472\n",
      "iteration 2400 / 15400: loss 1.521418\n",
      "epoch done... acc 0.458\n",
      "iteration 2500 / 15400: loss 1.597111\n",
      "iteration 2600 / 15400: loss 1.262226\n",
      "iteration 2700 / 15400: loss 1.487428\n",
      "iteration 2800 / 15400: loss 1.499272\n",
      "iteration 2900 / 15400: loss 1.230364\n",
      "epoch done... acc 0.468\n",
      "iteration 3000 / 15400: loss 1.463602\n",
      "iteration 3100 / 15400: loss 1.489724\n",
      "iteration 3200 / 15400: loss 1.387832\n",
      "iteration 3300 / 15400: loss 1.498710\n",
      "iteration 3400 / 15400: loss 1.465318\n",
      "epoch done... acc 0.459\n",
      "iteration 3500 / 15400: loss 1.376693\n",
      "iteration 3600 / 15400: loss 1.361934\n",
      "iteration 3700 / 15400: loss 1.464514\n",
      "iteration 3800 / 15400: loss 1.297653\n",
      "iteration 3900 / 15400: loss 1.348229\n",
      "epoch done... acc 0.477\n",
      "iteration 4000 / 15400: loss 1.397237\n",
      "iteration 4100 / 15400: loss 1.448863\n",
      "iteration 4200 / 15400: loss 1.452553\n",
      "iteration 4300 / 15400: loss 1.366742\n",
      "iteration 4400 / 15400: loss 1.287587\n",
      "epoch done... acc 0.488\n",
      "iteration 4500 / 15400: loss 1.230021\n",
      "iteration 4600 / 15400: loss 1.300545\n",
      "iteration 4700 / 15400: loss 1.491194\n",
      "iteration 4800 / 15400: loss 1.252919\n",
      "iteration 4900 / 15400: loss 1.309603\n",
      "epoch done... acc 0.493\n",
      "iteration 5000 / 15400: loss 1.426269\n",
      "iteration 5100 / 15400: loss 1.307797\n",
      "iteration 5200 / 15400: loss 1.416799\n",
      "iteration 5300 / 15400: loss 1.272240\n",
      "epoch done... acc 0.505\n",
      "iteration 5400 / 15400: loss 1.257321\n",
      "iteration 5500 / 15400: loss 1.258185\n",
      "iteration 5600 / 15400: loss 1.236101\n",
      "iteration 5700 / 15400: loss 1.235574\n",
      "iteration 5800 / 15400: loss 1.433906\n",
      "epoch done... acc 0.492\n",
      "iteration 5900 / 15400: loss 1.352351\n",
      "iteration 6000 / 15400: loss 1.526492\n",
      "iteration 6100 / 15400: loss 1.106714\n",
      "iteration 6200 / 15400: loss 1.456493\n",
      "iteration 6300 / 15400: loss 1.309100\n",
      "epoch done... acc 0.509\n",
      "iteration 6400 / 15400: loss 1.245358\n",
      "iteration 6500 / 15400: loss 1.239771\n",
      "iteration 6600 / 15400: loss 1.162915\n",
      "iteration 6700 / 15400: loss 1.245023\n",
      "iteration 6800 / 15400: loss 1.110951\n",
      "epoch done... acc 0.517\n",
      "iteration 6900 / 15400: loss 1.020353\n",
      "iteration 7000 / 15400: loss 1.210342\n",
      "iteration 7100 / 15400: loss 1.088576\n",
      "iteration 7200 / 15400: loss 1.071633\n",
      "iteration 7300 / 15400: loss 1.046789\n",
      "epoch done... acc 0.506\n",
      "iteration 7400 / 15400: loss 1.285867\n",
      "iteration 7500 / 15400: loss 1.366804\n",
      "iteration 7600 / 15400: loss 1.259284\n",
      "iteration 7700 / 15400: loss 1.196699\n",
      "iteration 7800 / 15400: loss 1.125751\n",
      "epoch done... acc 0.513\n",
      "iteration 7900 / 15400: loss 1.154470\n",
      "iteration 8000 / 15400: loss 1.140784\n",
      "iteration 8100 / 15400: loss 1.140762\n",
      "iteration 8200 / 15400: loss 1.224651\n",
      "iteration 8300 / 15400: loss 1.122726\n",
      "epoch done... acc 0.506\n",
      "iteration 8400 / 15400: loss 1.372149\n",
      "iteration 8500 / 15400: loss 1.191416\n",
      "iteration 8600 / 15400: loss 1.151276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8700 / 15400: loss 1.273740\n",
      "iteration 8800 / 15400: loss 1.174846\n",
      "epoch done... acc 0.523\n",
      "iteration 8900 / 15400: loss 1.194382\n",
      "iteration 9000 / 15400: loss 1.331426\n",
      "iteration 9100 / 15400: loss 1.257377\n",
      "iteration 9200 / 15400: loss 1.269982\n",
      "iteration 9300 / 15400: loss 1.029790\n",
      "epoch done... acc 0.521\n",
      "iteration 9400 / 15400: loss 1.157865\n",
      "iteration 9500 / 15400: loss 1.076621\n",
      "iteration 9600 / 15400: loss 1.213876\n",
      "iteration 9700 / 15400: loss 1.238686\n",
      "iteration 9800 / 15400: loss 0.871326\n",
      "epoch done... acc 0.513\n",
      "iteration 9900 / 15400: loss 0.998439\n",
      "iteration 10000 / 15400: loss 1.188358\n",
      "iteration 10100 / 15400: loss 1.057629\n",
      "iteration 10200 / 15400: loss 1.063811\n",
      "epoch done... acc 0.513\n",
      "iteration 10300 / 15400: loss 1.241302\n",
      "iteration 10400 / 15400: loss 1.017755\n",
      "iteration 10500 / 15400: loss 1.083242\n",
      "iteration 10600 / 15400: loss 1.079511\n",
      "iteration 10700 / 15400: loss 1.053339\n",
      "epoch done... acc 0.515\n",
      "iteration 10800 / 15400: loss 1.067591\n",
      "iteration 10900 / 15400: loss 0.982316\n",
      "iteration 11000 / 15400: loss 1.206767\n",
      "iteration 11100 / 15400: loss 0.964764\n",
      "iteration 11200 / 15400: loss 1.200054\n",
      "epoch done... acc 0.512\n",
      "iteration 11300 / 15400: loss 1.182051\n",
      "iteration 11400 / 15400: loss 1.156428\n",
      "iteration 11500 / 15400: loss 1.193491\n",
      "iteration 11600 / 15400: loss 1.079421\n",
      "iteration 11700 / 15400: loss 1.213279\n",
      "epoch done... acc 0.513\n",
      "iteration 11800 / 15400: loss 1.255018\n",
      "iteration 11900 / 15400: loss 0.975937\n",
      "iteration 12000 / 15400: loss 1.185603\n",
      "iteration 12100 / 15400: loss 0.930113\n",
      "iteration 12200 / 15400: loss 1.159192\n",
      "epoch done... acc 0.511\n",
      "iteration 12300 / 15400: loss 1.119463\n",
      "iteration 12400 / 15400: loss 1.296687\n",
      "iteration 12500 / 15400: loss 1.112738\n",
      "iteration 12600 / 15400: loss 1.058523\n",
      "iteration 12700 / 15400: loss 0.943189\n",
      "epoch done... acc 0.514\n",
      "iteration 12800 / 15400: loss 1.013295\n",
      "iteration 12900 / 15400: loss 0.909043\n",
      "iteration 13000 / 15400: loss 1.120056\n",
      "iteration 13100 / 15400: loss 1.024279\n",
      "iteration 13200 / 15400: loss 0.925566\n",
      "epoch done... acc 0.508\n",
      "iteration 13300 / 15400: loss 1.074601\n",
      "iteration 13400 / 15400: loss 1.158197\n",
      "iteration 13500 / 15400: loss 1.108417\n",
      "iteration 13600 / 15400: loss 1.013893\n",
      "iteration 13700 / 15400: loss 0.980635\n",
      "epoch done... acc 0.518\n",
      "iteration 13800 / 15400: loss 0.850171\n",
      "iteration 13900 / 15400: loss 0.893732\n",
      "iteration 14000 / 15400: loss 1.135577\n",
      "iteration 14100 / 15400: loss 0.911695\n",
      "iteration 14200 / 15400: loss 1.120004\n",
      "epoch done... acc 0.51\n",
      "iteration 14300 / 15400: loss 1.134446\n",
      "iteration 14400 / 15400: loss 1.007075\n",
      "iteration 14500 / 15400: loss 0.905579\n",
      "iteration 14600 / 15400: loss 1.214815\n",
      "iteration 14700 / 15400: loss 1.071620\n",
      "epoch done... acc 0.519\n",
      "iteration 14800 / 15400: loss 0.957073\n",
      "iteration 14900 / 15400: loss 1.081438\n",
      "iteration 15000 / 15400: loss 0.903104\n",
      "iteration 15100 / 15400: loss 1.005799\n",
      "epoch done... acc 0.517\n",
      "iteration 15200 / 15400: loss 0.793917\n",
      "iteration 15300 / 15400: loss 0.929650\n",
      "Final training loss:  1.028980120293545\n",
      "Final validation loss:  1.3700003666855287\n",
      "Final validation accuracy:  0.517\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "6 1 1 350 14000 100 0.001 0.98 0.517\n",
      "iteration 0 / 12600: loss 2.302630\n",
      "epoch done... acc 0.124\n",
      "iteration 100 / 12600: loss 1.992536\n",
      "iteration 200 / 12600: loss 1.907627\n",
      "iteration 300 / 12600: loss 1.824863\n",
      "iteration 400 / 12600: loss 1.836836\n",
      "epoch done... acc 0.385\n",
      "iteration 500 / 12600: loss 1.817796\n",
      "iteration 600 / 12600: loss 1.744018\n",
      "iteration 700 / 12600: loss 1.674284\n",
      "iteration 800 / 12600: loss 1.627781\n",
      "iteration 900 / 12600: loss 1.672624\n",
      "epoch done... acc 0.42\n",
      "iteration 1000 / 12600: loss 1.591283\n",
      "iteration 1100 / 12600: loss 1.555717\n",
      "iteration 1200 / 12600: loss 1.685645\n",
      "iteration 1300 / 12600: loss 1.645112\n",
      "iteration 1400 / 12600: loss 1.613071\n",
      "epoch done... acc 0.447\n",
      "iteration 1500 / 12600: loss 1.544399\n",
      "iteration 1600 / 12600: loss 1.559869\n",
      "iteration 1700 / 12600: loss 1.467269\n",
      "iteration 1800 / 12600: loss 1.440304\n",
      "iteration 1900 / 12600: loss 1.459308\n",
      "epoch done... acc 0.456\n",
      "iteration 2000 / 12600: loss 1.561564\n",
      "iteration 2100 / 12600: loss 1.496711\n",
      "iteration 2200 / 12600: loss 1.435765\n",
      "iteration 2300 / 12600: loss 1.341103\n",
      "iteration 2400 / 12600: loss 1.606803\n",
      "epoch done... acc 0.467\n",
      "iteration 2500 / 12600: loss 1.591449\n",
      "iteration 2600 / 12600: loss 1.337839\n",
      "iteration 2700 / 12600: loss 1.288096\n",
      "iteration 2800 / 12600: loss 1.529766\n",
      "iteration 2900 / 12600: loss 1.716960\n",
      "epoch done... acc 0.452\n",
      "iteration 3000 / 12600: loss 1.457196\n",
      "iteration 3100 / 12600: loss 1.495659\n",
      "iteration 3200 / 12600: loss 1.516184\n",
      "iteration 3300 / 12600: loss 1.427741\n",
      "iteration 3400 / 12600: loss 1.380641\n",
      "epoch done... acc 0.483\n",
      "iteration 3500 / 12600: loss 1.358641\n",
      "iteration 3600 / 12600: loss 1.446063\n",
      "iteration 3700 / 12600: loss 1.272372\n",
      "iteration 3800 / 12600: loss 1.369441\n",
      "iteration 3900 / 12600: loss 1.415778\n",
      "epoch done... acc 0.492\n",
      "iteration 4000 / 12600: loss 1.221539\n",
      "iteration 4100 / 12600: loss 1.426303\n",
      "iteration 4200 / 12600: loss 1.345106\n",
      "iteration 4300 / 12600: loss 1.260073\n",
      "iteration 4400 / 12600: loss 1.250252\n",
      "epoch done... acc 0.506\n",
      "iteration 4500 / 12600: loss 1.378237\n",
      "iteration 4600 / 12600: loss 1.334422\n",
      "iteration 4700 / 12600: loss 1.268003\n",
      "iteration 4800 / 12600: loss 1.251538\n",
      "iteration 4900 / 12600: loss 1.220491\n",
      "epoch done... acc 0.496\n",
      "iteration 5000 / 12600: loss 1.298851\n",
      "iteration 5100 / 12600: loss 1.405821\n",
      "iteration 5200 / 12600: loss 1.453129\n",
      "iteration 5300 / 12600: loss 1.237508\n",
      "epoch done... acc 0.492\n",
      "iteration 5400 / 12600: loss 1.251330\n",
      "iteration 5500 / 12600: loss 1.230558\n",
      "iteration 5600 / 12600: loss 1.240212\n",
      "iteration 5700 / 12600: loss 1.334497\n",
      "iteration 5800 / 12600: loss 1.411538\n",
      "epoch done... acc 0.498\n",
      "iteration 5900 / 12600: loss 1.285074\n",
      "iteration 6000 / 12600: loss 1.188805\n",
      "iteration 6100 / 12600: loss 1.191873\n",
      "iteration 6200 / 12600: loss 1.232950\n",
      "iteration 6300 / 12600: loss 1.366267\n",
      "epoch done... acc 0.501\n",
      "iteration 6400 / 12600: loss 1.228506\n",
      "iteration 6500 / 12600: loss 1.327955\n",
      "iteration 6600 / 12600: loss 1.104307\n",
      "iteration 6700 / 12600: loss 1.119989\n",
      "iteration 6800 / 12600: loss 1.248662\n",
      "epoch done... acc 0.506\n",
      "iteration 6900 / 12600: loss 1.165822\n",
      "iteration 7000 / 12600: loss 1.308717\n",
      "iteration 7100 / 12600: loss 1.069834\n",
      "iteration 7200 / 12600: loss 1.101835\n",
      "iteration 7300 / 12600: loss 1.197802\n",
      "epoch done... acc 0.513\n",
      "iteration 7400 / 12600: loss 1.165744\n",
      "iteration 7500 / 12600: loss 1.172758\n",
      "iteration 7600 / 12600: loss 1.132419\n",
      "iteration 7700 / 12600: loss 1.228400\n",
      "iteration 7800 / 12600: loss 1.092968\n",
      "epoch done... acc 0.524\n",
      "iteration 7900 / 12600: loss 1.211859\n",
      "iteration 8000 / 12600: loss 1.004077\n",
      "iteration 8100 / 12600: loss 1.217203\n",
      "iteration 8200 / 12600: loss 1.064591\n",
      "iteration 8300 / 12600: loss 1.200887\n",
      "epoch done... acc 0.514\n",
      "iteration 8400 / 12600: loss 1.352879\n",
      "iteration 8500 / 12600: loss 1.228064\n",
      "iteration 8600 / 12600: loss 1.054025\n",
      "iteration 8700 / 12600: loss 1.135044\n",
      "iteration 8800 / 12600: loss 1.148387\n",
      "epoch done... acc 0.516\n",
      "iteration 8900 / 12600: loss 1.176167\n",
      "iteration 9000 / 12600: loss 1.173670\n",
      "iteration 9100 / 12600: loss 1.144381\n",
      "iteration 9200 / 12600: loss 1.400386\n",
      "iteration 9300 / 12600: loss 1.108794\n",
      "epoch done... acc 0.518\n",
      "iteration 9400 / 12600: loss 1.068829\n",
      "iteration 9500 / 12600: loss 1.090350\n",
      "iteration 9600 / 12600: loss 1.133187\n",
      "iteration 9700 / 12600: loss 1.013866\n",
      "iteration 9800 / 12600: loss 1.113846\n",
      "epoch done... acc 0.522\n",
      "iteration 9900 / 12600: loss 1.409727\n",
      "iteration 10000 / 12600: loss 1.188549\n",
      "iteration 10100 / 12600: loss 1.222982\n",
      "iteration 10200 / 12600: loss 1.054617\n",
      "epoch done... acc 0.519\n",
      "iteration 10300 / 12600: loss 1.122465\n",
      "iteration 10400 / 12600: loss 1.109542\n",
      "iteration 10500 / 12600: loss 1.086620\n",
      "iteration 10600 / 12600: loss 1.026033\n",
      "iteration 10700 / 12600: loss 1.003882\n",
      "epoch done... acc 0.515\n",
      "iteration 10800 / 12600: loss 1.137668\n",
      "iteration 10900 / 12600: loss 1.157313\n",
      "iteration 11000 / 12600: loss 1.212929\n",
      "iteration 11100 / 12600: loss 1.139355\n",
      "iteration 11200 / 12600: loss 1.031892\n",
      "epoch done... acc 0.528\n",
      "iteration 11300 / 12600: loss 1.019067\n",
      "iteration 11400 / 12600: loss 1.097546\n",
      "iteration 11500 / 12600: loss 1.173831\n",
      "iteration 11600 / 12600: loss 1.046437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 11700 / 12600: loss 1.044200\n",
      "epoch done... acc 0.531\n",
      "iteration 11800 / 12600: loss 0.976834\n",
      "iteration 11900 / 12600: loss 1.106850\n",
      "iteration 12000 / 12600: loss 0.973967\n",
      "iteration 12100 / 12600: loss 1.169762\n",
      "iteration 12200 / 12600: loss 1.101238\n",
      "epoch done... acc 0.53\n",
      "iteration 12300 / 12600: loss 1.070233\n",
      "iteration 12400 / 12600: loss 0.901155\n",
      "iteration 12500 / 12600: loss 1.073093\n",
      "Final training loss:  0.8828421388752135\n",
      "Final validation loss:  1.352841195149252\n",
      "Final validation accuracy:  0.53\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "7 2 1 350 14000 100 0.001 0.98 0.53\n",
      "iteration 0 / 14000: loss 2.302512\n",
      "epoch done... acc 0.112\n",
      "iteration 100 / 14000: loss 2.017645\n",
      "iteration 200 / 14000: loss 1.911080\n",
      "iteration 300 / 14000: loss 1.900255\n",
      "iteration 400 / 14000: loss 1.823656\n",
      "epoch done... acc 0.387\n",
      "iteration 500 / 14000: loss 1.683023\n",
      "iteration 600 / 14000: loss 1.788542\n",
      "iteration 700 / 14000: loss 1.671869\n",
      "iteration 800 / 14000: loss 1.592568\n",
      "iteration 900 / 14000: loss 1.608405\n",
      "epoch done... acc 0.435\n",
      "iteration 1000 / 14000: loss 1.645335\n",
      "iteration 1100 / 14000: loss 1.673718\n",
      "iteration 1200 / 14000: loss 1.648705\n",
      "iteration 1300 / 14000: loss 1.653519\n",
      "iteration 1400 / 14000: loss 1.909216\n",
      "epoch done... acc 0.454\n",
      "iteration 1500 / 14000: loss 1.699057\n",
      "iteration 1600 / 14000: loss 1.591775\n",
      "iteration 1700 / 14000: loss 1.605895\n",
      "iteration 1800 / 14000: loss 1.515830\n",
      "iteration 1900 / 14000: loss 1.601535\n",
      "epoch done... acc 0.464\n",
      "iteration 2000 / 14000: loss 1.433680\n",
      "iteration 2100 / 14000: loss 1.456851\n",
      "iteration 2200 / 14000: loss 1.352132\n",
      "iteration 2300 / 14000: loss 1.501307\n",
      "iteration 2400 / 14000: loss 1.317764\n",
      "epoch done... acc 0.475\n",
      "iteration 2500 / 14000: loss 1.691783\n",
      "iteration 2600 / 14000: loss 1.578088\n",
      "iteration 2700 / 14000: loss 1.452672\n",
      "iteration 2800 / 14000: loss 1.619309\n",
      "iteration 2900 / 14000: loss 1.497299\n",
      "epoch done... acc 0.475\n",
      "iteration 3000 / 14000: loss 1.339741\n",
      "iteration 3100 / 14000: loss 1.406673\n",
      "iteration 3200 / 14000: loss 1.416782\n",
      "iteration 3300 / 14000: loss 1.359461\n",
      "iteration 3400 / 14000: loss 1.454350\n",
      "epoch done... acc 0.497\n",
      "iteration 3500 / 14000: loss 1.276972\n",
      "iteration 3600 / 14000: loss 1.448531\n",
      "iteration 3700 / 14000: loss 1.347272\n",
      "iteration 3800 / 14000: loss 1.569901\n",
      "iteration 3900 / 14000: loss 1.364494\n",
      "epoch done... acc 0.469\n",
      "iteration 4000 / 14000: loss 1.208334\n",
      "iteration 4100 / 14000: loss 1.443420\n",
      "iteration 4200 / 14000: loss 1.359316\n",
      "iteration 4300 / 14000: loss 1.252433\n",
      "iteration 4400 / 14000: loss 1.381238\n",
      "epoch done... acc 0.498\n",
      "iteration 4500 / 14000: loss 1.440569\n",
      "iteration 4600 / 14000: loss 1.416804\n",
      "iteration 4700 / 14000: loss 1.287021\n",
      "iteration 4800 / 14000: loss 1.494919\n",
      "iteration 4900 / 14000: loss 1.192391\n",
      "epoch done... acc 0.499\n",
      "iteration 5000 / 14000: loss 1.362920\n",
      "iteration 5100 / 14000: loss 1.349577\n",
      "iteration 5200 / 14000: loss 1.191152\n",
      "iteration 5300 / 14000: loss 1.263401\n",
      "epoch done... acc 0.497\n",
      "iteration 5400 / 14000: loss 1.352930\n",
      "iteration 5500 / 14000: loss 1.256331\n",
      "iteration 5600 / 14000: loss 1.263912\n",
      "iteration 5700 / 14000: loss 1.428102\n",
      "iteration 5800 / 14000: loss 1.426488\n",
      "epoch done... acc 0.505\n",
      "iteration 5900 / 14000: loss 1.284245\n",
      "iteration 6000 / 14000: loss 1.206911\n",
      "iteration 6100 / 14000: loss 1.209853\n",
      "iteration 6200 / 14000: loss 1.114930\n",
      "iteration 6300 / 14000: loss 1.167020\n",
      "epoch done... acc 0.508\n",
      "iteration 6400 / 14000: loss 1.149888\n",
      "iteration 6500 / 14000: loss 1.238582\n",
      "iteration 6600 / 14000: loss 1.178508\n",
      "iteration 6700 / 14000: loss 1.037163\n",
      "iteration 6800 / 14000: loss 1.256593\n",
      "epoch done... acc 0.505\n",
      "iteration 6900 / 14000: loss 1.137703\n",
      "iteration 7000 / 14000: loss 1.170736\n",
      "iteration 7100 / 14000: loss 1.231759\n",
      "iteration 7200 / 14000: loss 1.273640\n",
      "iteration 7300 / 14000: loss 1.224478\n",
      "epoch done... acc 0.536\n",
      "iteration 7400 / 14000: loss 1.321511\n",
      "iteration 7500 / 14000: loss 1.116677\n",
      "iteration 7600 / 14000: loss 1.214088\n",
      "iteration 7700 / 14000: loss 1.133998\n",
      "iteration 7800 / 14000: loss 1.067416\n",
      "epoch done... acc 0.53\n",
      "iteration 7900 / 14000: loss 1.181541\n",
      "iteration 8000 / 14000: loss 1.130725\n",
      "iteration 8100 / 14000: loss 1.150360\n",
      "iteration 8200 / 14000: loss 1.188923\n",
      "iteration 8300 / 14000: loss 1.156777\n",
      "epoch done... acc 0.528\n",
      "iteration 8400 / 14000: loss 1.048194\n",
      "iteration 8500 / 14000: loss 1.175333\n",
      "iteration 8600 / 14000: loss 1.064133\n",
      "iteration 8700 / 14000: loss 1.192591\n",
      "iteration 8800 / 14000: loss 1.212250\n",
      "epoch done... acc 0.522\n",
      "iteration 8900 / 14000: loss 1.281057\n",
      "iteration 9000 / 14000: loss 1.254515\n",
      "iteration 9100 / 14000: loss 1.107092\n",
      "iteration 9200 / 14000: loss 1.104687\n",
      "iteration 9300 / 14000: loss 1.178386\n",
      "epoch done... acc 0.526\n",
      "iteration 9400 / 14000: loss 1.135719\n",
      "iteration 9500 / 14000: loss 0.946623\n",
      "iteration 9600 / 14000: loss 1.063656\n",
      "iteration 9700 / 14000: loss 1.141458\n",
      "iteration 9800 / 14000: loss 1.138829\n",
      "epoch done... acc 0.525\n",
      "iteration 9900 / 14000: loss 1.182758\n",
      "iteration 10000 / 14000: loss 1.286614\n",
      "iteration 10100 / 14000: loss 1.238816\n",
      "iteration 10200 / 14000: loss 1.263807\n",
      "epoch done... acc 0.533\n",
      "iteration 10300 / 14000: loss 1.128363\n",
      "iteration 10400 / 14000: loss 1.136239\n",
      "iteration 10500 / 14000: loss 1.103712\n",
      "iteration 10600 / 14000: loss 1.146745\n",
      "iteration 10700 / 14000: loss 1.159768\n",
      "epoch done... acc 0.537\n",
      "iteration 10800 / 14000: loss 1.096444\n",
      "iteration 10900 / 14000: loss 1.058251\n",
      "iteration 11000 / 14000: loss 0.958907\n",
      "iteration 11100 / 14000: loss 1.097742\n",
      "iteration 11200 / 14000: loss 1.086606\n",
      "epoch done... acc 0.548\n",
      "iteration 11300 / 14000: loss 1.142445\n",
      "iteration 11400 / 14000: loss 1.113719\n",
      "iteration 11500 / 14000: loss 1.282779\n",
      "iteration 11600 / 14000: loss 1.071795\n",
      "iteration 11700 / 14000: loss 1.018855\n",
      "epoch done... acc 0.532\n",
      "iteration 11800 / 14000: loss 1.079745\n",
      "iteration 11900 / 14000: loss 0.809884\n",
      "iteration 12000 / 14000: loss 1.075563\n",
      "iteration 12100 / 14000: loss 0.979156\n",
      "iteration 12200 / 14000: loss 1.090199\n",
      "epoch done... acc 0.542\n",
      "iteration 12300 / 14000: loss 1.160753\n",
      "iteration 12400 / 14000: loss 1.136459\n",
      "iteration 12500 / 14000: loss 0.976233\n",
      "iteration 12600 / 14000: loss 1.095794\n",
      "iteration 12700 / 14000: loss 1.168666\n",
      "epoch done... acc 0.546\n",
      "iteration 12800 / 14000: loss 1.269617\n",
      "iteration 12900 / 14000: loss 0.990128\n",
      "iteration 13000 / 14000: loss 0.917630\n",
      "iteration 13100 / 14000: loss 1.204429\n",
      "iteration 13200 / 14000: loss 1.087748\n",
      "epoch done... acc 0.529\n",
      "iteration 13300 / 14000: loss 1.025396\n",
      "iteration 13400 / 14000: loss 0.892256\n",
      "iteration 13500 / 14000: loss 1.043575\n",
      "iteration 13600 / 14000: loss 1.254451\n",
      "iteration 13700 / 14000: loss 1.056053\n",
      "epoch done... acc 0.544\n",
      "iteration 13800 / 14000: loss 1.153928\n",
      "iteration 13900 / 14000: loss 0.947643\n",
      "Final training loss:  0.9672919492006445\n",
      "Final validation loss:  1.3581256849158412\n",
      "Final validation accuracy:  0.544\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "8 0 0 385 14000 100 0.001 0.98 0.544\n",
      "iteration 0 / 14000: loss 2.302620\n",
      "epoch done... acc 0.176\n",
      "iteration 100 / 14000: loss 1.950410\n",
      "iteration 200 / 14000: loss 2.020879\n",
      "iteration 300 / 14000: loss 1.870669\n",
      "iteration 400 / 14000: loss 1.813449\n",
      "epoch done... acc 0.375\n",
      "iteration 500 / 14000: loss 1.858715\n",
      "iteration 600 / 14000: loss 1.629231\n",
      "iteration 700 / 14000: loss 1.586648\n",
      "iteration 800 / 14000: loss 1.567925\n",
      "iteration 900 / 14000: loss 1.800899\n",
      "epoch done... acc 0.419\n",
      "iteration 1000 / 14000: loss 1.682759\n",
      "iteration 1100 / 14000: loss 1.613732\n",
      "iteration 1200 / 14000: loss 1.594556\n",
      "iteration 1300 / 14000: loss 1.545685\n",
      "iteration 1400 / 14000: loss 1.477829\n",
      "epoch done... acc 0.451\n",
      "iteration 1500 / 14000: loss 1.524048\n",
      "iteration 1600 / 14000: loss 1.692795\n",
      "iteration 1700 / 14000: loss 1.595730\n",
      "iteration 1800 / 14000: loss 1.361464\n",
      "iteration 1900 / 14000: loss 1.431795\n",
      "epoch done... acc 0.451\n",
      "iteration 2000 / 14000: loss 1.425460\n",
      "iteration 2100 / 14000: loss 1.471476\n",
      "iteration 2200 / 14000: loss 1.330744\n",
      "iteration 2300 / 14000: loss 1.563756\n",
      "iteration 2400 / 14000: loss 1.589223\n",
      "epoch done... acc 0.453\n",
      "iteration 2500 / 14000: loss 1.522302\n",
      "iteration 2600 / 14000: loss 1.246331\n",
      "iteration 2700 / 14000: loss 1.380711\n",
      "iteration 2800 / 14000: loss 1.558218\n",
      "iteration 2900 / 14000: loss 1.459582\n",
      "epoch done... acc 0.47\n",
      "iteration 3000 / 14000: loss 1.448122\n",
      "iteration 3100 / 14000: loss 1.380154\n",
      "iteration 3200 / 14000: loss 1.403451\n",
      "iteration 3300 / 14000: loss 1.364822\n",
      "iteration 3400 / 14000: loss 1.469608\n",
      "epoch done... acc 0.476\n",
      "iteration 3500 / 14000: loss 1.469944\n",
      "iteration 3600 / 14000: loss 1.329255\n",
      "iteration 3700 / 14000: loss 1.417854\n",
      "iteration 3800 / 14000: loss 1.281088\n",
      "iteration 3900 / 14000: loss 1.668959\n",
      "epoch done... acc 0.487\n",
      "iteration 4000 / 14000: loss 1.378765\n",
      "iteration 4100 / 14000: loss 1.393671\n",
      "iteration 4200 / 14000: loss 1.276075\n",
      "iteration 4300 / 14000: loss 1.469895\n",
      "iteration 4400 / 14000: loss 1.581108\n",
      "epoch done... acc 0.472\n",
      "iteration 4500 / 14000: loss 1.299601\n",
      "iteration 4600 / 14000: loss 1.219806\n",
      "iteration 4700 / 14000: loss 1.235942\n",
      "iteration 4800 / 14000: loss 1.365569\n",
      "iteration 4900 / 14000: loss 1.187419\n",
      "epoch done... acc 0.476\n",
      "iteration 5000 / 14000: loss 1.346219\n",
      "iteration 5100 / 14000: loss 1.409155\n",
      "iteration 5200 / 14000: loss 1.216463\n",
      "iteration 5300 / 14000: loss 1.278147\n",
      "epoch done... acc 0.498\n",
      "iteration 5400 / 14000: loss 1.217071\n",
      "iteration 5500 / 14000: loss 1.355862\n",
      "iteration 5600 / 14000: loss 1.250630\n",
      "iteration 5700 / 14000: loss 1.350843\n",
      "iteration 5800 / 14000: loss 1.481801\n",
      "epoch done... acc 0.51\n",
      "iteration 5900 / 14000: loss 1.217681\n",
      "iteration 6000 / 14000: loss 1.089080\n",
      "iteration 6100 / 14000: loss 1.298012\n",
      "iteration 6200 / 14000: loss 1.200565\n",
      "iteration 6300 / 14000: loss 1.176769\n",
      "epoch done... acc 0.505\n",
      "iteration 6400 / 14000: loss 1.272280\n",
      "iteration 6500 / 14000: loss 1.211374\n",
      "iteration 6600 / 14000: loss 1.100662\n",
      "iteration 6700 / 14000: loss 1.341561\n",
      "iteration 6800 / 14000: loss 1.135530\n",
      "epoch done... acc 0.49\n",
      "iteration 6900 / 14000: loss 1.184717\n",
      "iteration 7000 / 14000: loss 1.241680\n",
      "iteration 7100 / 14000: loss 1.230749\n",
      "iteration 7200 / 14000: loss 1.279530\n",
      "iteration 7300 / 14000: loss 1.241333\n",
      "epoch done... acc 0.502\n",
      "iteration 7400 / 14000: loss 1.344431\n",
      "iteration 7500 / 14000: loss 1.342097\n",
      "iteration 7600 / 14000: loss 1.149778\n",
      "iteration 7700 / 14000: loss 1.136121\n",
      "iteration 7800 / 14000: loss 1.090052\n",
      "epoch done... acc 0.504\n",
      "iteration 7900 / 14000: loss 1.137239\n",
      "iteration 8000 / 14000: loss 1.117796\n",
      "iteration 8100 / 14000: loss 1.230705\n",
      "iteration 8200 / 14000: loss 1.195368\n",
      "iteration 8300 / 14000: loss 1.217638\n",
      "epoch done... acc 0.516\n",
      "iteration 8400 / 14000: loss 1.284843\n",
      "iteration 8500 / 14000: loss 1.237871\n",
      "iteration 8600 / 14000: loss 1.158275\n",
      "iteration 8700 / 14000: loss 1.250879\n",
      "iteration 8800 / 14000: loss 1.156851\n",
      "epoch done... acc 0.507\n",
      "iteration 8900 / 14000: loss 1.058671\n",
      "iteration 9000 / 14000: loss 1.221973\n",
      "iteration 9100 / 14000: loss 1.029459\n",
      "iteration 9200 / 14000: loss 1.021445\n",
      "iteration 9300 / 14000: loss 1.028999\n",
      "epoch done... acc 0.503\n",
      "iteration 9400 / 14000: loss 1.203936\n",
      "iteration 9500 / 14000: loss 1.181170\n",
      "iteration 9600 / 14000: loss 1.159786\n",
      "iteration 9700 / 14000: loss 0.979281\n",
      "iteration 9800 / 14000: loss 1.089817\n",
      "epoch done... acc 0.522\n",
      "iteration 9900 / 14000: loss 1.045277\n",
      "iteration 10000 / 14000: loss 1.103620\n",
      "iteration 10100 / 14000: loss 1.042694\n",
      "iteration 10200 / 14000: loss 1.146445\n",
      "epoch done... acc 0.514\n",
      "iteration 10300 / 14000: loss 1.164477\n",
      "iteration 10400 / 14000: loss 1.115062\n",
      "iteration 10500 / 14000: loss 1.118550\n",
      "iteration 10600 / 14000: loss 1.088602\n",
      "iteration 10700 / 14000: loss 1.220465\n",
      "epoch done... acc 0.508\n",
      "iteration 10800 / 14000: loss 1.058129\n",
      "iteration 10900 / 14000: loss 1.014824\n",
      "iteration 11000 / 14000: loss 1.308118\n",
      "iteration 11100 / 14000: loss 1.056352\n",
      "iteration 11200 / 14000: loss 1.049442\n",
      "epoch done... acc 0.528\n",
      "iteration 11300 / 14000: loss 0.912624\n",
      "iteration 11400 / 14000: loss 0.944690\n",
      "iteration 11500 / 14000: loss 1.031833\n",
      "iteration 11600 / 14000: loss 0.994918\n",
      "iteration 11700 / 14000: loss 0.992304\n",
      "epoch done... acc 0.513\n",
      "iteration 11800 / 14000: loss 1.132650\n",
      "iteration 11900 / 14000: loss 0.996038\n",
      "iteration 12000 / 14000: loss 1.003977\n",
      "iteration 12100 / 14000: loss 0.996729\n",
      "iteration 12200 / 14000: loss 0.993182\n",
      "epoch done... acc 0.529\n",
      "iteration 12300 / 14000: loss 1.138356\n",
      "iteration 12400 / 14000: loss 1.040274\n",
      "iteration 12500 / 14000: loss 0.996167\n",
      "iteration 12600 / 14000: loss 1.000854\n",
      "iteration 12700 / 14000: loss 1.064141\n",
      "epoch done... acc 0.528\n",
      "iteration 12800 / 14000: loss 1.020642\n",
      "iteration 12900 / 14000: loss 1.073508\n",
      "iteration 13000 / 14000: loss 1.006569\n",
      "iteration 13100 / 14000: loss 0.982546\n",
      "iteration 13200 / 14000: loss 0.923600\n",
      "epoch done... acc 0.527\n",
      "iteration 13300 / 14000: loss 0.892845\n",
      "iteration 13400 / 14000: loss 1.069113\n",
      "iteration 13500 / 14000: loss 0.962977\n",
      "iteration 13600 / 14000: loss 0.880182\n",
      "iteration 13700 / 14000: loss 0.895483\n",
      "epoch done... acc 0.512\n",
      "iteration 13800 / 14000: loss 0.995054\n",
      "iteration 13900 / 14000: loss 1.010135\n",
      "Final training loss:  0.9945373355171948\n",
      "Final validation loss:  1.3881740754254146\n",
      "Final validation accuracy:  0.512\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "9 1 0 385 14000 100 0.001 0.98 0.512\n",
      "iteration 0 / 14000: loss 2.302535\n",
      "epoch done... acc 0.162\n",
      "iteration 100 / 14000: loss 2.054690\n",
      "iteration 200 / 14000: loss 1.800508\n",
      "iteration 300 / 14000: loss 1.827911\n",
      "iteration 400 / 14000: loss 1.915523\n",
      "epoch done... acc 0.387\n",
      "iteration 500 / 14000: loss 1.712414\n",
      "iteration 600 / 14000: loss 1.835650\n",
      "iteration 700 / 14000: loss 1.730316\n",
      "iteration 800 / 14000: loss 1.532059\n",
      "iteration 900 / 14000: loss 1.665559\n",
      "epoch done... acc 0.422\n",
      "iteration 1000 / 14000: loss 1.756447\n",
      "iteration 1100 / 14000: loss 1.791400\n",
      "iteration 1200 / 14000: loss 1.549660\n",
      "iteration 1300 / 14000: loss 1.523370\n",
      "iteration 1400 / 14000: loss 1.684853\n",
      "epoch done... acc 0.439\n",
      "iteration 1500 / 14000: loss 1.616946\n",
      "iteration 1600 / 14000: loss 1.640514\n",
      "iteration 1700 / 14000: loss 1.476455\n",
      "iteration 1800 / 14000: loss 1.489173\n",
      "iteration 1900 / 14000: loss 1.722929\n",
      "epoch done... acc 0.463\n",
      "iteration 2000 / 14000: loss 1.355726\n",
      "iteration 2100 / 14000: loss 1.318999\n",
      "iteration 2200 / 14000: loss 1.560051\n",
      "iteration 2300 / 14000: loss 1.493511\n",
      "iteration 2400 / 14000: loss 1.446750\n",
      "epoch done... acc 0.46\n",
      "iteration 2500 / 14000: loss 1.657721\n",
      "iteration 2600 / 14000: loss 1.437845\n",
      "iteration 2700 / 14000: loss 1.443993\n",
      "iteration 2800 / 14000: loss 1.388365\n",
      "iteration 2900 / 14000: loss 1.434466\n",
      "epoch done... acc 0.476\n",
      "iteration 3000 / 14000: loss 1.411329\n",
      "iteration 3100 / 14000: loss 1.233596\n",
      "iteration 3200 / 14000: loss 1.582551\n",
      "iteration 3300 / 14000: loss 1.201470\n",
      "iteration 3400 / 14000: loss 1.359764\n",
      "epoch done... acc 0.47\n",
      "iteration 3500 / 14000: loss 1.355266\n",
      "iteration 3600 / 14000: loss 1.406627\n",
      "iteration 3700 / 14000: loss 1.340073\n",
      "iteration 3800 / 14000: loss 1.412273\n",
      "iteration 3900 / 14000: loss 1.335935\n",
      "epoch done... acc 0.491\n",
      "iteration 4000 / 14000: loss 1.486495\n",
      "iteration 4100 / 14000: loss 1.357469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4200 / 14000: loss 1.261494\n",
      "iteration 4300 / 14000: loss 1.269354\n",
      "iteration 4400 / 14000: loss 1.277938\n",
      "epoch done... acc 0.483\n",
      "iteration 4500 / 14000: loss 1.282952\n",
      "iteration 4600 / 14000: loss 1.039976\n",
      "iteration 4700 / 14000: loss 1.337556\n",
      "iteration 4800 / 14000: loss 1.246148\n",
      "iteration 4900 / 14000: loss 1.443100\n",
      "epoch done... acc 0.5\n",
      "iteration 5000 / 14000: loss 1.266091\n",
      "iteration 5100 / 14000: loss 1.237137\n",
      "iteration 5200 / 14000: loss 1.265031\n",
      "iteration 5300 / 14000: loss 1.431172\n",
      "epoch done... acc 0.506\n",
      "iteration 5400 / 14000: loss 1.404539\n",
      "iteration 5500 / 14000: loss 1.284354\n",
      "iteration 5600 / 14000: loss 1.469751\n",
      "iteration 5700 / 14000: loss 1.335842\n",
      "iteration 5800 / 14000: loss 1.200545\n",
      "epoch done... acc 0.503\n",
      "iteration 5900 / 14000: loss 1.262225\n",
      "iteration 6000 / 14000: loss 1.400782\n",
      "iteration 6100 / 14000: loss 1.255972\n",
      "iteration 6200 / 14000: loss 1.213969\n",
      "iteration 6300 / 14000: loss 1.276398\n",
      "epoch done... acc 0.5\n",
      "iteration 6400 / 14000: loss 1.229474\n",
      "iteration 6500 / 14000: loss 1.299292\n",
      "iteration 6600 / 14000: loss 1.109874\n",
      "iteration 6700 / 14000: loss 1.212924\n",
      "iteration 6800 / 14000: loss 1.342312\n",
      "epoch done... acc 0.492\n",
      "iteration 6900 / 14000: loss 1.114922\n",
      "iteration 7000 / 14000: loss 1.106087\n",
      "iteration 7100 / 14000: loss 1.038282\n",
      "iteration 7200 / 14000: loss 1.229718\n",
      "iteration 7300 / 14000: loss 1.165071\n",
      "epoch done... acc 0.502\n",
      "iteration 7400 / 14000: loss 1.189017\n",
      "iteration 7500 / 14000: loss 1.140178\n",
      "iteration 7600 / 14000: loss 1.378200\n",
      "iteration 7700 / 14000: loss 1.253958\n",
      "iteration 7800 / 14000: loss 1.278616\n",
      "epoch done... acc 0.515\n",
      "iteration 7900 / 14000: loss 1.078330\n",
      "iteration 8000 / 14000: loss 1.113436\n",
      "iteration 8100 / 14000: loss 1.219191\n",
      "iteration 8200 / 14000: loss 1.063173\n",
      "iteration 8300 / 14000: loss 1.098683\n",
      "epoch done... acc 0.519\n",
      "iteration 8400 / 14000: loss 1.182754\n",
      "iteration 8500 / 14000: loss 1.310066\n",
      "iteration 8600 / 14000: loss 1.142353\n",
      "iteration 8700 / 14000: loss 1.087554\n",
      "iteration 8800 / 14000: loss 1.089104\n",
      "epoch done... acc 0.505\n",
      "iteration 8900 / 14000: loss 1.107107\n",
      "iteration 9000 / 14000: loss 1.232833\n",
      "iteration 9100 / 14000: loss 1.150027\n",
      "iteration 9200 / 14000: loss 1.247975\n",
      "iteration 9300 / 14000: loss 1.088197\n",
      "epoch done... acc 0.519\n",
      "iteration 9400 / 14000: loss 1.151093\n",
      "iteration 9500 / 14000: loss 1.178628\n",
      "iteration 9600 / 14000: loss 1.147604\n",
      "iteration 9700 / 14000: loss 1.222652\n",
      "iteration 9800 / 14000: loss 1.160837\n",
      "epoch done... acc 0.492\n",
      "iteration 9900 / 14000: loss 1.089983\n",
      "iteration 10000 / 14000: loss 1.182364\n",
      "iteration 10100 / 14000: loss 0.913319\n",
      "iteration 10200 / 14000: loss 1.206242\n",
      "epoch done... acc 0.512\n",
      "iteration 10300 / 14000: loss 1.043390\n",
      "iteration 10400 / 14000: loss 0.993714\n",
      "iteration 10500 / 14000: loss 1.103878\n",
      "iteration 10600 / 14000: loss 1.139366\n",
      "iteration 10700 / 14000: loss 1.078119\n",
      "epoch done... acc 0.504\n",
      "iteration 10800 / 14000: loss 1.025956\n",
      "iteration 10900 / 14000: loss 1.119314\n",
      "iteration 11000 / 14000: loss 0.981478\n",
      "iteration 11100 / 14000: loss 1.154462\n",
      "iteration 11200 / 14000: loss 1.055190\n",
      "epoch done... acc 0.528\n",
      "iteration 11300 / 14000: loss 1.220801\n",
      "iteration 11400 / 14000: loss 1.078745\n",
      "iteration 11500 / 14000: loss 1.074594\n",
      "iteration 11600 / 14000: loss 0.941922\n",
      "iteration 11700 / 14000: loss 1.147726\n",
      "epoch done... acc 0.517\n",
      "iteration 11800 / 14000: loss 1.144181\n",
      "iteration 11900 / 14000: loss 1.007084\n",
      "iteration 12000 / 14000: loss 1.089352\n",
      "iteration 12100 / 14000: loss 1.276529\n",
      "iteration 12200 / 14000: loss 1.145215\n",
      "epoch done... acc 0.518\n",
      "iteration 12300 / 14000: loss 0.955287\n",
      "iteration 12400 / 14000: loss 1.136379\n",
      "iteration 12500 / 14000: loss 0.975458\n",
      "iteration 12600 / 14000: loss 1.028124\n",
      "iteration 12700 / 14000: loss 1.101702\n",
      "epoch done... acc 0.526\n",
      "iteration 12800 / 14000: loss 1.078317\n",
      "iteration 12900 / 14000: loss 1.148560\n",
      "iteration 13000 / 14000: loss 1.159293\n",
      "iteration 13100 / 14000: loss 1.230918\n",
      "iteration 13200 / 14000: loss 1.064109\n",
      "epoch done... acc 0.522\n",
      "iteration 13300 / 14000: loss 1.121443\n",
      "iteration 13400 / 14000: loss 0.910871\n",
      "iteration 13500 / 14000: loss 1.271616\n",
      "iteration 13600 / 14000: loss 1.276359\n",
      "iteration 13700 / 14000: loss 0.904838\n",
      "epoch done... acc 0.515\n",
      "iteration 13800 / 14000: loss 1.034690\n",
      "iteration 13900 / 14000: loss 0.982280\n",
      "Final training loss:  1.1611990953360058\n",
      "Final validation loss:  1.3777260810041754\n",
      "Final validation accuracy:  0.515\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "10 2 0 385 14000 100 0.001 0.98 0.515\n",
      "iteration 0 / 15400: loss 2.302635\n",
      "epoch done... acc 0.185\n",
      "iteration 100 / 15400: loss 1.977971\n",
      "iteration 200 / 15400: loss 1.951699\n",
      "iteration 300 / 15400: loss 1.770200\n",
      "iteration 400 / 15400: loss 1.866238\n",
      "epoch done... acc 0.379\n",
      "iteration 500 / 15400: loss 1.630124\n",
      "iteration 600 / 15400: loss 1.782722\n",
      "iteration 700 / 15400: loss 1.796775\n",
      "iteration 800 / 15400: loss 1.614234\n",
      "iteration 900 / 15400: loss 1.655155\n",
      "epoch done... acc 0.428\n",
      "iteration 1000 / 15400: loss 1.684411\n",
      "iteration 1100 / 15400: loss 1.806560\n",
      "iteration 1200 / 15400: loss 1.621180\n",
      "iteration 1300 / 15400: loss 1.669061\n",
      "iteration 1400 / 15400: loss 1.585262\n",
      "epoch done... acc 0.446\n",
      "iteration 1500 / 15400: loss 1.539049\n",
      "iteration 1600 / 15400: loss 1.767301\n",
      "iteration 1700 / 15400: loss 1.576572\n",
      "iteration 1800 / 15400: loss 1.514653\n",
      "iteration 1900 / 15400: loss 1.499322\n",
      "epoch done... acc 0.461\n",
      "iteration 2000 / 15400: loss 1.431249\n",
      "iteration 2100 / 15400: loss 1.365306\n",
      "iteration 2200 / 15400: loss 1.441753\n",
      "iteration 2300 / 15400: loss 1.404071\n",
      "iteration 2400 / 15400: loss 1.588389\n",
      "epoch done... acc 0.477\n",
      "iteration 2500 / 15400: loss 1.410848\n",
      "iteration 2600 / 15400: loss 1.518052\n",
      "iteration 2700 / 15400: loss 1.539992\n",
      "iteration 2800 / 15400: loss 1.519579\n",
      "iteration 2900 / 15400: loss 1.450574\n",
      "epoch done... acc 0.485\n",
      "iteration 3000 / 15400: loss 1.352256\n",
      "iteration 3100 / 15400: loss 1.362764\n",
      "iteration 3200 / 15400: loss 1.306409\n",
      "iteration 3300 / 15400: loss 1.673940\n",
      "iteration 3400 / 15400: loss 1.379896\n",
      "epoch done... acc 0.49\n",
      "iteration 3500 / 15400: loss 1.600696\n",
      "iteration 3600 / 15400: loss 1.430353\n",
      "iteration 3700 / 15400: loss 1.125132\n",
      "iteration 3800 / 15400: loss 1.418128\n",
      "iteration 3900 / 15400: loss 1.397470\n",
      "epoch done... acc 0.48\n",
      "iteration 4000 / 15400: loss 1.228753\n",
      "iteration 4100 / 15400: loss 1.307642\n",
      "iteration 4200 / 15400: loss 1.490366\n",
      "iteration 4300 / 15400: loss 1.333563\n",
      "iteration 4400 / 15400: loss 1.166551\n",
      "epoch done... acc 0.486\n",
      "iteration 4500 / 15400: loss 1.221730\n",
      "iteration 4600 / 15400: loss 1.407298\n",
      "iteration 4700 / 15400: loss 1.211848\n",
      "iteration 4800 / 15400: loss 1.259467\n",
      "iteration 4900 / 15400: loss 1.356250\n",
      "epoch done... acc 0.481\n",
      "iteration 5000 / 15400: loss 1.297375\n",
      "iteration 5100 / 15400: loss 1.292794\n",
      "iteration 5200 / 15400: loss 1.374995\n",
      "iteration 5300 / 15400: loss 1.369409\n",
      "epoch done... acc 0.498\n",
      "iteration 5400 / 15400: loss 1.324918\n",
      "iteration 5500 / 15400: loss 1.152467\n",
      "iteration 5600 / 15400: loss 1.065610\n",
      "iteration 5700 / 15400: loss 1.345470\n",
      "iteration 5800 / 15400: loss 1.251390\n",
      "epoch done... acc 0.502\n",
      "iteration 5900 / 15400: loss 1.123611\n",
      "iteration 6000 / 15400: loss 1.217682\n",
      "iteration 6100 / 15400: loss 1.107214\n",
      "iteration 6200 / 15400: loss 1.360305\n",
      "iteration 6300 / 15400: loss 1.287538\n",
      "epoch done... acc 0.502\n",
      "iteration 6400 / 15400: loss 1.254353\n",
      "iteration 6500 / 15400: loss 1.319938\n",
      "iteration 6600 / 15400: loss 1.444236\n",
      "iteration 6700 / 15400: loss 1.248354\n",
      "iteration 6800 / 15400: loss 1.183658\n",
      "epoch done... acc 0.514\n",
      "iteration 6900 / 15400: loss 1.337669\n",
      "iteration 7000 / 15400: loss 1.162328\n",
      "iteration 7100 / 15400: loss 1.346633\n",
      "iteration 7200 / 15400: loss 0.996487\n",
      "iteration 7300 / 15400: loss 1.086650\n",
      "epoch done... acc 0.509\n",
      "iteration 7400 / 15400: loss 1.320691\n",
      "iteration 7500 / 15400: loss 1.190774\n",
      "iteration 7600 / 15400: loss 1.223396\n",
      "iteration 7700 / 15400: loss 1.155554\n",
      "iteration 7800 / 15400: loss 1.152315\n",
      "epoch done... acc 0.511\n",
      "iteration 7900 / 15400: loss 1.269578\n",
      "iteration 8000 / 15400: loss 1.048255\n",
      "iteration 8100 / 15400: loss 0.977021\n",
      "iteration 8200 / 15400: loss 1.010502\n",
      "iteration 8300 / 15400: loss 1.199306\n",
      "epoch done... acc 0.518\n",
      "iteration 8400 / 15400: loss 1.152501\n",
      "iteration 8500 / 15400: loss 1.245580\n",
      "iteration 8600 / 15400: loss 1.196023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8700 / 15400: loss 1.033815\n",
      "iteration 8800 / 15400: loss 1.027225\n",
      "epoch done... acc 0.531\n",
      "iteration 8900 / 15400: loss 1.168890\n",
      "iteration 9000 / 15400: loss 1.217526\n",
      "iteration 9100 / 15400: loss 1.008834\n",
      "iteration 9200 / 15400: loss 1.152179\n",
      "iteration 9300 / 15400: loss 1.164878\n",
      "epoch done... acc 0.526\n",
      "iteration 9400 / 15400: loss 1.150467\n",
      "iteration 9500 / 15400: loss 1.129865\n",
      "iteration 9600 / 15400: loss 1.168539\n",
      "iteration 9700 / 15400: loss 1.089468\n",
      "iteration 9800 / 15400: loss 1.248435\n",
      "epoch done... acc 0.501\n",
      "iteration 9900 / 15400: loss 1.139019\n",
      "iteration 10000 / 15400: loss 1.009518\n",
      "iteration 10100 / 15400: loss 1.476711\n",
      "iteration 10200 / 15400: loss 1.001935\n",
      "epoch done... acc 0.519\n",
      "iteration 10300 / 15400: loss 1.179051\n",
      "iteration 10400 / 15400: loss 1.114070\n",
      "iteration 10500 / 15400: loss 1.125107\n",
      "iteration 10600 / 15400: loss 1.081984\n",
      "iteration 10700 / 15400: loss 1.023659\n",
      "epoch done... acc 0.512\n",
      "iteration 10800 / 15400: loss 1.032485\n",
      "iteration 10900 / 15400: loss 1.111881\n",
      "iteration 11000 / 15400: loss 1.119118\n",
      "iteration 11100 / 15400: loss 1.267517\n",
      "iteration 11200 / 15400: loss 1.005300\n",
      "epoch done... acc 0.532\n",
      "iteration 11300 / 15400: loss 1.152550\n",
      "iteration 11400 / 15400: loss 1.184927\n",
      "iteration 11500 / 15400: loss 0.936897\n",
      "iteration 11600 / 15400: loss 1.096547\n",
      "iteration 11700 / 15400: loss 1.091763\n",
      "epoch done... acc 0.535\n",
      "iteration 11800 / 15400: loss 1.081355\n",
      "iteration 11900 / 15400: loss 1.048038\n",
      "iteration 12000 / 15400: loss 1.140376\n",
      "iteration 12100 / 15400: loss 1.233629\n",
      "iteration 12200 / 15400: loss 1.148313\n",
      "epoch done... acc 0.524\n",
      "iteration 12300 / 15400: loss 0.858614\n",
      "iteration 12400 / 15400: loss 0.967416\n",
      "iteration 12500 / 15400: loss 1.161182\n",
      "iteration 12600 / 15400: loss 1.024672\n",
      "iteration 12700 / 15400: loss 1.163847\n",
      "epoch done... acc 0.535\n",
      "iteration 12800 / 15400: loss 0.973697\n",
      "iteration 12900 / 15400: loss 1.106592\n",
      "iteration 13000 / 15400: loss 0.922169\n",
      "iteration 13100 / 15400: loss 0.977040\n",
      "iteration 13200 / 15400: loss 1.103159\n",
      "epoch done... acc 0.518\n",
      "iteration 13300 / 15400: loss 1.027122\n",
      "iteration 13400 / 15400: loss 1.050376\n",
      "iteration 13500 / 15400: loss 1.014099\n",
      "iteration 13600 / 15400: loss 0.963517\n",
      "iteration 13700 / 15400: loss 0.864336\n",
      "epoch done... acc 0.517\n",
      "iteration 13800 / 15400: loss 1.269096\n",
      "iteration 13900 / 15400: loss 1.083915\n",
      "iteration 14000 / 15400: loss 0.979256\n",
      "iteration 14100 / 15400: loss 1.062241\n",
      "iteration 14200 / 15400: loss 1.205172\n",
      "epoch done... acc 0.523\n",
      "iteration 14300 / 15400: loss 1.034500\n",
      "iteration 14400 / 15400: loss 0.967998\n",
      "iteration 14500 / 15400: loss 0.953221\n",
      "iteration 14600 / 15400: loss 1.116380\n",
      "iteration 14700 / 15400: loss 0.935487\n",
      "epoch done... acc 0.512\n",
      "iteration 14800 / 15400: loss 1.068423\n",
      "iteration 14900 / 15400: loss 0.881141\n",
      "iteration 15000 / 15400: loss 0.983214\n",
      "iteration 15100 / 15400: loss 0.984629\n",
      "epoch done... acc 0.526\n",
      "iteration 15200 / 15400: loss 0.951815\n",
      "iteration 15300 / 15400: loss 1.080360\n",
      "Final training loss:  1.0650475229513683\n",
      "Final validation loss:  1.3775370003629743\n",
      "Final validation accuracy:  0.526\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "11 1 1 385 14000 100 0.001 0.98 0.526\n",
      "iteration 0 / 12600: loss 2.302583\n",
      "epoch done... acc 0.106\n",
      "iteration 100 / 12600: loss 1.992983\n",
      "iteration 200 / 12600: loss 1.891875\n",
      "iteration 300 / 12600: loss 1.807312\n",
      "iteration 400 / 12600: loss 1.842187\n",
      "epoch done... acc 0.395\n",
      "iteration 500 / 12600: loss 1.824976\n",
      "iteration 600 / 12600: loss 1.688084\n",
      "iteration 700 / 12600: loss 1.869147\n",
      "iteration 800 / 12600: loss 1.656452\n",
      "iteration 900 / 12600: loss 1.650636\n",
      "epoch done... acc 0.428\n",
      "iteration 1000 / 12600: loss 1.578832\n",
      "iteration 1100 / 12600: loss 1.707752\n",
      "iteration 1200 / 12600: loss 1.522449\n",
      "iteration 1300 / 12600: loss 1.542463\n",
      "iteration 1400 / 12600: loss 1.540255\n",
      "epoch done... acc 0.441\n",
      "iteration 1500 / 12600: loss 1.545340\n",
      "iteration 1600 / 12600: loss 1.435618\n",
      "iteration 1700 / 12600: loss 1.516196\n",
      "iteration 1800 / 12600: loss 1.631008\n",
      "iteration 1900 / 12600: loss 1.547443\n",
      "epoch done... acc 0.457\n",
      "iteration 2000 / 12600: loss 1.505416\n",
      "iteration 2100 / 12600: loss 1.422708\n",
      "iteration 2200 / 12600: loss 1.332524\n",
      "iteration 2300 / 12600: loss 1.443293\n",
      "iteration 2400 / 12600: loss 1.571078\n",
      "epoch done... acc 0.455\n",
      "iteration 2500 / 12600: loss 1.422936\n",
      "iteration 2600 / 12600: loss 1.668936\n",
      "iteration 2700 / 12600: loss 1.327851\n",
      "iteration 2800 / 12600: loss 1.343757\n",
      "iteration 2900 / 12600: loss 1.457717\n",
      "epoch done... acc 0.473\n",
      "iteration 3000 / 12600: loss 1.377014\n",
      "iteration 3100 / 12600: loss 1.509785\n",
      "iteration 3200 / 12600: loss 1.292171\n",
      "iteration 3300 / 12600: loss 1.377895\n",
      "iteration 3400 / 12600: loss 1.373363\n",
      "epoch done... acc 0.469\n",
      "iteration 3500 / 12600: loss 1.440360\n",
      "iteration 3600 / 12600: loss 1.187016\n",
      "iteration 3700 / 12600: loss 1.267524\n",
      "iteration 3800 / 12600: loss 1.291048\n",
      "iteration 3900 / 12600: loss 1.258398\n",
      "epoch done... acc 0.483\n",
      "iteration 4000 / 12600: loss 1.317218\n",
      "iteration 4100 / 12600: loss 1.569381\n",
      "iteration 4200 / 12600: loss 1.400261\n",
      "iteration 4300 / 12600: loss 1.239362\n",
      "iteration 4400 / 12600: loss 1.490227\n",
      "epoch done... acc 0.481\n",
      "iteration 4500 / 12600: loss 1.345237\n",
      "iteration 4600 / 12600: loss 1.372491\n",
      "iteration 4700 / 12600: loss 1.447720\n",
      "iteration 4800 / 12600: loss 1.425829\n",
      "iteration 4900 / 12600: loss 1.181057\n",
      "epoch done... acc 0.486\n",
      "iteration 5000 / 12600: loss 1.365562\n",
      "iteration 5100 / 12600: loss 1.267265\n",
      "iteration 5200 / 12600: loss 1.225571\n",
      "iteration 5300 / 12600: loss 1.160560\n",
      "epoch done... acc 0.504\n",
      "iteration 5400 / 12600: loss 1.348319\n",
      "iteration 5500 / 12600: loss 1.091395\n",
      "iteration 5600 / 12600: loss 1.199644\n",
      "iteration 5700 / 12600: loss 1.324775\n",
      "iteration 5800 / 12600: loss 1.432835\n",
      "epoch done... acc 0.496\n",
      "iteration 5900 / 12600: loss 1.382922\n",
      "iteration 6000 / 12600: loss 1.247603\n",
      "iteration 6100 / 12600: loss 1.281888\n",
      "iteration 6200 / 12600: loss 1.200539\n",
      "iteration 6300 / 12600: loss 1.180171\n",
      "epoch done... acc 0.488\n",
      "iteration 6400 / 12600: loss 1.353984\n",
      "iteration 6500 / 12600: loss 1.351661\n",
      "iteration 6600 / 12600: loss 1.299071\n",
      "iteration 6700 / 12600: loss 1.324421\n",
      "iteration 6800 / 12600: loss 1.324029\n",
      "epoch done... acc 0.49\n",
      "iteration 6900 / 12600: loss 1.185361\n",
      "iteration 7000 / 12600: loss 1.266824\n",
      "iteration 7100 / 12600: loss 1.402660\n",
      "iteration 7200 / 12600: loss 1.245040\n",
      "iteration 7300 / 12600: loss 1.243387\n",
      "epoch done... acc 0.5\n",
      "iteration 7400 / 12600: loss 1.108778\n",
      "iteration 7500 / 12600: loss 1.117652\n",
      "iteration 7600 / 12600: loss 1.101565\n",
      "iteration 7700 / 12600: loss 1.269596\n",
      "iteration 7800 / 12600: loss 1.272391\n",
      "epoch done... acc 0.51\n",
      "iteration 7900 / 12600: loss 1.103357\n",
      "iteration 8000 / 12600: loss 1.197436\n",
      "iteration 8100 / 12600: loss 1.130143\n",
      "iteration 8200 / 12600: loss 1.167552\n",
      "iteration 8300 / 12600: loss 1.071693\n",
      "epoch done... acc 0.513\n",
      "iteration 8400 / 12600: loss 1.103778\n",
      "iteration 8500 / 12600: loss 1.324063\n",
      "iteration 8600 / 12600: loss 1.192572\n",
      "iteration 8700 / 12600: loss 1.018888\n",
      "iteration 8800 / 12600: loss 1.141098\n",
      "epoch done... acc 0.517\n",
      "iteration 8900 / 12600: loss 1.180384\n",
      "iteration 9000 / 12600: loss 1.025102\n",
      "iteration 9100 / 12600: loss 1.160196\n",
      "iteration 9200 / 12600: loss 0.937126\n",
      "iteration 9300 / 12600: loss 1.249103\n",
      "epoch done... acc 0.52\n",
      "iteration 9400 / 12600: loss 0.911469\n",
      "iteration 9500 / 12600: loss 1.184867\n",
      "iteration 9600 / 12600: loss 1.160294\n",
      "iteration 9700 / 12600: loss 1.064333\n",
      "iteration 9800 / 12600: loss 1.119868\n",
      "epoch done... acc 0.523\n",
      "iteration 9900 / 12600: loss 1.080421\n",
      "iteration 10000 / 12600: loss 1.154369\n",
      "iteration 10100 / 12600: loss 1.167345\n",
      "iteration 10200 / 12600: loss 1.288041\n",
      "epoch done... acc 0.532\n",
      "iteration 10300 / 12600: loss 1.060990\n",
      "iteration 10400 / 12600: loss 1.091027\n",
      "iteration 10500 / 12600: loss 0.947142\n",
      "iteration 10600 / 12600: loss 1.260157\n",
      "iteration 10700 / 12600: loss 1.284810\n",
      "epoch done... acc 0.526\n",
      "iteration 10800 / 12600: loss 1.140370\n",
      "iteration 10900 / 12600: loss 1.137581\n",
      "iteration 11000 / 12600: loss 0.888502\n",
      "iteration 11100 / 12600: loss 1.097326\n",
      "iteration 11200 / 12600: loss 1.144054\n",
      "epoch done... acc 0.525\n",
      "iteration 11300 / 12600: loss 1.186167\n",
      "iteration 11400 / 12600: loss 1.116699\n",
      "iteration 11500 / 12600: loss 0.998015\n",
      "iteration 11600 / 12600: loss 1.183240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 11700 / 12600: loss 1.077587\n",
      "epoch done... acc 0.521\n",
      "iteration 11800 / 12600: loss 1.065287\n",
      "iteration 11900 / 12600: loss 1.117895\n",
      "iteration 12000 / 12600: loss 0.949394\n",
      "iteration 12100 / 12600: loss 0.911105\n",
      "iteration 12200 / 12600: loss 1.071686\n",
      "epoch done... acc 0.528\n",
      "iteration 12300 / 12600: loss 1.208506\n",
      "iteration 12400 / 12600: loss 1.084000\n",
      "iteration 12500 / 12600: loss 1.294078\n",
      "Final training loss:  1.002411072097805\n",
      "Final validation loss:  1.3511841751534315\n",
      "Final validation accuracy:  0.528\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "12 2 1 385 14000 100 0.001 0.98 0.528\n",
      "iteration 0 / 14000: loss 2.302621\n",
      "epoch done... acc 0.169\n",
      "iteration 100 / 14000: loss 2.040248\n",
      "iteration 200 / 14000: loss 1.936538\n",
      "iteration 300 / 14000: loss 1.929290\n",
      "iteration 400 / 14000: loss 1.753437\n",
      "epoch done... acc 0.394\n",
      "iteration 500 / 14000: loss 1.697156\n",
      "iteration 600 / 14000: loss 1.601720\n",
      "iteration 700 / 14000: loss 1.544441\n",
      "iteration 800 / 14000: loss 1.775327\n",
      "iteration 900 / 14000: loss 1.757419\n",
      "epoch done... acc 0.416\n",
      "iteration 1000 / 14000: loss 1.447116\n",
      "iteration 1100 / 14000: loss 1.608640\n",
      "iteration 1200 / 14000: loss 1.476384\n",
      "iteration 1300 / 14000: loss 1.620973\n",
      "iteration 1400 / 14000: loss 1.545081\n",
      "epoch done... acc 0.439\n",
      "iteration 1500 / 14000: loss 1.519375\n",
      "iteration 1600 / 14000: loss 1.483632\n",
      "iteration 1700 / 14000: loss 1.440763\n",
      "iteration 1800 / 14000: loss 1.542138\n",
      "iteration 1900 / 14000: loss 1.431060\n",
      "epoch done... acc 0.463\n",
      "iteration 2000 / 14000: loss 1.411880\n",
      "iteration 2100 / 14000: loss 1.444885\n",
      "iteration 2200 / 14000: loss 1.517869\n",
      "iteration 2300 / 14000: loss 1.347441\n",
      "iteration 2400 / 14000: loss 1.552562\n",
      "epoch done... acc 0.443\n",
      "iteration 2500 / 14000: loss 1.510760\n",
      "iteration 2600 / 14000: loss 1.429805\n",
      "iteration 2700 / 14000: loss 1.411249\n",
      "iteration 2800 / 14000: loss 1.441418\n",
      "iteration 2900 / 14000: loss 1.359883\n",
      "epoch done... acc 0.484\n",
      "iteration 3000 / 14000: loss 1.485547\n",
      "iteration 3100 / 14000: loss 1.339178\n",
      "iteration 3200 / 14000: loss 1.268156\n",
      "iteration 3300 / 14000: loss 1.414078\n",
      "iteration 3400 / 14000: loss 1.188274\n",
      "epoch done... acc 0.491\n",
      "iteration 3500 / 14000: loss 1.450912\n",
      "iteration 3600 / 14000: loss 1.373132\n",
      "iteration 3700 / 14000: loss 1.479731\n",
      "iteration 3800 / 14000: loss 1.478294\n",
      "iteration 3900 / 14000: loss 1.425592\n",
      "epoch done... acc 0.479\n",
      "iteration 4000 / 14000: loss 1.337741\n",
      "iteration 4100 / 14000: loss 1.397964\n",
      "iteration 4200 / 14000: loss 1.369499\n",
      "iteration 4300 / 14000: loss 1.295310\n",
      "iteration 4400 / 14000: loss 1.134393\n",
      "epoch done... acc 0.495\n",
      "iteration 4500 / 14000: loss 1.384686\n",
      "iteration 4600 / 14000: loss 1.337140\n",
      "iteration 4700 / 14000: loss 1.097896\n",
      "iteration 4800 / 14000: loss 1.269729\n",
      "iteration 4900 / 14000: loss 1.424967\n",
      "epoch done... acc 0.496\n",
      "iteration 5000 / 14000: loss 1.344946\n",
      "iteration 5100 / 14000: loss 1.392433\n",
      "iteration 5200 / 14000: loss 1.350955\n",
      "iteration 5300 / 14000: loss 1.375644\n",
      "epoch done... acc 0.493\n",
      "iteration 5400 / 14000: loss 1.446241\n",
      "iteration 5500 / 14000: loss 1.354099\n",
      "iteration 5600 / 14000: loss 1.449174\n",
      "iteration 5700 / 14000: loss 1.153931\n",
      "iteration 5800 / 14000: loss 1.267656\n",
      "epoch done... acc 0.504\n",
      "iteration 5900 / 14000: loss 1.336787\n",
      "iteration 6000 / 14000: loss 1.200571\n",
      "iteration 6100 / 14000: loss 1.217289\n",
      "iteration 6200 / 14000: loss 1.250075\n",
      "iteration 6300 / 14000: loss 1.051967\n",
      "epoch done... acc 0.513\n",
      "iteration 6400 / 14000: loss 1.102877\n",
      "iteration 6500 / 14000: loss 1.190821\n",
      "iteration 6600 / 14000: loss 1.123151\n",
      "iteration 6700 / 14000: loss 1.188683\n",
      "iteration 6800 / 14000: loss 1.317139\n",
      "epoch done... acc 0.512\n",
      "iteration 6900 / 14000: loss 1.123381\n",
      "iteration 7000 / 14000: loss 1.226761\n",
      "iteration 7100 / 14000: loss 1.241729\n",
      "iteration 7200 / 14000: loss 1.194219\n",
      "iteration 7300 / 14000: loss 1.187769\n",
      "epoch done... acc 0.516\n",
      "iteration 7400 / 14000: loss 1.282913\n",
      "iteration 7500 / 14000: loss 1.190991\n",
      "iteration 7600 / 14000: loss 1.200525\n",
      "iteration 7700 / 14000: loss 1.149342\n",
      "iteration 7800 / 14000: loss 1.252720\n",
      "epoch done... acc 0.502\n",
      "iteration 7900 / 14000: loss 1.194346\n",
      "iteration 8000 / 14000: loss 1.164120\n",
      "iteration 8100 / 14000: loss 1.141550\n",
      "iteration 8200 / 14000: loss 1.150838\n",
      "iteration 8300 / 14000: loss 1.185482\n",
      "epoch done... acc 0.525\n",
      "iteration 8400 / 14000: loss 1.030632\n",
      "iteration 8500 / 14000: loss 1.107841\n",
      "iteration 8600 / 14000: loss 1.024464\n",
      "iteration 8700 / 14000: loss 1.184564\n",
      "iteration 8800 / 14000: loss 1.111088\n",
      "epoch done... acc 0.524\n",
      "iteration 8900 / 14000: loss 1.264547\n",
      "iteration 9000 / 14000: loss 1.088317\n",
      "iteration 9100 / 14000: loss 1.123817\n",
      "iteration 9200 / 14000: loss 1.008544\n",
      "iteration 9300 / 14000: loss 1.159343\n",
      "epoch done... acc 0.525\n",
      "iteration 9400 / 14000: loss 0.999533\n",
      "iteration 9500 / 14000: loss 0.967765\n",
      "iteration 9600 / 14000: loss 1.025377\n",
      "iteration 9700 / 14000: loss 1.150106\n",
      "iteration 9800 / 14000: loss 1.117861\n",
      "epoch done... acc 0.531\n",
      "iteration 9900 / 14000: loss 1.165928\n",
      "iteration 10000 / 14000: loss 1.135665\n",
      "iteration 10100 / 14000: loss 1.214449\n",
      "iteration 10200 / 14000: loss 1.103346\n",
      "epoch done... acc 0.529\n",
      "iteration 10300 / 14000: loss 1.213794\n",
      "iteration 10400 / 14000: loss 0.931841\n",
      "iteration 10500 / 14000: loss 1.199883\n",
      "iteration 10600 / 14000: loss 1.160585\n",
      "iteration 10700 / 14000: loss 0.974516\n",
      "epoch done... acc 0.522\n",
      "iteration 10800 / 14000: loss 1.013891\n",
      "iteration 10900 / 14000: loss 1.130022\n",
      "iteration 11000 / 14000: loss 1.118707\n",
      "iteration 11100 / 14000: loss 1.120451\n",
      "iteration 11200 / 14000: loss 1.056656\n",
      "epoch done... acc 0.515\n",
      "iteration 11300 / 14000: loss 1.156904\n",
      "iteration 11400 / 14000: loss 1.064546\n",
      "iteration 11500 / 14000: loss 0.882160\n",
      "iteration 11600 / 14000: loss 1.110899\n",
      "iteration 11700 / 14000: loss 0.947624\n",
      "epoch done... acc 0.518\n",
      "iteration 11800 / 14000: loss 0.980471\n",
      "iteration 11900 / 14000: loss 1.204808\n",
      "iteration 12000 / 14000: loss 1.024263\n",
      "iteration 12100 / 14000: loss 0.924956\n",
      "iteration 12200 / 14000: loss 0.931316\n",
      "epoch done... acc 0.53\n",
      "iteration 12300 / 14000: loss 1.311607\n",
      "iteration 12400 / 14000: loss 0.963463\n",
      "iteration 12500 / 14000: loss 1.106581\n",
      "iteration 12600 / 14000: loss 1.013055\n",
      "iteration 12700 / 14000: loss 0.834637\n",
      "epoch done... acc 0.537\n",
      "iteration 12800 / 14000: loss 0.881395\n",
      "iteration 12900 / 14000: loss 1.050618\n",
      "iteration 13000 / 14000: loss 0.904856\n",
      "iteration 13100 / 14000: loss 0.921558\n",
      "iteration 13200 / 14000: loss 1.038232\n",
      "epoch done... acc 0.542\n",
      "iteration 13300 / 14000: loss 1.073468\n",
      "iteration 13400 / 14000: loss 0.909705\n",
      "iteration 13500 / 14000: loss 1.020660\n",
      "iteration 13600 / 14000: loss 0.977582\n",
      "iteration 13700 / 14000: loss 0.958650\n",
      "epoch done... acc 0.524\n",
      "iteration 13800 / 14000: loss 0.983792\n",
      "iteration 13900 / 14000: loss 0.874642\n",
      "Final training loss:  0.9546374503895817\n",
      "Final validation loss:  1.3482764318888818\n",
      "Final validation accuracy:  0.524\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "13 1 0 385 14000 100 0.001 0.98 0.524\n",
      "iteration 0 / 14000: loss 2.302670\n",
      "epoch done... acc 0.145\n",
      "iteration 100 / 14000: loss 2.071450\n",
      "iteration 200 / 14000: loss 1.963326\n",
      "iteration 300 / 14000: loss 1.828053\n",
      "iteration 400 / 14000: loss 1.862611\n",
      "epoch done... acc 0.376\n",
      "iteration 500 / 14000: loss 1.754830\n",
      "iteration 600 / 14000: loss 1.730806\n",
      "iteration 700 / 14000: loss 1.787319\n",
      "iteration 800 / 14000: loss 1.712309\n",
      "iteration 900 / 14000: loss 1.701290\n",
      "epoch done... acc 0.402\n",
      "iteration 1000 / 14000: loss 1.650312\n",
      "iteration 1100 / 14000: loss 1.559276\n",
      "iteration 1200 / 14000: loss 1.617424\n",
      "iteration 1300 / 14000: loss 1.538461\n",
      "iteration 1400 / 14000: loss 1.610715\n",
      "epoch done... acc 0.436\n",
      "iteration 1500 / 14000: loss 1.633510\n",
      "iteration 1600 / 14000: loss 1.690121\n",
      "iteration 1700 / 14000: loss 1.514451\n",
      "iteration 1800 / 14000: loss 1.537638\n",
      "iteration 1900 / 14000: loss 1.585980\n",
      "epoch done... acc 0.438\n",
      "iteration 2000 / 14000: loss 1.580024\n",
      "iteration 2100 / 14000: loss 1.414160\n",
      "iteration 2200 / 14000: loss 1.487556\n",
      "iteration 2300 / 14000: loss 1.466229\n",
      "iteration 2400 / 14000: loss 1.448593\n",
      "epoch done... acc 0.459\n",
      "iteration 2500 / 14000: loss 1.393421\n",
      "iteration 2600 / 14000: loss 1.317077\n",
      "iteration 2700 / 14000: loss 1.652163\n",
      "iteration 2800 / 14000: loss 1.449148\n",
      "iteration 2900 / 14000: loss 1.374875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch done... acc 0.448\n",
      "iteration 3000 / 14000: loss 1.485232\n",
      "iteration 3100 / 14000: loss 1.461443\n",
      "iteration 3200 / 14000: loss 1.433581\n",
      "iteration 3300 / 14000: loss 1.286687\n",
      "iteration 3400 / 14000: loss 1.389942\n",
      "epoch done... acc 0.477\n",
      "iteration 3500 / 14000: loss 1.312892\n",
      "iteration 3600 / 14000: loss 1.425046\n",
      "iteration 3700 / 14000: loss 1.336418\n",
      "iteration 3800 / 14000: loss 1.479152\n",
      "iteration 3900 / 14000: loss 1.410415\n",
      "epoch done... acc 0.467\n",
      "iteration 4000 / 14000: loss 1.264053\n",
      "iteration 4100 / 14000: loss 1.267079\n",
      "iteration 4200 / 14000: loss 1.480629\n",
      "iteration 4300 / 14000: loss 1.527014\n",
      "iteration 4400 / 14000: loss 1.443085\n",
      "epoch done... acc 0.472\n",
      "iteration 4500 / 14000: loss 1.186501\n",
      "iteration 4600 / 14000: loss 1.350449\n",
      "iteration 4700 / 14000: loss 1.394980\n",
      "iteration 4800 / 14000: loss 1.297749\n",
      "iteration 4900 / 14000: loss 1.398813\n",
      "epoch done... acc 0.472\n",
      "iteration 5000 / 14000: loss 1.370542\n",
      "iteration 5100 / 14000: loss 1.537280\n",
      "iteration 5200 / 14000: loss 1.322706\n",
      "iteration 5300 / 14000: loss 1.269266\n",
      "epoch done... acc 0.472\n",
      "iteration 5400 / 14000: loss 1.321236\n",
      "iteration 5500 / 14000: loss 1.401944\n",
      "iteration 5600 / 14000: loss 1.376948\n",
      "iteration 5700 / 14000: loss 1.470094\n",
      "iteration 5800 / 14000: loss 1.453785\n",
      "epoch done... acc 0.501\n",
      "iteration 5900 / 14000: loss 1.239887\n",
      "iteration 6000 / 14000: loss 1.168690\n",
      "iteration 6100 / 14000: loss 1.344813\n",
      "iteration 6200 / 14000: loss 1.094345\n",
      "iteration 6300 / 14000: loss 1.245194\n",
      "epoch done... acc 0.512\n",
      "iteration 6400 / 14000: loss 1.331678\n",
      "iteration 6500 / 14000: loss 1.398777\n",
      "iteration 6600 / 14000: loss 1.243782\n",
      "iteration 6700 / 14000: loss 1.482682\n",
      "iteration 6800 / 14000: loss 1.412289\n",
      "epoch done... acc 0.492\n",
      "iteration 6900 / 14000: loss 1.322428\n",
      "iteration 7000 / 14000: loss 1.192397\n",
      "iteration 7100 / 14000: loss 1.223576\n",
      "iteration 7200 / 14000: loss 1.212084\n",
      "iteration 7300 / 14000: loss 1.157497\n",
      "epoch done... acc 0.492\n",
      "iteration 7400 / 14000: loss 0.977074\n",
      "iteration 7500 / 14000: loss 1.189537\n",
      "iteration 7600 / 14000: loss 1.295978\n",
      "iteration 7700 / 14000: loss 1.477623\n",
      "iteration 7800 / 14000: loss 1.104867\n",
      "epoch done... acc 0.493\n",
      "iteration 7900 / 14000: loss 1.146707\n",
      "iteration 8000 / 14000: loss 1.247121\n",
      "iteration 8100 / 14000: loss 1.215907\n",
      "iteration 8200 / 14000: loss 1.180049\n",
      "iteration 8300 / 14000: loss 1.151135\n",
      "epoch done... acc 0.508\n",
      "iteration 8400 / 14000: loss 1.245214\n",
      "iteration 8500 / 14000: loss 1.093537\n",
      "iteration 8600 / 14000: loss 1.292422\n",
      "iteration 8700 / 14000: loss 1.234537\n",
      "iteration 8800 / 14000: loss 1.271863\n",
      "epoch done... acc 0.515\n",
      "iteration 8900 / 14000: loss 1.052164\n",
      "iteration 9000 / 14000: loss 1.181245\n",
      "iteration 9100 / 14000: loss 1.132878\n",
      "iteration 9200 / 14000: loss 1.272548\n",
      "iteration 9300 / 14000: loss 1.210753\n",
      "epoch done... acc 0.525\n",
      "iteration 9400 / 14000: loss 1.211736\n",
      "iteration 9500 / 14000: loss 1.139523\n",
      "iteration 9600 / 14000: loss 1.223774\n",
      "iteration 9700 / 14000: loss 0.887404\n",
      "iteration 9800 / 14000: loss 1.003876\n",
      "epoch done... acc 0.518\n",
      "iteration 9900 / 14000: loss 1.199045\n",
      "iteration 10000 / 14000: loss 1.311726\n",
      "iteration 10100 / 14000: loss 0.961502\n",
      "iteration 10200 / 14000: loss 1.090334\n",
      "epoch done... acc 0.543\n",
      "iteration 10300 / 14000: loss 1.126585\n",
      "iteration 10400 / 14000: loss 1.192485\n",
      "iteration 10500 / 14000: loss 1.061089\n",
      "iteration 10600 / 14000: loss 1.192467\n",
      "iteration 10700 / 14000: loss 1.010199\n",
      "epoch done... acc 0.518\n",
      "iteration 10800 / 14000: loss 1.113239\n",
      "iteration 10900 / 14000: loss 1.117320\n",
      "iteration 11000 / 14000: loss 1.020585\n",
      "iteration 11100 / 14000: loss 0.986050\n",
      "iteration 11200 / 14000: loss 1.375245\n",
      "epoch done... acc 0.519\n",
      "iteration 11300 / 14000: loss 1.085595\n",
      "iteration 11400 / 14000: loss 1.184118\n",
      "iteration 11500 / 14000: loss 1.209450\n",
      "iteration 11600 / 14000: loss 1.201315\n",
      "iteration 11700 / 14000: loss 1.017457\n",
      "epoch done... acc 0.513\n",
      "iteration 11800 / 14000: loss 1.132321\n",
      "iteration 11900 / 14000: loss 1.106521\n",
      "iteration 12000 / 14000: loss 1.103957\n",
      "iteration 12100 / 14000: loss 0.946381\n",
      "iteration 12200 / 14000: loss 1.262097\n",
      "epoch done... acc 0.503\n",
      "iteration 12300 / 14000: loss 1.093998\n",
      "iteration 12400 / 14000: loss 1.036807\n",
      "iteration 12500 / 14000: loss 1.041948\n",
      "iteration 12600 / 14000: loss 1.107369\n",
      "iteration 12700 / 14000: loss 0.976864\n",
      "epoch done... acc 0.532\n",
      "iteration 12800 / 14000: loss 1.082095\n",
      "iteration 12900 / 14000: loss 0.941015\n",
      "iteration 13000 / 14000: loss 1.065543\n",
      "iteration 13100 / 14000: loss 1.108762\n",
      "iteration 13200 / 14000: loss 1.022550\n",
      "epoch done... acc 0.511\n",
      "iteration 13300 / 14000: loss 0.944101\n",
      "iteration 13400 / 14000: loss 0.890534\n",
      "iteration 13500 / 14000: loss 1.134012\n",
      "iteration 13600 / 14000: loss 0.987847\n",
      "iteration 13700 / 14000: loss 1.139076\n",
      "epoch done... acc 0.515\n",
      "iteration 13800 / 14000: loss 1.030097\n",
      "iteration 13900 / 14000: loss 1.019545\n",
      "Final training loss:  1.1027105872084348\n",
      "Final validation loss:  1.3792337181802345\n",
      "Final validation accuracy:  0.515\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "14 2 0 385 14000 100 0.001 0.98 0.515\n",
      "iteration 0 / 15400: loss 2.302574\n",
      "epoch done... acc 0.183\n",
      "iteration 100 / 15400: loss 2.076497\n",
      "iteration 200 / 15400: loss 1.974939\n",
      "iteration 300 / 15400: loss 1.948102\n",
      "iteration 400 / 15400: loss 1.830398\n",
      "epoch done... acc 0.373\n",
      "iteration 500 / 15400: loss 1.802847\n",
      "iteration 600 / 15400: loss 1.830754\n",
      "iteration 700 / 15400: loss 1.702207\n",
      "iteration 800 / 15400: loss 1.796810\n",
      "iteration 900 / 15400: loss 1.796357\n",
      "epoch done... acc 0.42\n",
      "iteration 1000 / 15400: loss 1.444657\n",
      "iteration 1100 / 15400: loss 1.610835\n",
      "iteration 1200 / 15400: loss 1.642392\n",
      "iteration 1300 / 15400: loss 1.519621\n",
      "iteration 1400 / 15400: loss 1.603456\n",
      "epoch done... acc 0.447\n",
      "iteration 1500 / 15400: loss 1.551994\n",
      "iteration 1600 / 15400: loss 1.476487\n",
      "iteration 1700 / 15400: loss 1.521610\n",
      "iteration 1800 / 15400: loss 1.440733\n",
      "iteration 1900 / 15400: loss 1.445719\n",
      "epoch done... acc 0.456\n",
      "iteration 2000 / 15400: loss 1.415752\n",
      "iteration 2100 / 15400: loss 1.520584\n",
      "iteration 2200 / 15400: loss 1.500545\n",
      "iteration 2300 / 15400: loss 1.415681\n",
      "iteration 2400 / 15400: loss 1.375713\n",
      "epoch done... acc 0.449\n",
      "iteration 2500 / 15400: loss 1.586767\n",
      "iteration 2600 / 15400: loss 1.657480\n",
      "iteration 2700 / 15400: loss 1.371810\n",
      "iteration 2800 / 15400: loss 1.522965\n",
      "iteration 2900 / 15400: loss 1.486378\n",
      "epoch done... acc 0.471\n",
      "iteration 3000 / 15400: loss 1.362962\n",
      "iteration 3100 / 15400: loss 1.415634\n",
      "iteration 3200 / 15400: loss 1.470537\n",
      "iteration 3300 / 15400: loss 1.297168\n",
      "iteration 3400 / 15400: loss 1.527876\n",
      "epoch done... acc 0.479\n",
      "iteration 3500 / 15400: loss 1.368697\n",
      "iteration 3600 / 15400: loss 1.269342\n",
      "iteration 3700 / 15400: loss 1.375484\n",
      "iteration 3800 / 15400: loss 1.234304\n",
      "iteration 3900 / 15400: loss 1.343889\n",
      "epoch done... acc 0.479\n",
      "iteration 4000 / 15400: loss 1.362248\n",
      "iteration 4100 / 15400: loss 1.316679\n",
      "iteration 4200 / 15400: loss 1.388808\n",
      "iteration 4300 / 15400: loss 1.262919\n",
      "iteration 4400 / 15400: loss 1.279705\n",
      "epoch done... acc 0.5\n",
      "iteration 4500 / 15400: loss 1.474510\n",
      "iteration 4600 / 15400: loss 1.358725\n",
      "iteration 4700 / 15400: loss 1.397262\n",
      "iteration 4800 / 15400: loss 1.382605\n",
      "iteration 4900 / 15400: loss 1.330512\n",
      "epoch done... acc 0.487\n",
      "iteration 5000 / 15400: loss 1.252130\n",
      "iteration 5100 / 15400: loss 1.333657\n",
      "iteration 5200 / 15400: loss 1.285379\n",
      "iteration 5300 / 15400: loss 1.363326\n",
      "epoch done... acc 0.51\n",
      "iteration 5400 / 15400: loss 1.324117\n",
      "iteration 5500 / 15400: loss 1.316205\n",
      "iteration 5600 / 15400: loss 1.378471\n",
      "iteration 5700 / 15400: loss 1.145224\n",
      "iteration 5800 / 15400: loss 1.234451\n",
      "epoch done... acc 0.495\n",
      "iteration 5900 / 15400: loss 1.448196\n",
      "iteration 6000 / 15400: loss 1.387751\n",
      "iteration 6100 / 15400: loss 1.318249\n",
      "iteration 6200 / 15400: loss 1.307635\n",
      "iteration 6300 / 15400: loss 1.313767\n",
      "epoch done... acc 0.512\n",
      "iteration 6400 / 15400: loss 1.343049\n",
      "iteration 6500 / 15400: loss 1.315518\n",
      "iteration 6600 / 15400: loss 1.100525\n",
      "iteration 6700 / 15400: loss 1.326092\n",
      "iteration 6800 / 15400: loss 1.365478\n",
      "epoch done... acc 0.522\n",
      "iteration 6900 / 15400: loss 1.253804\n",
      "iteration 7000 / 15400: loss 1.193619\n",
      "iteration 7100 / 15400: loss 1.339871\n",
      "iteration 7200 / 15400: loss 1.235004\n",
      "iteration 7300 / 15400: loss 1.200883\n",
      "epoch done... acc 0.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7400 / 15400: loss 1.177902\n",
      "iteration 7500 / 15400: loss 1.091856\n",
      "iteration 7600 / 15400: loss 1.082311\n",
      "iteration 7700 / 15400: loss 1.358942\n",
      "iteration 7800 / 15400: loss 1.045845\n",
      "epoch done... acc 0.515\n",
      "iteration 7900 / 15400: loss 1.106082\n",
      "iteration 8000 / 15400: loss 1.163693\n",
      "iteration 8100 / 15400: loss 1.287021\n",
      "iteration 8200 / 15400: loss 1.064009\n",
      "iteration 8300 / 15400: loss 1.137065\n",
      "epoch done... acc 0.522\n",
      "iteration 8400 / 15400: loss 1.240589\n",
      "iteration 8500 / 15400: loss 1.018953\n",
      "iteration 8600 / 15400: loss 1.222577\n",
      "iteration 8700 / 15400: loss 1.250557\n",
      "iteration 8800 / 15400: loss 1.100950\n",
      "epoch done... acc 0.541\n",
      "iteration 8900 / 15400: loss 1.097215\n",
      "iteration 9000 / 15400: loss 1.326678\n",
      "iteration 9100 / 15400: loss 1.072852\n",
      "iteration 9200 / 15400: loss 1.129540\n",
      "iteration 9300 / 15400: loss 1.177683\n",
      "epoch done... acc 0.526\n",
      "iteration 9400 / 15400: loss 1.109806\n",
      "iteration 9500 / 15400: loss 1.045460\n",
      "iteration 9600 / 15400: loss 0.987363\n",
      "iteration 9700 / 15400: loss 1.186388\n",
      "iteration 9800 / 15400: loss 1.165918\n",
      "epoch done... acc 0.54\n",
      "iteration 9900 / 15400: loss 0.972620\n",
      "iteration 10000 / 15400: loss 1.145194\n",
      "iteration 10100 / 15400: loss 1.252224\n",
      "iteration 10200 / 15400: loss 1.067447\n",
      "epoch done... acc 0.519\n",
      "iteration 10300 / 15400: loss 1.143813\n",
      "iteration 10400 / 15400: loss 1.099260\n",
      "iteration 10500 / 15400: loss 0.941221\n",
      "iteration 10600 / 15400: loss 1.244522\n",
      "iteration 10700 / 15400: loss 1.188580\n",
      "epoch done... acc 0.527\n",
      "iteration 10800 / 15400: loss 1.195371\n",
      "iteration 10900 / 15400: loss 1.046980\n",
      "iteration 11000 / 15400: loss 1.123657\n",
      "iteration 11100 / 15400: loss 1.229825\n",
      "iteration 11200 / 15400: loss 0.938594\n",
      "epoch done... acc 0.518\n",
      "iteration 11300 / 15400: loss 1.051669\n",
      "iteration 11400 / 15400: loss 1.175212\n",
      "iteration 11500 / 15400: loss 0.978419\n",
      "iteration 11600 / 15400: loss 0.964390\n",
      "iteration 11700 / 15400: loss 0.800082\n",
      "epoch done... acc 0.518\n",
      "iteration 11800 / 15400: loss 1.141891\n",
      "iteration 11900 / 15400: loss 1.074294\n",
      "iteration 12000 / 15400: loss 1.008525\n",
      "iteration 12100 / 15400: loss 0.912737\n",
      "iteration 12200 / 15400: loss 0.985023\n",
      "epoch done... acc 0.524\n",
      "iteration 12300 / 15400: loss 1.094486\n",
      "iteration 12400 / 15400: loss 1.225909\n",
      "iteration 12500 / 15400: loss 1.030276\n",
      "iteration 12600 / 15400: loss 1.062374\n",
      "iteration 12700 / 15400: loss 0.971640\n",
      "epoch done... acc 0.533\n",
      "iteration 12800 / 15400: loss 0.931874\n",
      "iteration 12900 / 15400: loss 1.096210\n",
      "iteration 13000 / 15400: loss 1.004161\n",
      "iteration 13100 / 15400: loss 1.175310\n",
      "iteration 13200 / 15400: loss 1.047354\n",
      "epoch done... acc 0.532\n",
      "iteration 13300 / 15400: loss 1.085973\n",
      "iteration 13400 / 15400: loss 1.095432\n",
      "iteration 13500 / 15400: loss 1.041559\n",
      "iteration 13600 / 15400: loss 1.059882\n",
      "iteration 13700 / 15400: loss 0.967730\n",
      "epoch done... acc 0.521\n",
      "iteration 13800 / 15400: loss 1.123325\n",
      "iteration 13900 / 15400: loss 1.025563\n",
      "iteration 14000 / 15400: loss 1.029169\n",
      "iteration 14100 / 15400: loss 1.093520\n",
      "iteration 14200 / 15400: loss 1.104504\n",
      "epoch done... acc 0.523\n",
      "iteration 14300 / 15400: loss 0.918432\n",
      "iteration 14400 / 15400: loss 0.927688\n",
      "iteration 14500 / 15400: loss 1.036328\n",
      "iteration 14600 / 15400: loss 0.833013\n",
      "iteration 14700 / 15400: loss 0.912808\n",
      "epoch done... acc 0.508\n",
      "iteration 14800 / 15400: loss 0.951952\n",
      "iteration 14900 / 15400: loss 0.864604\n",
      "iteration 15000 / 15400: loss 1.089775\n",
      "iteration 15100 / 15400: loss 0.981527\n",
      "epoch done... acc 0.525\n",
      "iteration 15200 / 15400: loss 0.851035\n",
      "iteration 15300 / 15400: loss 0.867630\n",
      "Final training loss:  0.9865685121097649\n",
      "Final validation loss:  1.359514363098298\n",
      "Final validation accuracy:  0.525\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "15 1 1 385 14000 100 0.001 0.98 0.525\n",
      "iteration 0 / 12600: loss 2.302631\n",
      "epoch done... acc 0.177\n",
      "iteration 100 / 12600: loss 2.061237\n",
      "iteration 200 / 12600: loss 1.895203\n",
      "iteration 300 / 12600: loss 1.798806\n",
      "iteration 400 / 12600: loss 1.951383\n",
      "epoch done... acc 0.402\n",
      "iteration 500 / 12600: loss 1.787046\n",
      "iteration 600 / 12600: loss 1.679686\n",
      "iteration 700 / 12600: loss 1.748986\n",
      "iteration 800 / 12600: loss 1.713426\n",
      "iteration 900 / 12600: loss 1.579451\n",
      "epoch done... acc 0.429\n",
      "iteration 1000 / 12600: loss 1.498175\n",
      "iteration 1100 / 12600: loss 1.580291\n",
      "iteration 1200 / 12600: loss 1.670725\n",
      "iteration 1300 / 12600: loss 1.568641\n",
      "iteration 1400 / 12600: loss 1.551120\n",
      "epoch done... acc 0.447\n",
      "iteration 1500 / 12600: loss 1.515955\n",
      "iteration 1600 / 12600: loss 1.517377\n",
      "iteration 1700 / 12600: loss 1.439892\n",
      "iteration 1800 / 12600: loss 1.557032\n",
      "iteration 1900 / 12600: loss 1.396410\n",
      "epoch done... acc 0.468\n",
      "iteration 2000 / 12600: loss 1.527073\n",
      "iteration 2100 / 12600: loss 1.609225\n",
      "iteration 2200 / 12600: loss 1.367462\n",
      "iteration 2300 / 12600: loss 1.421254\n",
      "iteration 2400 / 12600: loss 1.554922\n",
      "epoch done... acc 0.474\n",
      "iteration 2500 / 12600: loss 1.428687\n",
      "iteration 2600 / 12600: loss 1.393052\n",
      "iteration 2700 / 12600: loss 1.531582\n",
      "iteration 2800 / 12600: loss 1.510507\n",
      "iteration 2900 / 12600: loss 1.346715\n",
      "epoch done... acc 0.476\n",
      "iteration 3000 / 12600: loss 1.671693\n",
      "iteration 3100 / 12600: loss 1.287809\n",
      "iteration 3200 / 12600: loss 1.295791\n",
      "iteration 3300 / 12600: loss 1.618441\n",
      "iteration 3400 / 12600: loss 1.305184\n",
      "epoch done... acc 0.474\n",
      "iteration 3500 / 12600: loss 1.337198\n",
      "iteration 3600 / 12600: loss 1.362780\n",
      "iteration 3700 / 12600: loss 1.578920\n",
      "iteration 3800 / 12600: loss 1.300215\n",
      "iteration 3900 / 12600: loss 1.450136\n",
      "epoch done... acc 0.482\n",
      "iteration 4000 / 12600: loss 1.370000\n",
      "iteration 4100 / 12600: loss 1.541263\n",
      "iteration 4200 / 12600: loss 1.312839\n",
      "iteration 4300 / 12600: loss 1.371448\n",
      "iteration 4400 / 12600: loss 1.388719\n",
      "epoch done... acc 0.486\n",
      "iteration 4500 / 12600: loss 1.518470\n",
      "iteration 4600 / 12600: loss 1.344525\n",
      "iteration 4700 / 12600: loss 1.258349\n",
      "iteration 4800 / 12600: loss 1.284244\n",
      "iteration 4900 / 12600: loss 1.258481\n",
      "epoch done... acc 0.498\n",
      "iteration 5000 / 12600: loss 1.505907\n",
      "iteration 5100 / 12600: loss 1.331480\n",
      "iteration 5200 / 12600: loss 1.339701\n",
      "iteration 5300 / 12600: loss 1.404733\n",
      "epoch done... acc 0.504\n",
      "iteration 5400 / 12600: loss 1.265542\n",
      "iteration 5500 / 12600: loss 1.413559\n",
      "iteration 5600 / 12600: loss 1.101187\n",
      "iteration 5700 / 12600: loss 1.410693\n",
      "iteration 5800 / 12600: loss 1.194324\n",
      "epoch done... acc 0.509\n",
      "iteration 5900 / 12600: loss 1.304907\n",
      "iteration 6000 / 12600: loss 1.291863\n",
      "iteration 6100 / 12600: loss 1.147870\n",
      "iteration 6200 / 12600: loss 1.382789\n",
      "iteration 6300 / 12600: loss 1.136396\n",
      "epoch done... acc 0.511\n",
      "iteration 6400 / 12600: loss 1.108358\n",
      "iteration 6500 / 12600: loss 1.058167\n",
      "iteration 6600 / 12600: loss 1.057887\n",
      "iteration 6700 / 12600: loss 1.127306\n",
      "iteration 6800 / 12600: loss 1.167316\n",
      "epoch done... acc 0.507\n",
      "iteration 6900 / 12600: loss 1.159629\n",
      "iteration 7000 / 12600: loss 1.376286\n",
      "iteration 7100 / 12600: loss 1.168314\n",
      "iteration 7200 / 12600: loss 1.372087\n",
      "iteration 7300 / 12600: loss 1.299073\n",
      "epoch done... acc 0.502\n",
      "iteration 7400 / 12600: loss 1.194998\n",
      "iteration 7500 / 12600: loss 0.997522\n",
      "iteration 7600 / 12600: loss 1.154111\n",
      "iteration 7700 / 12600: loss 1.163662\n",
      "iteration 7800 / 12600: loss 1.201145\n",
      "epoch done... acc 0.501\n",
      "iteration 7900 / 12600: loss 1.156066\n",
      "iteration 8000 / 12600: loss 1.100090\n",
      "iteration 8100 / 12600: loss 1.110282\n",
      "iteration 8200 / 12600: loss 1.198335\n",
      "iteration 8300 / 12600: loss 1.288204\n",
      "epoch done... acc 0.503\n",
      "iteration 8400 / 12600: loss 1.273827\n",
      "iteration 8500 / 12600: loss 1.239217\n",
      "iteration 8600 / 12600: loss 1.180211\n",
      "iteration 8700 / 12600: loss 1.256225\n",
      "iteration 8800 / 12600: loss 1.356509\n",
      "epoch done... acc 0.521\n",
      "iteration 8900 / 12600: loss 1.085902\n",
      "iteration 9000 / 12600: loss 1.209312\n",
      "iteration 9100 / 12600: loss 1.234490\n",
      "iteration 9200 / 12600: loss 1.267289\n",
      "iteration 9300 / 12600: loss 1.160214\n",
      "epoch done... acc 0.521\n",
      "iteration 9400 / 12600: loss 1.150479\n",
      "iteration 9500 / 12600: loss 1.149478\n",
      "iteration 9600 / 12600: loss 1.021094\n",
      "iteration 9700 / 12600: loss 1.160739\n",
      "iteration 9800 / 12600: loss 1.168025\n",
      "epoch done... acc 0.512\n",
      "iteration 9900 / 12600: loss 1.150715\n",
      "iteration 10000 / 12600: loss 1.296774\n",
      "iteration 10100 / 12600: loss 1.030894\n",
      "iteration 10200 / 12600: loss 1.215716\n",
      "epoch done... acc 0.498\n",
      "iteration 10300 / 12600: loss 1.002483\n",
      "iteration 10400 / 12600: loss 1.197413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10500 / 12600: loss 1.227121\n",
      "iteration 10600 / 12600: loss 1.142851\n",
      "iteration 10700 / 12600: loss 1.205585\n",
      "epoch done... acc 0.492\n",
      "iteration 10800 / 12600: loss 1.128542\n",
      "iteration 10900 / 12600: loss 1.008930\n",
      "iteration 11000 / 12600: loss 1.101480\n",
      "iteration 11100 / 12600: loss 1.140130\n",
      "iteration 11200 / 12600: loss 1.110604\n",
      "epoch done... acc 0.524\n",
      "iteration 11300 / 12600: loss 1.106047\n",
      "iteration 11400 / 12600: loss 1.081325\n",
      "iteration 11500 / 12600: loss 1.138388\n",
      "iteration 11600 / 12600: loss 1.112297\n",
      "iteration 11700 / 12600: loss 0.972339\n",
      "epoch done... acc 0.518\n",
      "iteration 11800 / 12600: loss 1.121440\n",
      "iteration 11900 / 12600: loss 1.252783\n",
      "iteration 12000 / 12600: loss 0.973838\n",
      "iteration 12100 / 12600: loss 1.066288\n",
      "iteration 12200 / 12600: loss 1.001307\n",
      "epoch done... acc 0.523\n",
      "iteration 12300 / 12600: loss 1.041048\n",
      "iteration 12400 / 12600: loss 0.883251\n",
      "iteration 12500 / 12600: loss 1.247339\n",
      "Final training loss:  1.0042956181510991\n",
      "Final validation loss:  1.375521643385532\n",
      "Final validation accuracy:  0.523\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "16 2 1 385 14000 100 0.001 0.98 0.523\n",
      "iteration 0 / 14000: loss 2.302626\n",
      "epoch done... acc 0.108\n",
      "iteration 100 / 14000: loss 2.089636\n",
      "iteration 200 / 14000: loss 1.876812\n",
      "iteration 300 / 14000: loss 1.866591\n",
      "iteration 400 / 14000: loss 1.864859\n",
      "epoch done... acc 0.389\n",
      "iteration 500 / 14000: loss 1.782030\n",
      "iteration 600 / 14000: loss 1.654347\n",
      "iteration 700 / 14000: loss 1.741516\n",
      "iteration 800 / 14000: loss 1.734463\n",
      "iteration 900 / 14000: loss 1.944040\n",
      "epoch done... acc 0.425\n",
      "iteration 1000 / 14000: loss 1.724522\n",
      "iteration 1100 / 14000: loss 1.533546\n",
      "iteration 1200 / 14000: loss 1.705720\n",
      "iteration 1300 / 14000: loss 1.446598\n",
      "iteration 1400 / 14000: loss 1.568347\n",
      "epoch done... acc 0.45\n",
      "iteration 1500 / 14000: loss 1.609576\n",
      "iteration 1600 / 14000: loss 1.372709\n",
      "iteration 1700 / 14000: loss 1.498704\n",
      "iteration 1800 / 14000: loss 1.526382\n",
      "iteration 1900 / 14000: loss 1.605730\n",
      "epoch done... acc 0.463\n",
      "iteration 2000 / 14000: loss 1.586160\n",
      "iteration 2100 / 14000: loss 1.443158\n",
      "iteration 2200 / 14000: loss 1.442993\n",
      "iteration 2300 / 14000: loss 1.527555\n",
      "iteration 2400 / 14000: loss 1.484306\n",
      "epoch done... acc 0.469\n",
      "iteration 2500 / 14000: loss 1.486393\n",
      "iteration 2600 / 14000: loss 1.363703\n",
      "iteration 2700 / 14000: loss 1.385731\n",
      "iteration 2800 / 14000: loss 1.548557\n",
      "iteration 2900 / 14000: loss 1.287420\n",
      "epoch done... acc 0.461\n",
      "iteration 3000 / 14000: loss 1.278338\n",
      "iteration 3100 / 14000: loss 1.318690\n",
      "iteration 3200 / 14000: loss 1.559253\n",
      "iteration 3300 / 14000: loss 1.230352\n",
      "iteration 3400 / 14000: loss 1.489091\n",
      "epoch done... acc 0.488\n",
      "iteration 3500 / 14000: loss 1.283994\n",
      "iteration 3600 / 14000: loss 1.331747\n",
      "iteration 3700 / 14000: loss 1.461767\n",
      "iteration 3800 / 14000: loss 1.454752\n",
      "iteration 3900 / 14000: loss 1.336374\n",
      "epoch done... acc 0.491\n",
      "iteration 4000 / 14000: loss 1.372996\n",
      "iteration 4100 / 14000: loss 1.301466\n",
      "iteration 4200 / 14000: loss 1.351801\n",
      "iteration 4300 / 14000: loss 1.260698\n",
      "iteration 4400 / 14000: loss 1.353773\n",
      "epoch done... acc 0.497\n",
      "iteration 4500 / 14000: loss 1.230351\n",
      "iteration 4600 / 14000: loss 1.188820\n",
      "iteration 4700 / 14000: loss 1.496497\n",
      "iteration 4800 / 14000: loss 1.352996\n",
      "iteration 4900 / 14000: loss 1.314934\n",
      "epoch done... acc 0.505\n",
      "iteration 5000 / 14000: loss 1.092350\n",
      "iteration 5100 / 14000: loss 1.335695\n",
      "iteration 5200 / 14000: loss 1.198663\n",
      "iteration 5300 / 14000: loss 1.191251\n",
      "epoch done... acc 0.494\n",
      "iteration 5400 / 14000: loss 1.257100\n",
      "iteration 5500 / 14000: loss 1.310080\n",
      "iteration 5600 / 14000: loss 1.367942\n",
      "iteration 5700 / 14000: loss 1.366732\n",
      "iteration 5800 / 14000: loss 1.146360\n",
      "epoch done... acc 0.513\n",
      "iteration 5900 / 14000: loss 1.455139\n",
      "iteration 6000 / 14000: loss 1.100957\n",
      "iteration 6100 / 14000: loss 1.293404\n",
      "iteration 6200 / 14000: loss 1.390665\n",
      "iteration 6300 / 14000: loss 1.314732\n",
      "epoch done... acc 0.503\n",
      "iteration 6400 / 14000: loss 1.212457\n",
      "iteration 6500 / 14000: loss 1.455502\n",
      "iteration 6600 / 14000: loss 1.157177\n",
      "iteration 6700 / 14000: loss 1.348293\n",
      "iteration 6800 / 14000: loss 1.142726\n",
      "epoch done... acc 0.519\n",
      "iteration 6900 / 14000: loss 1.052071\n",
      "iteration 7000 / 14000: loss 1.319264\n",
      "iteration 7100 / 14000: loss 1.188479\n",
      "iteration 7200 / 14000: loss 1.114325\n",
      "iteration 7300 / 14000: loss 1.361419\n",
      "epoch done... acc 0.515\n",
      "iteration 7400 / 14000: loss 1.130075\n",
      "iteration 7500 / 14000: loss 1.241424\n",
      "iteration 7600 / 14000: loss 0.989046\n",
      "iteration 7700 / 14000: loss 1.138062\n",
      "iteration 7800 / 14000: loss 1.211468\n",
      "epoch done... acc 0.507\n",
      "iteration 7900 / 14000: loss 1.103069\n",
      "iteration 8000 / 14000: loss 1.393145\n",
      "iteration 8100 / 14000: loss 1.257882\n",
      "iteration 8200 / 14000: loss 1.082849\n",
      "iteration 8300 / 14000: loss 1.012015\n",
      "epoch done... acc 0.514\n",
      "iteration 8400 / 14000: loss 1.296749\n",
      "iteration 8500 / 14000: loss 1.093871\n",
      "iteration 8600 / 14000: loss 1.277157\n",
      "iteration 8700 / 14000: loss 1.043083\n",
      "iteration 8800 / 14000: loss 1.139958\n",
      "epoch done... acc 0.528\n",
      "iteration 8900 / 14000: loss 1.157386\n",
      "iteration 9000 / 14000: loss 1.099472\n",
      "iteration 9100 / 14000: loss 1.026988\n",
      "iteration 9200 / 14000: loss 1.003250\n",
      "iteration 9300 / 14000: loss 1.164291\n",
      "epoch done... acc 0.536\n",
      "iteration 9400 / 14000: loss 1.181890\n",
      "iteration 9500 / 14000: loss 1.101634\n",
      "iteration 9600 / 14000: loss 1.149142\n",
      "iteration 9700 / 14000: loss 1.126735\n",
      "iteration 9800 / 14000: loss 1.111865\n",
      "epoch done... acc 0.531\n",
      "iteration 9900 / 14000: loss 1.116457\n",
      "iteration 10000 / 14000: loss 1.091755\n",
      "iteration 10100 / 14000: loss 1.155065\n",
      "iteration 10200 / 14000: loss 1.177706\n",
      "epoch done... acc 0.528\n",
      "iteration 10300 / 14000: loss 1.168954\n",
      "iteration 10400 / 14000: loss 1.055914\n",
      "iteration 10500 / 14000: loss 1.100297\n",
      "iteration 10600 / 14000: loss 1.303238\n",
      "iteration 10700 / 14000: loss 0.956008\n",
      "epoch done... acc 0.511\n",
      "iteration 10800 / 14000: loss 1.215034\n",
      "iteration 10900 / 14000: loss 1.274121\n",
      "iteration 11000 / 14000: loss 1.040871\n",
      "iteration 11100 / 14000: loss 1.041907\n",
      "iteration 11200 / 14000: loss 1.101836\n",
      "epoch done... acc 0.533\n",
      "iteration 11300 / 14000: loss 1.122016\n",
      "iteration 11400 / 14000: loss 1.099130\n",
      "iteration 11500 / 14000: loss 0.930066\n",
      "iteration 11600 / 14000: loss 1.137323\n",
      "iteration 11700 / 14000: loss 0.884145\n",
      "epoch done... acc 0.528\n",
      "iteration 11800 / 14000: loss 1.169169\n",
      "iteration 11900 / 14000: loss 1.087577\n",
      "iteration 12000 / 14000: loss 1.039962\n",
      "iteration 12100 / 14000: loss 1.102711\n",
      "iteration 12200 / 14000: loss 1.122944\n",
      "epoch done... acc 0.53\n",
      "iteration 12300 / 14000: loss 0.932416\n",
      "iteration 12400 / 14000: loss 0.906566\n",
      "iteration 12500 / 14000: loss 0.972250\n",
      "iteration 12600 / 14000: loss 1.227557\n",
      "iteration 12700 / 14000: loss 0.949257\n",
      "epoch done... acc 0.533\n",
      "iteration 12800 / 14000: loss 0.891631\n",
      "iteration 12900 / 14000: loss 1.098855\n",
      "iteration 13000 / 14000: loss 1.105399\n",
      "iteration 13100 / 14000: loss 1.033822\n",
      "iteration 13200 / 14000: loss 1.127763\n",
      "epoch done... acc 0.511\n",
      "iteration 13300 / 14000: loss 1.048092\n",
      "iteration 13400 / 14000: loss 1.179118\n",
      "iteration 13500 / 14000: loss 0.969728\n",
      "iteration 13600 / 14000: loss 1.019920\n",
      "iteration 13700 / 14000: loss 1.039419\n",
      "epoch done... acc 0.538\n",
      "iteration 13800 / 14000: loss 0.932407\n",
      "iteration 13900 / 14000: loss 0.884724\n",
      "Final training loss:  0.9364451022962232\n",
      "Final validation loss:  1.372329915137012\n",
      "Final validation accuracy:  0.538\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "17 1 0 385 14000 100 0.001 0.98 0.538\n",
      "iteration 0 / 14000: loss 2.302595\n",
      "epoch done... acc 0.141\n",
      "iteration 100 / 14000: loss 1.981953\n",
      "iteration 200 / 14000: loss 1.962714\n",
      "iteration 300 / 14000: loss 1.720299\n",
      "iteration 400 / 14000: loss 1.847291\n",
      "epoch done... acc 0.376\n",
      "iteration 500 / 14000: loss 1.837026\n",
      "iteration 600 / 14000: loss 1.722517\n",
      "iteration 700 / 14000: loss 1.675579\n",
      "iteration 800 / 14000: loss 1.682201\n",
      "iteration 900 / 14000: loss 1.576792\n",
      "epoch done... acc 0.404\n",
      "iteration 1000 / 14000: loss 1.691194\n",
      "iteration 1100 / 14000: loss 1.422330\n",
      "iteration 1200 / 14000: loss 1.443542\n",
      "iteration 1300 / 14000: loss 1.751949\n",
      "iteration 1400 / 14000: loss 1.553881\n",
      "epoch done... acc 0.452\n",
      "iteration 1500 / 14000: loss 1.714485\n",
      "iteration 1600 / 14000: loss 1.580120\n",
      "iteration 1700 / 14000: loss 1.670053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1800 / 14000: loss 1.794950\n",
      "iteration 1900 / 14000: loss 1.615858\n",
      "epoch done... acc 0.456\n",
      "iteration 2000 / 14000: loss 1.540430\n",
      "iteration 2100 / 14000: loss 1.463265\n",
      "iteration 2200 / 14000: loss 1.537585\n",
      "iteration 2300 / 14000: loss 1.433277\n",
      "iteration 2400 / 14000: loss 1.616701\n",
      "epoch done... acc 0.468\n",
      "iteration 2500 / 14000: loss 1.564161\n",
      "iteration 2600 / 14000: loss 1.479694\n",
      "iteration 2700 / 14000: loss 1.304421\n",
      "iteration 2800 / 14000: loss 1.219750\n",
      "iteration 2900 / 14000: loss 1.327468\n",
      "epoch done... acc 0.472\n",
      "iteration 3000 / 14000: loss 1.402304\n",
      "iteration 3100 / 14000: loss 1.363120\n",
      "iteration 3200 / 14000: loss 1.397695\n",
      "iteration 3300 / 14000: loss 1.429892\n",
      "iteration 3400 / 14000: loss 1.396096\n",
      "epoch done... acc 0.49\n",
      "iteration 3500 / 14000: loss 1.439940\n",
      "iteration 3600 / 14000: loss 1.459212\n",
      "iteration 3700 / 14000: loss 1.344903\n",
      "iteration 3800 / 14000: loss 1.317253\n",
      "iteration 3900 / 14000: loss 1.226503\n",
      "epoch done... acc 0.48\n",
      "iteration 4000 / 14000: loss 1.475411\n",
      "iteration 4100 / 14000: loss 1.519946\n",
      "iteration 4200 / 14000: loss 1.357892\n",
      "iteration 4300 / 14000: loss 1.362874\n",
      "iteration 4400 / 14000: loss 1.245217\n",
      "epoch done... acc 0.48\n",
      "iteration 4500 / 14000: loss 1.352851\n",
      "iteration 4600 / 14000: loss 1.303059\n",
      "iteration 4700 / 14000: loss 1.292426\n",
      "iteration 4800 / 14000: loss 1.296577\n",
      "iteration 4900 / 14000: loss 1.251396\n",
      "epoch done... acc 0.493\n",
      "iteration 5000 / 14000: loss 1.347857\n",
      "iteration 5100 / 14000: loss 1.403437\n",
      "iteration 5200 / 14000: loss 1.414944\n",
      "iteration 5300 / 14000: loss 1.244702\n",
      "epoch done... acc 0.485\n",
      "iteration 5400 / 14000: loss 1.601080\n",
      "iteration 5500 / 14000: loss 1.162624\n",
      "iteration 5600 / 14000: loss 1.499791\n",
      "iteration 5700 / 14000: loss 1.249308\n",
      "iteration 5800 / 14000: loss 1.468547\n",
      "epoch done... acc 0.508\n",
      "iteration 5900 / 14000: loss 1.271199\n",
      "iteration 6000 / 14000: loss 1.230152\n",
      "iteration 6100 / 14000: loss 1.173252\n",
      "iteration 6200 / 14000: loss 1.270465\n",
      "iteration 6300 / 14000: loss 1.124001\n",
      "epoch done... acc 0.492\n",
      "iteration 6400 / 14000: loss 1.188145\n",
      "iteration 6500 / 14000: loss 1.294388\n",
      "iteration 6600 / 14000: loss 1.383441\n",
      "iteration 6700 / 14000: loss 1.179061\n",
      "iteration 6800 / 14000: loss 1.338097\n",
      "epoch done... acc 0.507\n",
      "iteration 6900 / 14000: loss 1.145735\n",
      "iteration 7000 / 14000: loss 1.332801\n",
      "iteration 7100 / 14000: loss 1.147726\n",
      "iteration 7200 / 14000: loss 1.343170\n",
      "iteration 7300 / 14000: loss 1.251339\n",
      "epoch done... acc 0.502\n",
      "iteration 7400 / 14000: loss 1.178597\n",
      "iteration 7500 / 14000: loss 1.285710\n",
      "iteration 7600 / 14000: loss 1.083484\n",
      "iteration 7700 / 14000: loss 1.240169\n",
      "iteration 7800 / 14000: loss 1.092357\n",
      "epoch done... acc 0.502\n",
      "iteration 7900 / 14000: loss 1.193763\n",
      "iteration 8000 / 14000: loss 1.142300\n",
      "iteration 8100 / 14000: loss 1.144434\n",
      "iteration 8200 / 14000: loss 1.225455\n",
      "iteration 8300 / 14000: loss 1.258588\n",
      "epoch done... acc 0.502\n",
      "iteration 8400 / 14000: loss 1.311766\n",
      "iteration 8500 / 14000: loss 1.219290\n",
      "iteration 8600 / 14000: loss 0.942634\n",
      "iteration 8700 / 14000: loss 1.083766\n",
      "iteration 8800 / 14000: loss 1.242169\n",
      "epoch done... acc 0.517\n",
      "iteration 8900 / 14000: loss 1.103078\n",
      "iteration 9000 / 14000: loss 1.034014\n",
      "iteration 9100 / 14000: loss 1.031630\n",
      "iteration 9200 / 14000: loss 0.976257\n",
      "iteration 9300 / 14000: loss 1.073776\n",
      "epoch done... acc 0.515\n",
      "iteration 9400 / 14000: loss 1.006239\n",
      "iteration 9500 / 14000: loss 1.104654\n",
      "iteration 9600 / 14000: loss 1.163595\n",
      "iteration 9700 / 14000: loss 1.209606\n",
      "iteration 9800 / 14000: loss 1.239418\n",
      "epoch done... acc 0.508\n",
      "iteration 9900 / 14000: loss 1.189511\n",
      "iteration 10000 / 14000: loss 1.008813\n",
      "iteration 10100 / 14000: loss 1.148324\n",
      "iteration 10200 / 14000: loss 1.075798\n",
      "epoch done... acc 0.529\n",
      "iteration 10300 / 14000: loss 0.959169\n",
      "iteration 10400 / 14000: loss 1.058423\n",
      "iteration 10500 / 14000: loss 1.082611\n",
      "iteration 10600 / 14000: loss 1.290030\n",
      "iteration 10700 / 14000: loss 1.254332\n",
      "epoch done... acc 0.516\n",
      "iteration 10800 / 14000: loss 1.158296\n",
      "iteration 10900 / 14000: loss 1.277959\n",
      "iteration 11000 / 14000: loss 1.022213\n",
      "iteration 11100 / 14000: loss 1.082931\n",
      "iteration 11200 / 14000: loss 1.056201\n",
      "epoch done... acc 0.517\n",
      "iteration 11300 / 14000: loss 1.199955\n",
      "iteration 11400 / 14000: loss 1.163244\n",
      "iteration 11500 / 14000: loss 0.910175\n",
      "iteration 11600 / 14000: loss 1.145311\n",
      "iteration 11700 / 14000: loss 0.937291\n",
      "epoch done... acc 0.528\n",
      "iteration 11800 / 14000: loss 1.146792\n",
      "iteration 11900 / 14000: loss 1.050056\n",
      "iteration 12000 / 14000: loss 1.040795\n",
      "iteration 12100 / 14000: loss 1.013425\n",
      "iteration 12200 / 14000: loss 0.941182\n",
      "epoch done... acc 0.519\n",
      "iteration 12300 / 14000: loss 0.984536\n",
      "iteration 12400 / 14000: loss 1.103724\n",
      "iteration 12500 / 14000: loss 1.081550\n",
      "iteration 12600 / 14000: loss 1.119043\n",
      "iteration 12700 / 14000: loss 1.164340\n",
      "epoch done... acc 0.51\n",
      "iteration 12800 / 14000: loss 1.083673\n",
      "iteration 12900 / 14000: loss 1.080633\n",
      "iteration 13000 / 14000: loss 0.991520\n",
      "iteration 13100 / 14000: loss 0.940606\n",
      "iteration 13200 / 14000: loss 1.048197\n",
      "epoch done... acc 0.521\n",
      "iteration 13300 / 14000: loss 0.973154\n",
      "iteration 13400 / 14000: loss 0.950343\n",
      "iteration 13500 / 14000: loss 1.071006\n",
      "iteration 13600 / 14000: loss 1.092693\n",
      "iteration 13700 / 14000: loss 0.930272\n",
      "epoch done... acc 0.519\n",
      "iteration 13800 / 14000: loss 0.950389\n",
      "iteration 13900 / 14000: loss 1.038937\n",
      "Final training loss:  1.0497176799670564\n",
      "Final validation loss:  1.3923420048952753\n",
      "Final validation accuracy:  0.519\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "18 2 0 385 14000 100 0.001 0.98 0.519\n",
      "iteration 0 / 15400: loss 2.302554\n",
      "epoch done... acc 0.177\n",
      "iteration 100 / 15400: loss 2.051940\n",
      "iteration 200 / 15400: loss 1.882190\n",
      "iteration 300 / 15400: loss 1.671217\n",
      "iteration 400 / 15400: loss 1.669878\n",
      "epoch done... acc 0.377\n",
      "iteration 500 / 15400: loss 1.829552\n",
      "iteration 600 / 15400: loss 1.599518\n",
      "iteration 700 / 15400: loss 1.713964\n",
      "iteration 800 / 15400: loss 1.617769\n",
      "iteration 900 / 15400: loss 1.490561\n",
      "epoch done... acc 0.431\n",
      "iteration 1000 / 15400: loss 1.694235\n",
      "iteration 1100 / 15400: loss 1.734776\n",
      "iteration 1200 / 15400: loss 1.514357\n",
      "iteration 1300 / 15400: loss 1.481315\n",
      "iteration 1400 / 15400: loss 1.492507\n",
      "epoch done... acc 0.434\n",
      "iteration 1500 / 15400: loss 1.536448\n",
      "iteration 1600 / 15400: loss 1.448605\n",
      "iteration 1700 / 15400: loss 1.455232\n",
      "iteration 1800 / 15400: loss 1.547392\n",
      "iteration 1900 / 15400: loss 1.575132\n",
      "epoch done... acc 0.437\n",
      "iteration 2000 / 15400: loss 1.568423\n",
      "iteration 2100 / 15400: loss 1.539311\n",
      "iteration 2200 / 15400: loss 1.453717\n",
      "iteration 2300 / 15400: loss 1.502739\n",
      "iteration 2400 / 15400: loss 1.705152\n",
      "epoch done... acc 0.462\n",
      "iteration 2500 / 15400: loss 1.418225\n",
      "iteration 2600 / 15400: loss 1.471420\n",
      "iteration 2700 / 15400: loss 1.600900\n",
      "iteration 2800 / 15400: loss 1.409417\n",
      "iteration 2900 / 15400: loss 1.490232\n",
      "epoch done... acc 0.475\n",
      "iteration 3000 / 15400: loss 1.437170\n",
      "iteration 3100 / 15400: loss 1.238595\n",
      "iteration 3200 / 15400: loss 1.359183\n",
      "iteration 3300 / 15400: loss 1.383992\n",
      "iteration 3400 / 15400: loss 1.350218\n",
      "epoch done... acc 0.464\n",
      "iteration 3500 / 15400: loss 1.475692\n",
      "iteration 3600 / 15400: loss 1.379468\n",
      "iteration 3700 / 15400: loss 1.251370\n",
      "iteration 3800 / 15400: loss 1.663676\n",
      "iteration 3900 / 15400: loss 1.357281\n",
      "epoch done... acc 0.483\n",
      "iteration 4000 / 15400: loss 1.285588\n",
      "iteration 4100 / 15400: loss 1.553809\n",
      "iteration 4200 / 15400: loss 1.483466\n",
      "iteration 4300 / 15400: loss 1.396551\n",
      "iteration 4400 / 15400: loss 1.323213\n",
      "epoch done... acc 0.485\n",
      "iteration 4500 / 15400: loss 1.420899\n",
      "iteration 4600 / 15400: loss 1.353611\n",
      "iteration 4700 / 15400: loss 1.181698\n",
      "iteration 4800 / 15400: loss 1.380072\n",
      "iteration 4900 / 15400: loss 1.250864\n",
      "epoch done... acc 0.493\n",
      "iteration 5000 / 15400: loss 1.372661\n",
      "iteration 5100 / 15400: loss 1.382527\n",
      "iteration 5200 / 15400: loss 1.285695\n",
      "iteration 5300 / 15400: loss 1.268878\n",
      "epoch done... acc 0.488\n",
      "iteration 5400 / 15400: loss 1.275802\n",
      "iteration 5500 / 15400: loss 1.335820\n",
      "iteration 5600 / 15400: loss 1.276069\n",
      "iteration 5700 / 15400: loss 1.233788\n",
      "iteration 5800 / 15400: loss 1.170201\n",
      "epoch done... acc 0.506\n",
      "iteration 5900 / 15400: loss 1.346868\n",
      "iteration 6000 / 15400: loss 1.215715\n",
      "iteration 6100 / 15400: loss 1.414616\n",
      "iteration 6200 / 15400: loss 1.150614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6300 / 15400: loss 1.278015\n",
      "epoch done... acc 0.518\n",
      "iteration 6400 / 15400: loss 1.465569\n",
      "iteration 6500 / 15400: loss 1.382978\n",
      "iteration 6600 / 15400: loss 1.177289\n",
      "iteration 6700 / 15400: loss 1.221707\n",
      "iteration 6800 / 15400: loss 1.276575\n",
      "epoch done... acc 0.498\n",
      "iteration 6900 / 15400: loss 1.248634\n",
      "iteration 7000 / 15400: loss 1.219558\n",
      "iteration 7100 / 15400: loss 1.275859\n",
      "iteration 7200 / 15400: loss 1.153914\n",
      "iteration 7300 / 15400: loss 1.255486\n",
      "epoch done... acc 0.511\n",
      "iteration 7400 / 15400: loss 1.331353\n",
      "iteration 7500 / 15400: loss 1.200047\n",
      "iteration 7600 / 15400: loss 1.248688\n",
      "iteration 7700 / 15400: loss 1.151212\n",
      "iteration 7800 / 15400: loss 1.090947\n",
      "epoch done... acc 0.512\n",
      "iteration 7900 / 15400: loss 1.139843\n",
      "iteration 8000 / 15400: loss 1.176013\n",
      "iteration 8100 / 15400: loss 1.206081\n",
      "iteration 8200 / 15400: loss 1.112647\n",
      "iteration 8300 / 15400: loss 1.190145\n",
      "epoch done... acc 0.51\n",
      "iteration 8400 / 15400: loss 1.194947\n",
      "iteration 8500 / 15400: loss 1.067603\n",
      "iteration 8600 / 15400: loss 1.174339\n",
      "iteration 8700 / 15400: loss 1.194172\n",
      "iteration 8800 / 15400: loss 1.054312\n",
      "epoch done... acc 0.517\n",
      "iteration 8900 / 15400: loss 1.201561\n",
      "iteration 9000 / 15400: loss 1.142100\n",
      "iteration 9100 / 15400: loss 1.200467\n",
      "iteration 9200 / 15400: loss 1.199957\n",
      "iteration 9300 / 15400: loss 1.100610\n",
      "epoch done... acc 0.501\n",
      "iteration 9400 / 15400: loss 1.250491\n",
      "iteration 9500 / 15400: loss 1.132549\n",
      "iteration 9600 / 15400: loss 1.188045\n",
      "iteration 9700 / 15400: loss 1.081834\n",
      "iteration 9800 / 15400: loss 1.000681\n",
      "epoch done... acc 0.518\n",
      "iteration 9900 / 15400: loss 1.128691\n",
      "iteration 10000 / 15400: loss 1.187760\n",
      "iteration 10100 / 15400: loss 1.028783\n",
      "iteration 10200 / 15400: loss 1.191612\n",
      "epoch done... acc 0.528\n",
      "iteration 10300 / 15400: loss 0.981809\n",
      "iteration 10400 / 15400: loss 1.015194\n",
      "iteration 10500 / 15400: loss 1.192098\n",
      "iteration 10600 / 15400: loss 0.984450\n",
      "iteration 10700 / 15400: loss 1.177318\n",
      "epoch done... acc 0.519\n",
      "iteration 10800 / 15400: loss 1.189505\n",
      "iteration 10900 / 15400: loss 1.072735\n",
      "iteration 11000 / 15400: loss 0.980931\n",
      "iteration 11100 / 15400: loss 1.224335\n",
      "iteration 11200 / 15400: loss 1.205441\n",
      "epoch done... acc 0.527\n",
      "iteration 11300 / 15400: loss 1.062026\n",
      "iteration 11400 / 15400: loss 1.223730\n",
      "iteration 11500 / 15400: loss 1.189129\n",
      "iteration 11600 / 15400: loss 1.245603\n",
      "iteration 11700 / 15400: loss 1.062758\n",
      "epoch done... acc 0.52\n",
      "iteration 11800 / 15400: loss 1.006826\n",
      "iteration 11900 / 15400: loss 0.951851\n",
      "iteration 12000 / 15400: loss 1.098081\n",
      "iteration 12100 / 15400: loss 1.144257\n",
      "iteration 12200 / 15400: loss 1.179804\n",
      "epoch done... acc 0.515\n",
      "iteration 12300 / 15400: loss 0.878301\n",
      "iteration 12400 / 15400: loss 1.012482\n",
      "iteration 12500 / 15400: loss 0.980126\n",
      "iteration 12600 / 15400: loss 1.073298\n",
      "iteration 12700 / 15400: loss 0.949980\n",
      "epoch done... acc 0.509\n",
      "iteration 12800 / 15400: loss 1.291364\n",
      "iteration 12900 / 15400: loss 0.975248\n",
      "iteration 13000 / 15400: loss 1.265563\n",
      "iteration 13100 / 15400: loss 1.130016\n",
      "iteration 13200 / 15400: loss 1.087992\n",
      "epoch done... acc 0.52\n",
      "iteration 13300 / 15400: loss 1.063912\n",
      "iteration 13400 / 15400: loss 1.031802\n",
      "iteration 13500 / 15400: loss 1.297690\n",
      "iteration 13600 / 15400: loss 0.968981\n",
      "iteration 13700 / 15400: loss 1.053574\n",
      "epoch done... acc 0.536\n",
      "iteration 13800 / 15400: loss 1.208080\n",
      "iteration 13900 / 15400: loss 0.995259\n",
      "iteration 14000 / 15400: loss 0.955284\n",
      "iteration 14100 / 15400: loss 1.078057\n",
      "iteration 14200 / 15400: loss 0.915657\n",
      "epoch done... acc 0.528\n",
      "iteration 14300 / 15400: loss 1.088567\n",
      "iteration 14400 / 15400: loss 1.082580\n",
      "iteration 14500 / 15400: loss 0.964048\n",
      "iteration 14600 / 15400: loss 0.792584\n",
      "iteration 14700 / 15400: loss 1.054411\n",
      "epoch done... acc 0.519\n",
      "iteration 14800 / 15400: loss 1.127952\n",
      "iteration 14900 / 15400: loss 0.957779\n",
      "iteration 15000 / 15400: loss 1.020074\n",
      "iteration 15100 / 15400: loss 0.912199\n",
      "epoch done... acc 0.527\n",
      "iteration 15200 / 15400: loss 0.983912\n",
      "iteration 15300 / 15400: loss 0.969449\n",
      "Final training loss:  0.9673083199746163\n",
      "Final validation loss:  1.3682397766933567\n",
      "Final validation accuracy:  0.527\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "19 1 1 385 14000 100 0.001 0.98 0.527\n",
      "iteration 0 / 12600: loss 2.302534\n",
      "epoch done... acc 0.182\n",
      "iteration 100 / 12600: loss 2.000483\n",
      "iteration 200 / 12600: loss 1.830356\n",
      "iteration 300 / 12600: loss 1.759926\n",
      "iteration 400 / 12600: loss 1.848120\n",
      "epoch done... acc 0.382\n",
      "iteration 500 / 12600: loss 1.795753\n",
      "iteration 600 / 12600: loss 1.728883\n",
      "iteration 700 / 12600: loss 1.690470\n",
      "iteration 800 / 12600: loss 1.658044\n",
      "iteration 900 / 12600: loss 1.706799\n",
      "epoch done... acc 0.428\n",
      "iteration 1000 / 12600: loss 1.741314\n",
      "iteration 1100 / 12600: loss 1.664244\n",
      "iteration 1200 / 12600: loss 1.623721\n",
      "iteration 1300 / 12600: loss 1.526327\n",
      "iteration 1400 / 12600: loss 1.650195\n",
      "epoch done... acc 0.446\n",
      "iteration 1500 / 12600: loss 1.540188\n",
      "iteration 1600 / 12600: loss 1.511535\n",
      "iteration 1700 / 12600: loss 1.560503\n",
      "iteration 1800 / 12600: loss 1.531367\n",
      "iteration 1900 / 12600: loss 1.446824\n",
      "epoch done... acc 0.462\n",
      "iteration 2000 / 12600: loss 1.597398\n",
      "iteration 2100 / 12600: loss 1.471734\n",
      "iteration 2200 / 12600: loss 1.398855\n",
      "iteration 2300 / 12600: loss 1.375711\n",
      "iteration 2400 / 12600: loss 1.455668\n",
      "epoch done... acc 0.464\n",
      "iteration 2500 / 12600: loss 1.523555\n",
      "iteration 2600 / 12600: loss 1.242323\n",
      "iteration 2700 / 12600: loss 1.353142\n",
      "iteration 2800 / 12600: loss 1.318402\n",
      "iteration 2900 / 12600: loss 1.336056\n",
      "epoch done... acc 0.472\n",
      "iteration 3000 / 12600: loss 1.428690\n",
      "iteration 3100 / 12600: loss 1.398590\n",
      "iteration 3200 / 12600: loss 1.439706\n",
      "iteration 3300 / 12600: loss 1.509906\n",
      "iteration 3400 / 12600: loss 1.558000\n",
      "epoch done... acc 0.477\n",
      "iteration 3500 / 12600: loss 1.349689\n",
      "iteration 3600 / 12600: loss 1.514080\n",
      "iteration 3700 / 12600: loss 1.333718\n",
      "iteration 3800 / 12600: loss 1.379082\n",
      "iteration 3900 / 12600: loss 1.382611\n",
      "epoch done... acc 0.492\n",
      "iteration 4000 / 12600: loss 1.333023\n",
      "iteration 4100 / 12600: loss 1.300465\n",
      "iteration 4200 / 12600: loss 1.273365\n",
      "iteration 4300 / 12600: loss 1.171737\n",
      "iteration 4400 / 12600: loss 1.471171\n",
      "epoch done... acc 0.491\n",
      "iteration 4500 / 12600: loss 1.271728\n",
      "iteration 4600 / 12600: loss 1.427702\n",
      "iteration 4700 / 12600: loss 1.192912\n",
      "iteration 4800 / 12600: loss 1.237575\n",
      "iteration 4900 / 12600: loss 1.238965\n",
      "epoch done... acc 0.502\n",
      "iteration 5000 / 12600: loss 1.344120\n",
      "iteration 5100 / 12600: loss 1.239434\n",
      "iteration 5200 / 12600: loss 1.260890\n",
      "iteration 5300 / 12600: loss 1.127391\n",
      "epoch done... acc 0.499\n",
      "iteration 5400 / 12600: loss 1.168123\n",
      "iteration 5500 / 12600: loss 1.425577\n",
      "iteration 5600 / 12600: loss 1.257245\n",
      "iteration 5700 / 12600: loss 1.255997\n",
      "iteration 5800 / 12600: loss 1.403537\n",
      "epoch done... acc 0.497\n",
      "iteration 5900 / 12600: loss 1.335493\n",
      "iteration 6000 / 12600: loss 1.272094\n",
      "iteration 6100 / 12600: loss 1.322538\n",
      "iteration 6200 / 12600: loss 1.088630\n",
      "iteration 6300 / 12600: loss 1.254473\n",
      "epoch done... acc 0.495\n",
      "iteration 6400 / 12600: loss 1.370050\n",
      "iteration 6500 / 12600: loss 1.173891\n",
      "iteration 6600 / 12600: loss 1.099209\n",
      "iteration 6700 / 12600: loss 1.082666\n",
      "iteration 6800 / 12600: loss 1.263846\n",
      "epoch done... acc 0.502\n",
      "iteration 6900 / 12600: loss 1.196090\n",
      "iteration 7000 / 12600: loss 1.356828\n",
      "iteration 7100 / 12600: loss 1.203742\n",
      "iteration 7200 / 12600: loss 1.252679\n",
      "iteration 7300 / 12600: loss 1.137165\n",
      "epoch done... acc 0.515\n",
      "iteration 7400 / 12600: loss 1.246982\n",
      "iteration 7500 / 12600: loss 1.186525\n",
      "iteration 7600 / 12600: loss 1.278474\n",
      "iteration 7700 / 12600: loss 1.175167\n",
      "iteration 7800 / 12600: loss 1.221061\n",
      "epoch done... acc 0.512\n",
      "iteration 7900 / 12600: loss 1.213842\n",
      "iteration 8000 / 12600: loss 1.119933\n",
      "iteration 8100 / 12600: loss 1.209921\n",
      "iteration 8200 / 12600: loss 1.354153\n",
      "iteration 8300 / 12600: loss 1.382687\n",
      "epoch done... acc 0.524\n",
      "iteration 8400 / 12600: loss 1.116982\n",
      "iteration 8500 / 12600: loss 1.283685\n",
      "iteration 8600 / 12600: loss 1.117868\n",
      "iteration 8700 / 12600: loss 1.215376\n",
      "iteration 8800 / 12600: loss 1.091500\n",
      "epoch done... acc 0.515\n",
      "iteration 8900 / 12600: loss 1.135867\n",
      "iteration 9000 / 12600: loss 1.094259\n",
      "iteration 9100 / 12600: loss 1.232530\n",
      "iteration 9200 / 12600: loss 1.238451\n",
      "iteration 9300 / 12600: loss 1.017858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch done... acc 0.52\n",
      "iteration 9400 / 12600: loss 1.044545\n",
      "iteration 9500 / 12600: loss 1.027025\n",
      "iteration 9600 / 12600: loss 1.082864\n",
      "iteration 9700 / 12600: loss 1.245430\n",
      "iteration 9800 / 12600: loss 1.337400\n",
      "epoch done... acc 0.515\n",
      "iteration 9900 / 12600: loss 1.056111\n",
      "iteration 10000 / 12600: loss 0.937944\n",
      "iteration 10100 / 12600: loss 1.092392\n",
      "iteration 10200 / 12600: loss 1.111439\n",
      "epoch done... acc 0.511\n",
      "iteration 10300 / 12600: loss 1.217583\n",
      "iteration 10400 / 12600: loss 1.084195\n",
      "iteration 10500 / 12600: loss 1.018279\n",
      "iteration 10600 / 12600: loss 1.124371\n",
      "iteration 10700 / 12600: loss 1.088195\n",
      "epoch done... acc 0.515\n",
      "iteration 10800 / 12600: loss 1.016736\n",
      "iteration 10900 / 12600: loss 1.106655\n",
      "iteration 11000 / 12600: loss 1.093392\n",
      "iteration 11100 / 12600: loss 1.137026\n",
      "iteration 11200 / 12600: loss 1.096645\n",
      "epoch done... acc 0.512\n",
      "iteration 11300 / 12600: loss 1.091449\n",
      "iteration 11400 / 12600: loss 1.154539\n",
      "iteration 11500 / 12600: loss 1.105484\n",
      "iteration 11600 / 12600: loss 1.010875\n",
      "iteration 11700 / 12600: loss 1.032255\n",
      "epoch done... acc 0.527\n",
      "iteration 11800 / 12600: loss 1.096200\n",
      "iteration 11900 / 12600: loss 1.385923\n",
      "iteration 12000 / 12600: loss 0.946827\n",
      "iteration 12100 / 12600: loss 0.900541\n",
      "iteration 12200 / 12600: loss 1.037300\n",
      "epoch done... acc 0.54\n",
      "iteration 12300 / 12600: loss 1.100241\n",
      "iteration 12400 / 12600: loss 0.906739\n",
      "iteration 12500 / 12600: loss 0.990478\n",
      "Final training loss:  0.977979342377711\n",
      "Final validation loss:  1.361770527306437\n",
      "Final validation accuracy:  0.54\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "20 2 1 385 14000 100 0.001 0.98 0.54\n",
      "iteration 0 / 14000: loss 2.302530\n",
      "epoch done... acc 0.153\n",
      "iteration 100 / 14000: loss 2.077371\n",
      "iteration 200 / 14000: loss 1.794645\n",
      "iteration 300 / 14000: loss 1.821517\n",
      "iteration 400 / 14000: loss 1.854231\n",
      "epoch done... acc 0.397\n",
      "iteration 500 / 14000: loss 1.810127\n",
      "iteration 600 / 14000: loss 1.568966\n",
      "iteration 700 / 14000: loss 1.676343\n",
      "iteration 800 / 14000: loss 1.654370\n",
      "iteration 900 / 14000: loss 1.465714\n",
      "epoch done... acc 0.411\n",
      "iteration 1000 / 14000: loss 1.661933\n",
      "iteration 1100 / 14000: loss 1.497586\n",
      "iteration 1200 / 14000: loss 1.562191\n",
      "iteration 1300 / 14000: loss 1.620181\n",
      "iteration 1400 / 14000: loss 1.680971\n",
      "epoch done... acc 0.433\n",
      "iteration 1500 / 14000: loss 1.503900\n",
      "iteration 1600 / 14000: loss 1.438449\n",
      "iteration 1700 / 14000: loss 1.757215\n",
      "iteration 1800 / 14000: loss 1.577161\n",
      "iteration 1900 / 14000: loss 1.593211\n",
      "epoch done... acc 0.456\n",
      "iteration 2000 / 14000: loss 1.522400\n",
      "iteration 2100 / 14000: loss 1.589305\n",
      "iteration 2200 / 14000: loss 1.279651\n",
      "iteration 2300 / 14000: loss 1.781229\n",
      "iteration 2400 / 14000: loss 1.320858\n",
      "epoch done... acc 0.456\n",
      "iteration 2500 / 14000: loss 1.415895\n",
      "iteration 2600 / 14000: loss 1.469424\n",
      "iteration 2700 / 14000: loss 1.551214\n",
      "iteration 2800 / 14000: loss 1.455829\n",
      "iteration 2900 / 14000: loss 1.397915\n",
      "epoch done... acc 0.486\n",
      "iteration 3000 / 14000: loss 1.249853\n",
      "iteration 3100 / 14000: loss 1.520657\n",
      "iteration 3200 / 14000: loss 1.449606\n",
      "iteration 3300 / 14000: loss 1.523614\n",
      "iteration 3400 / 14000: loss 1.439081\n",
      "epoch done... acc 0.478\n",
      "iteration 3500 / 14000: loss 1.516789\n",
      "iteration 3600 / 14000: loss 1.509664\n",
      "iteration 3700 / 14000: loss 1.422844\n",
      "iteration 3800 / 14000: loss 1.298336\n",
      "iteration 3900 / 14000: loss 1.376658\n",
      "epoch done... acc 0.507\n",
      "iteration 4000 / 14000: loss 1.292769\n",
      "iteration 4100 / 14000: loss 1.397451\n",
      "iteration 4200 / 14000: loss 1.292104\n",
      "iteration 4300 / 14000: loss 1.443611\n",
      "iteration 4400 / 14000: loss 1.314707\n",
      "epoch done... acc 0.483\n",
      "iteration 4500 / 14000: loss 1.168618\n",
      "iteration 4600 / 14000: loss 1.387839\n",
      "iteration 4700 / 14000: loss 1.340458\n",
      "iteration 4800 / 14000: loss 1.279019\n",
      "iteration 4900 / 14000: loss 1.250745\n",
      "epoch done... acc 0.504\n",
      "iteration 5000 / 14000: loss 1.414874\n",
      "iteration 5100 / 14000: loss 1.293544\n",
      "iteration 5200 / 14000: loss 1.452914\n",
      "iteration 5300 / 14000: loss 1.338013\n",
      "epoch done... acc 0.505\n",
      "iteration 5400 / 14000: loss 1.215093\n",
      "iteration 5500 / 14000: loss 1.142352\n",
      "iteration 5600 / 14000: loss 1.448694\n",
      "iteration 5700 / 14000: loss 1.216446\n",
      "iteration 5800 / 14000: loss 1.197923\n",
      "epoch done... acc 0.508\n",
      "iteration 5900 / 14000: loss 1.090951\n",
      "iteration 6000 / 14000: loss 1.092562\n",
      "iteration 6100 / 14000: loss 1.391575\n",
      "iteration 6200 / 14000: loss 1.225684\n",
      "iteration 6300 / 14000: loss 1.143001\n",
      "epoch done... acc 0.517\n",
      "iteration 6400 / 14000: loss 1.311243\n",
      "iteration 6500 / 14000: loss 1.333534\n",
      "iteration 6600 / 14000: loss 1.299656\n",
      "iteration 6700 / 14000: loss 1.293258\n",
      "iteration 6800 / 14000: loss 1.327497\n",
      "epoch done... acc 0.517\n",
      "iteration 6900 / 14000: loss 1.046688\n",
      "iteration 7000 / 14000: loss 1.154438\n",
      "iteration 7100 / 14000: loss 1.231512\n",
      "iteration 7200 / 14000: loss 1.113735\n",
      "iteration 7300 / 14000: loss 1.321269\n",
      "epoch done... acc 0.515\n",
      "iteration 7400 / 14000: loss 1.274102\n",
      "iteration 7500 / 14000: loss 1.212210\n",
      "iteration 7600 / 14000: loss 1.126007\n",
      "iteration 7700 / 14000: loss 1.207668\n",
      "iteration 7800 / 14000: loss 1.198438\n",
      "epoch done... acc 0.521\n",
      "iteration 7900 / 14000: loss 1.064417\n",
      "iteration 8000 / 14000: loss 1.106796\n",
      "iteration 8100 / 14000: loss 1.310049\n",
      "iteration 8200 / 14000: loss 1.028051\n",
      "iteration 8300 / 14000: loss 1.299006\n",
      "epoch done... acc 0.539\n",
      "iteration 8400 / 14000: loss 1.201692\n",
      "iteration 8500 / 14000: loss 1.287364\n",
      "iteration 8600 / 14000: loss 1.138047\n",
      "iteration 8700 / 14000: loss 1.061781\n",
      "iteration 8800 / 14000: loss 1.173974\n",
      "epoch done... acc 0.52\n",
      "iteration 8900 / 14000: loss 1.145773\n",
      "iteration 9000 / 14000: loss 1.143549\n",
      "iteration 9100 / 14000: loss 1.056531\n",
      "iteration 9200 / 14000: loss 1.208305\n",
      "iteration 9300 / 14000: loss 1.174829\n",
      "epoch done... acc 0.514\n",
      "iteration 9400 / 14000: loss 1.040138\n",
      "iteration 9500 / 14000: loss 1.131608\n",
      "iteration 9600 / 14000: loss 1.040566\n",
      "iteration 9700 / 14000: loss 1.229991\n",
      "iteration 9800 / 14000: loss 1.235845\n",
      "epoch done... acc 0.519\n",
      "iteration 9900 / 14000: loss 1.094937\n",
      "iteration 10000 / 14000: loss 1.120581\n",
      "iteration 10100 / 14000: loss 1.053705\n",
      "iteration 10200 / 14000: loss 1.246279\n",
      "epoch done... acc 0.53\n",
      "iteration 10300 / 14000: loss 1.117014\n",
      "iteration 10400 / 14000: loss 1.296604\n",
      "iteration 10500 / 14000: loss 1.084976\n",
      "iteration 10600 / 14000: loss 0.964864\n",
      "iteration 10700 / 14000: loss 0.973786\n",
      "epoch done... acc 0.516\n",
      "iteration 10800 / 14000: loss 1.117692\n",
      "iteration 10900 / 14000: loss 1.377945\n",
      "iteration 11000 / 14000: loss 1.268660\n",
      "iteration 11100 / 14000: loss 1.055168\n",
      "iteration 11200 / 14000: loss 1.188308\n",
      "epoch done... acc 0.521\n",
      "iteration 11300 / 14000: loss 0.984879\n",
      "iteration 11400 / 14000: loss 0.978091\n",
      "iteration 11500 / 14000: loss 1.136946\n",
      "iteration 11600 / 14000: loss 0.974623\n",
      "iteration 11700 / 14000: loss 1.051852\n",
      "epoch done... acc 0.515\n",
      "iteration 11800 / 14000: loss 1.141708\n",
      "iteration 11900 / 14000: loss 1.060645\n",
      "iteration 12000 / 14000: loss 1.058860\n",
      "iteration 12100 / 14000: loss 1.035558\n",
      "iteration 12200 / 14000: loss 1.138905\n",
      "epoch done... acc 0.527\n",
      "iteration 12300 / 14000: loss 1.119575\n",
      "iteration 12400 / 14000: loss 1.163584\n",
      "iteration 12500 / 14000: loss 0.910574\n",
      "iteration 12600 / 14000: loss 1.089127\n",
      "iteration 12700 / 14000: loss 0.919823\n",
      "epoch done... acc 0.519\n",
      "iteration 12800 / 14000: loss 1.023740\n",
      "iteration 12900 / 14000: loss 1.126575\n",
      "iteration 13000 / 14000: loss 0.978792\n",
      "iteration 13100 / 14000: loss 1.023875\n",
      "iteration 13200 / 14000: loss 0.939810\n",
      "epoch done... acc 0.53\n",
      "iteration 13300 / 14000: loss 1.184443\n",
      "iteration 13400 / 14000: loss 0.868750\n",
      "iteration 13500 / 14000: loss 0.931455\n",
      "iteration 13600 / 14000: loss 1.174844\n",
      "iteration 13700 / 14000: loss 1.099689\n",
      "epoch done... acc 0.543\n",
      "iteration 13800 / 14000: loss 1.187976\n",
      "iteration 13900 / 14000: loss 0.897478\n",
      "Final training loss:  1.0090258978666895\n",
      "Final validation loss:  1.3610710171013256\n",
      "Final validation accuracy:  0.543\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "21 1 0 385 14000 100 0.001 0.98 0.543\n",
      "iteration 0 / 14000: loss 2.302612\n",
      "epoch done... acc 0.16\n",
      "iteration 100 / 14000: loss 2.003704\n",
      "iteration 200 / 14000: loss 1.851922\n",
      "iteration 300 / 14000: loss 1.903199\n",
      "iteration 400 / 14000: loss 1.743254\n",
      "epoch done... acc 0.388\n",
      "iteration 500 / 14000: loss 1.753860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 600 / 14000: loss 1.715153\n",
      "iteration 700 / 14000: loss 1.728436\n",
      "iteration 800 / 14000: loss 1.740899\n",
      "iteration 900 / 14000: loss 1.628011\n",
      "epoch done... acc 0.421\n",
      "iteration 1000 / 14000: loss 1.590147\n",
      "iteration 1100 / 14000: loss 1.691796\n",
      "iteration 1200 / 14000: loss 1.573987\n",
      "iteration 1300 / 14000: loss 1.562839\n",
      "iteration 1400 / 14000: loss 1.534653\n",
      "epoch done... acc 0.452\n",
      "iteration 1500 / 14000: loss 1.600959\n",
      "iteration 1600 / 14000: loss 1.516470\n",
      "iteration 1700 / 14000: loss 1.426823\n",
      "iteration 1800 / 14000: loss 1.411035\n",
      "iteration 1900 / 14000: loss 1.546336\n",
      "epoch done... acc 0.445\n",
      "iteration 2000 / 14000: loss 1.352797\n",
      "iteration 2100 / 14000: loss 1.744644\n",
      "iteration 2200 / 14000: loss 1.573901\n",
      "iteration 2300 / 14000: loss 1.392073\n",
      "iteration 2400 / 14000: loss 1.552659\n",
      "epoch done... acc 0.465\n",
      "iteration 2500 / 14000: loss 1.594108\n",
      "iteration 2600 / 14000: loss 1.410329\n",
      "iteration 2700 / 14000: loss 1.540077\n",
      "iteration 2800 / 14000: loss 1.525041\n",
      "iteration 2900 / 14000: loss 1.339720\n",
      "epoch done... acc 0.466\n",
      "iteration 3000 / 14000: loss 1.338498\n",
      "iteration 3100 / 14000: loss 1.418025\n",
      "iteration 3200 / 14000: loss 1.483284\n",
      "iteration 3300 / 14000: loss 1.592552\n",
      "iteration 3400 / 14000: loss 1.491964\n",
      "epoch done... acc 0.482\n",
      "iteration 3500 / 14000: loss 1.446787\n",
      "iteration 3600 / 14000: loss 1.348377\n",
      "iteration 3700 / 14000: loss 1.174951\n",
      "iteration 3800 / 14000: loss 1.435879\n",
      "iteration 3900 / 14000: loss 1.495237\n",
      "epoch done... acc 0.483\n",
      "iteration 4000 / 14000: loss 1.204487\n",
      "iteration 4100 / 14000: loss 1.319911\n",
      "iteration 4200 / 14000: loss 1.416303\n",
      "iteration 4300 / 14000: loss 1.370159\n",
      "iteration 4400 / 14000: loss 1.310024\n",
      "epoch done... acc 0.494\n",
      "iteration 4500 / 14000: loss 1.312249\n",
      "iteration 4600 / 14000: loss 1.557885\n",
      "iteration 4700 / 14000: loss 1.333867\n",
      "iteration 4800 / 14000: loss 1.314449\n",
      "iteration 4900 / 14000: loss 1.200085\n",
      "epoch done... acc 0.505\n",
      "iteration 5000 / 14000: loss 1.248127\n",
      "iteration 5100 / 14000: loss 1.128501\n",
      "iteration 5200 / 14000: loss 1.386386\n",
      "iteration 5300 / 14000: loss 1.410408\n",
      "epoch done... acc 0.495\n",
      "iteration 5400 / 14000: loss 1.476721\n",
      "iteration 5500 / 14000: loss 1.140486\n",
      "iteration 5600 / 14000: loss 1.259449\n",
      "iteration 5700 / 14000: loss 1.207570\n",
      "iteration 5800 / 14000: loss 1.201453\n",
      "epoch done... acc 0.494\n",
      "iteration 5900 / 14000: loss 1.186630\n",
      "iteration 6000 / 14000: loss 1.358930\n",
      "iteration 6100 / 14000: loss 1.186418\n",
      "iteration 6200 / 14000: loss 1.114429\n",
      "iteration 6300 / 14000: loss 1.136655\n",
      "epoch done... acc 0.503\n",
      "iteration 6400 / 14000: loss 1.219120\n",
      "iteration 6500 / 14000: loss 1.307483\n",
      "iteration 6600 / 14000: loss 1.405629\n",
      "iteration 6700 / 14000: loss 1.245381\n",
      "iteration 6800 / 14000: loss 1.304560\n",
      "epoch done... acc 0.523\n",
      "iteration 6900 / 14000: loss 1.230977\n",
      "iteration 7000 / 14000: loss 1.195766\n",
      "iteration 7100 / 14000: loss 1.098735\n",
      "iteration 7200 / 14000: loss 1.169623\n",
      "iteration 7300 / 14000: loss 1.128613\n",
      "epoch done... acc 0.517\n",
      "iteration 7400 / 14000: loss 1.203419\n",
      "iteration 7500 / 14000: loss 1.240729\n",
      "iteration 7600 / 14000: loss 1.122822\n",
      "iteration 7700 / 14000: loss 1.449440\n",
      "iteration 7800 / 14000: loss 1.065374\n",
      "epoch done... acc 0.516\n",
      "iteration 7900 / 14000: loss 1.176141\n",
      "iteration 8000 / 14000: loss 1.179813\n",
      "iteration 8100 / 14000: loss 1.035475\n",
      "iteration 8200 / 14000: loss 1.128733\n",
      "iteration 8300 / 14000: loss 1.279593\n",
      "epoch done... acc 0.525\n",
      "iteration 8400 / 14000: loss 1.248329\n",
      "iteration 8500 / 14000: loss 1.081976\n",
      "iteration 8600 / 14000: loss 1.064817\n",
      "iteration 8700 / 14000: loss 1.102463\n",
      "iteration 8800 / 14000: loss 1.157302\n",
      "epoch done... acc 0.525\n",
      "iteration 8900 / 14000: loss 1.235344\n",
      "iteration 9000 / 14000: loss 1.196315\n",
      "iteration 9100 / 14000: loss 1.176874\n",
      "iteration 9200 / 14000: loss 1.270016\n",
      "iteration 9300 / 14000: loss 1.224457\n",
      "epoch done... acc 0.512\n",
      "iteration 9400 / 14000: loss 1.221850\n",
      "iteration 9500 / 14000: loss 1.231620\n",
      "iteration 9600 / 14000: loss 1.218836\n",
      "iteration 9700 / 14000: loss 1.240661\n",
      "iteration 9800 / 14000: loss 1.031169\n",
      "epoch done... acc 0.52\n",
      "iteration 9900 / 14000: loss 1.196365\n",
      "iteration 10000 / 14000: loss 1.076389\n",
      "iteration 10100 / 14000: loss 1.019600\n",
      "iteration 10200 / 14000: loss 1.096948\n",
      "epoch done... acc 0.518\n",
      "iteration 10300 / 14000: loss 1.182490\n",
      "iteration 10400 / 14000: loss 1.196589\n",
      "iteration 10500 / 14000: loss 1.044520\n",
      "iteration 10600 / 14000: loss 0.948292\n",
      "iteration 10700 / 14000: loss 1.051598\n",
      "epoch done... acc 0.517\n",
      "iteration 10800 / 14000: loss 1.077131\n",
      "iteration 10900 / 14000: loss 1.063362\n",
      "iteration 11000 / 14000: loss 1.229621\n",
      "iteration 11100 / 14000: loss 1.046745\n",
      "iteration 11200 / 14000: loss 1.103341\n",
      "epoch done... acc 0.513\n",
      "iteration 11300 / 14000: loss 0.921134\n",
      "iteration 11400 / 14000: loss 1.035992\n",
      "iteration 11500 / 14000: loss 1.150158\n",
      "iteration 11600 / 14000: loss 1.066995\n",
      "iteration 11700 / 14000: loss 1.171347\n",
      "epoch done... acc 0.519\n",
      "iteration 11800 / 14000: loss 1.140262\n",
      "iteration 11900 / 14000: loss 1.067809\n",
      "iteration 12000 / 14000: loss 1.189884\n",
      "iteration 12100 / 14000: loss 1.027816\n",
      "iteration 12200 / 14000: loss 1.062334\n",
      "epoch done... acc 0.527\n",
      "iteration 12300 / 14000: loss 0.987891\n",
      "iteration 12400 / 14000: loss 1.184814\n",
      "iteration 12500 / 14000: loss 1.136957\n",
      "iteration 12600 / 14000: loss 1.024429\n",
      "iteration 12700 / 14000: loss 1.339364\n",
      "epoch done... acc 0.507\n",
      "iteration 12800 / 14000: loss 1.145974\n",
      "iteration 12900 / 14000: loss 0.930636\n",
      "iteration 13000 / 14000: loss 0.983426\n",
      "iteration 13100 / 14000: loss 0.989448\n",
      "iteration 13200 / 14000: loss 1.087151\n",
      "epoch done... acc 0.52\n",
      "iteration 13300 / 14000: loss 1.096768\n",
      "iteration 13400 / 14000: loss 1.104570\n",
      "iteration 13500 / 14000: loss 1.097642\n",
      "iteration 13600 / 14000: loss 1.134227\n",
      "iteration 13700 / 14000: loss 1.114023\n",
      "epoch done... acc 0.516\n",
      "iteration 13800 / 14000: loss 0.982490\n",
      "iteration 13900 / 14000: loss 1.063366\n",
      "Final training loss:  1.0025090280080093\n",
      "Final validation loss:  1.3626009201494937\n",
      "Final validation accuracy:  0.516\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "22 2 0 385 14000 100 0.001 0.98 0.516\n",
      "iteration 0 / 15400: loss 2.302514\n",
      "epoch done... acc 0.159\n",
      "iteration 100 / 15400: loss 2.020541\n",
      "iteration 200 / 15400: loss 2.014162\n",
      "iteration 300 / 15400: loss 1.970747\n",
      "iteration 400 / 15400: loss 1.638831\n",
      "epoch done... acc 0.39\n",
      "iteration 500 / 15400: loss 1.510487\n",
      "iteration 600 / 15400: loss 1.775468\n",
      "iteration 700 / 15400: loss 1.633858\n",
      "iteration 800 / 15400: loss 1.850521\n",
      "iteration 900 / 15400: loss 1.639679\n",
      "epoch done... acc 0.417\n",
      "iteration 1000 / 15400: loss 1.554886\n",
      "iteration 1100 / 15400: loss 1.735292\n",
      "iteration 1200 / 15400: loss 1.548122\n",
      "iteration 1300 / 15400: loss 1.426512\n",
      "iteration 1400 / 15400: loss 1.849106\n",
      "epoch done... acc 0.449\n",
      "iteration 1500 / 15400: loss 1.508007\n",
      "iteration 1600 / 15400: loss 1.496757\n",
      "iteration 1700 / 15400: loss 1.608099\n",
      "iteration 1800 / 15400: loss 1.539018\n",
      "iteration 1900 / 15400: loss 1.406117\n",
      "epoch done... acc 0.462\n",
      "iteration 2000 / 15400: loss 1.752693\n",
      "iteration 2100 / 15400: loss 1.508559\n",
      "iteration 2200 / 15400: loss 1.632706\n",
      "iteration 2300 / 15400: loss 1.323782\n",
      "iteration 2400 / 15400: loss 1.449784\n",
      "epoch done... acc 0.466\n",
      "iteration 2500 / 15400: loss 1.407534\n",
      "iteration 2600 / 15400: loss 1.467357\n",
      "iteration 2700 / 15400: loss 1.406540\n",
      "iteration 2800 / 15400: loss 1.566580\n",
      "iteration 2900 / 15400: loss 1.320314\n",
      "epoch done... acc 0.459\n",
      "iteration 3000 / 15400: loss 1.465252\n",
      "iteration 3100 / 15400: loss 1.478599\n",
      "iteration 3200 / 15400: loss 1.455111\n",
      "iteration 3300 / 15400: loss 1.482796\n",
      "iteration 3400 / 15400: loss 1.353229\n",
      "epoch done... acc 0.493\n",
      "iteration 3500 / 15400: loss 1.331082\n",
      "iteration 3600 / 15400: loss 1.436052\n",
      "iteration 3700 / 15400: loss 1.307771\n",
      "iteration 3800 / 15400: loss 1.458067\n",
      "iteration 3900 / 15400: loss 1.397517\n",
      "epoch done... acc 0.494\n",
      "iteration 4000 / 15400: loss 1.273408\n",
      "iteration 4100 / 15400: loss 1.194054\n",
      "iteration 4200 / 15400: loss 1.233030\n",
      "iteration 4300 / 15400: loss 1.308253\n",
      "iteration 4400 / 15400: loss 1.287261\n",
      "epoch done... acc 0.476\n",
      "iteration 4500 / 15400: loss 1.323325\n",
      "iteration 4600 / 15400: loss 1.219923\n",
      "iteration 4700 / 15400: loss 1.335138\n",
      "iteration 4800 / 15400: loss 1.238038\n",
      "iteration 4900 / 15400: loss 1.343591\n",
      "epoch done... acc 0.489\n",
      "iteration 5000 / 15400: loss 1.412946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 5100 / 15400: loss 1.414491\n",
      "iteration 5200 / 15400: loss 1.309999\n",
      "iteration 5300 / 15400: loss 1.221481\n",
      "epoch done... acc 0.514\n",
      "iteration 5400 / 15400: loss 1.322697\n",
      "iteration 5500 / 15400: loss 1.250932\n",
      "iteration 5600 / 15400: loss 1.100902\n",
      "iteration 5700 / 15400: loss 1.340230\n",
      "iteration 5800 / 15400: loss 1.370872\n",
      "epoch done... acc 0.507\n",
      "iteration 5900 / 15400: loss 1.104678\n",
      "iteration 6000 / 15400: loss 1.151800\n",
      "iteration 6100 / 15400: loss 1.295015\n",
      "iteration 6200 / 15400: loss 1.347481\n",
      "iteration 6300 / 15400: loss 1.228266\n",
      "epoch done... acc 0.503\n",
      "iteration 6400 / 15400: loss 1.364807\n",
      "iteration 6500 / 15400: loss 1.199278\n",
      "iteration 6600 / 15400: loss 1.329374\n",
      "iteration 6700 / 15400: loss 1.260476\n",
      "iteration 6800 / 15400: loss 1.144621\n",
      "epoch done... acc 0.497\n",
      "iteration 6900 / 15400: loss 1.278267\n",
      "iteration 7000 / 15400: loss 1.170873\n",
      "iteration 7100 / 15400: loss 1.093515\n",
      "iteration 7200 / 15400: loss 1.060667\n",
      "iteration 7300 / 15400: loss 1.348087\n",
      "epoch done... acc 0.514\n",
      "iteration 7400 / 15400: loss 1.076601\n",
      "iteration 7500 / 15400: loss 1.055667\n",
      "iteration 7600 / 15400: loss 1.183705\n",
      "iteration 7700 / 15400: loss 1.172711\n",
      "iteration 7800 / 15400: loss 1.240996\n",
      "epoch done... acc 0.51\n",
      "iteration 7900 / 15400: loss 1.212126\n",
      "iteration 8000 / 15400: loss 0.959882\n",
      "iteration 8100 / 15400: loss 1.444028\n",
      "iteration 8200 / 15400: loss 1.258663\n",
      "iteration 8300 / 15400: loss 1.286937\n",
      "epoch done... acc 0.513\n",
      "iteration 8400 / 15400: loss 1.191681\n",
      "iteration 8500 / 15400: loss 1.244247\n",
      "iteration 8600 / 15400: loss 1.193554\n",
      "iteration 8700 / 15400: loss 1.114672\n",
      "iteration 8800 / 15400: loss 1.145311\n",
      "epoch done... acc 0.51\n",
      "iteration 8900 / 15400: loss 1.334409\n",
      "iteration 9000 / 15400: loss 1.128535\n",
      "iteration 9100 / 15400: loss 1.263595\n",
      "iteration 9200 / 15400: loss 1.300382\n",
      "iteration 9300 / 15400: loss 1.241867\n",
      "epoch done... acc 0.515\n",
      "iteration 9400 / 15400: loss 1.020221\n",
      "iteration 9500 / 15400: loss 1.077033\n",
      "iteration 9600 / 15400: loss 1.137850\n",
      "iteration 9700 / 15400: loss 1.194263\n",
      "iteration 9800 / 15400: loss 1.240338\n",
      "epoch done... acc 0.522\n",
      "iteration 9900 / 15400: loss 1.214156\n",
      "iteration 10000 / 15400: loss 0.950972\n",
      "iteration 10100 / 15400: loss 1.073815\n",
      "iteration 10200 / 15400: loss 1.085673\n",
      "epoch done... acc 0.515\n",
      "iteration 10300 / 15400: loss 1.152816\n",
      "iteration 10400 / 15400: loss 1.193588\n",
      "iteration 10500 / 15400: loss 1.138607\n",
      "iteration 10600 / 15400: loss 1.218640\n",
      "iteration 10700 / 15400: loss 1.154640\n",
      "epoch done... acc 0.519\n",
      "iteration 10800 / 15400: loss 1.203894\n",
      "iteration 10900 / 15400: loss 0.935511\n",
      "iteration 11000 / 15400: loss 1.046563\n",
      "iteration 11100 / 15400: loss 1.205263\n",
      "iteration 11200 / 15400: loss 1.191691\n",
      "epoch done... acc 0.504\n",
      "iteration 11300 / 15400: loss 1.126401\n",
      "iteration 11400 / 15400: loss 0.958163\n",
      "iteration 11500 / 15400: loss 1.143829\n",
      "iteration 11600 / 15400: loss 1.145889\n",
      "iteration 11700 / 15400: loss 1.119460\n",
      "epoch done... acc 0.513\n",
      "iteration 11800 / 15400: loss 1.143896\n",
      "iteration 11900 / 15400: loss 1.066413\n",
      "iteration 12000 / 15400: loss 1.052079\n",
      "iteration 12100 / 15400: loss 1.059481\n",
      "iteration 12200 / 15400: loss 1.015686\n",
      "epoch done... acc 0.514\n",
      "iteration 12300 / 15400: loss 1.098814\n",
      "iteration 12400 / 15400: loss 0.977022\n",
      "iteration 12500 / 15400: loss 1.050665\n",
      "iteration 12600 / 15400: loss 1.167049\n",
      "iteration 12700 / 15400: loss 1.094477\n",
      "epoch done... acc 0.529\n",
      "iteration 12800 / 15400: loss 0.912913\n",
      "iteration 12900 / 15400: loss 1.006846\n",
      "iteration 13000 / 15400: loss 0.836886\n",
      "iteration 13100 / 15400: loss 0.883565\n",
      "iteration 13200 / 15400: loss 1.076473\n",
      "epoch done... acc 0.513\n",
      "iteration 13300 / 15400: loss 1.138701\n",
      "iteration 13400 / 15400: loss 1.051326\n",
      "iteration 13500 / 15400: loss 1.112334\n",
      "iteration 13600 / 15400: loss 0.932262\n",
      "iteration 13700 / 15400: loss 1.008745\n",
      "epoch done... acc 0.528\n",
      "iteration 13800 / 15400: loss 1.089796\n",
      "iteration 13900 / 15400: loss 1.042677\n",
      "iteration 14000 / 15400: loss 1.000253\n",
      "iteration 14100 / 15400: loss 1.018671\n",
      "iteration 14200 / 15400: loss 1.058667\n",
      "epoch done... acc 0.529\n",
      "iteration 14300 / 15400: loss 0.875687\n",
      "iteration 14400 / 15400: loss 1.024926\n",
      "iteration 14500 / 15400: loss 0.977111\n",
      "iteration 14600 / 15400: loss 0.953179\n",
      "iteration 14700 / 15400: loss 1.008124\n",
      "epoch done... acc 0.524\n",
      "iteration 14800 / 15400: loss 1.073449\n",
      "iteration 14900 / 15400: loss 0.918722\n",
      "iteration 15000 / 15400: loss 1.079255\n",
      "iteration 15100 / 15400: loss 1.053837\n",
      "epoch done... acc 0.523\n",
      "iteration 15200 / 15400: loss 0.798216\n",
      "iteration 15300 / 15400: loss 1.024772\n",
      "Final training loss:  1.0921145852135254\n",
      "Final validation loss:  1.3778519294701501\n",
      "Final validation accuracy:  0.523\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "23 1 1 385 14000 100 0.001 0.98 0.523\n",
      "iteration 0 / 12600: loss 2.302499\n",
      "epoch done... acc 0.136\n",
      "iteration 100 / 12600: loss 2.010107\n",
      "iteration 200 / 12600: loss 1.927020\n",
      "iteration 300 / 12600: loss 1.783521\n",
      "iteration 400 / 12600: loss 1.734094\n",
      "epoch done... acc 0.393\n",
      "iteration 500 / 12600: loss 1.690322\n",
      "iteration 600 / 12600: loss 1.750652\n",
      "iteration 700 / 12600: loss 1.749472\n",
      "iteration 800 / 12600: loss 1.710010\n",
      "iteration 900 / 12600: loss 1.563028\n",
      "epoch done... acc 0.434\n",
      "iteration 1000 / 12600: loss 1.674171\n",
      "iteration 1100 / 12600: loss 1.691763\n",
      "iteration 1200 / 12600: loss 1.691641\n",
      "iteration 1300 / 12600: loss 1.633178\n",
      "iteration 1400 / 12600: loss 1.487626\n",
      "epoch done... acc 0.447\n",
      "iteration 1500 / 12600: loss 1.459554\n",
      "iteration 1600 / 12600: loss 1.442498\n",
      "iteration 1700 / 12600: loss 1.491265\n",
      "iteration 1800 / 12600: loss 1.543125\n",
      "iteration 1900 / 12600: loss 1.537724\n",
      "epoch done... acc 0.47\n",
      "iteration 2000 / 12600: loss 1.514185\n",
      "iteration 2100 / 12600: loss 1.571721\n",
      "iteration 2200 / 12600: loss 1.620784\n",
      "iteration 2300 / 12600: loss 1.600810\n",
      "iteration 2400 / 12600: loss 1.587420\n",
      "epoch done... acc 0.471\n",
      "iteration 2500 / 12600: loss 1.438001\n",
      "iteration 2600 / 12600: loss 1.531353\n",
      "iteration 2700 / 12600: loss 1.339482\n",
      "iteration 2800 / 12600: loss 1.364763\n",
      "iteration 2900 / 12600: loss 1.462355\n",
      "epoch done... acc 0.482\n",
      "iteration 3000 / 12600: loss 1.312394\n",
      "iteration 3100 / 12600: loss 1.538323\n",
      "iteration 3200 / 12600: loss 1.495043\n",
      "iteration 3300 / 12600: loss 1.508053\n",
      "iteration 3400 / 12600: loss 1.204715\n",
      "epoch done... acc 0.481\n",
      "iteration 3500 / 12600: loss 1.477548\n",
      "iteration 3600 / 12600: loss 1.561686\n",
      "iteration 3700 / 12600: loss 1.301907\n",
      "iteration 3800 / 12600: loss 1.257748\n",
      "iteration 3900 / 12600: loss 1.293182\n",
      "epoch done... acc 0.49\n",
      "iteration 4000 / 12600: loss 1.439067\n",
      "iteration 4100 / 12600: loss 1.475643\n",
      "iteration 4200 / 12600: loss 1.231012\n",
      "iteration 4300 / 12600: loss 1.306635\n",
      "iteration 4400 / 12600: loss 1.442662\n",
      "epoch done... acc 0.484\n",
      "iteration 4500 / 12600: loss 1.550323\n",
      "iteration 4600 / 12600: loss 1.289060\n",
      "iteration 4700 / 12600: loss 1.253419\n",
      "iteration 4800 / 12600: loss 1.252367\n",
      "iteration 4900 / 12600: loss 1.341148\n",
      "epoch done... acc 0.52\n",
      "iteration 5000 / 12600: loss 1.222225\n",
      "iteration 5100 / 12600: loss 1.226427\n",
      "iteration 5200 / 12600: loss 1.216745\n",
      "iteration 5300 / 12600: loss 1.196885\n",
      "epoch done... acc 0.498\n",
      "iteration 5400 / 12600: loss 1.402873\n",
      "iteration 5500 / 12600: loss 1.257122\n",
      "iteration 5600 / 12600: loss 1.016653\n",
      "iteration 5700 / 12600: loss 1.370462\n",
      "iteration 5800 / 12600: loss 1.173277\n",
      "epoch done... acc 0.5\n",
      "iteration 5900 / 12600: loss 1.532154\n",
      "iteration 6000 / 12600: loss 1.306832\n",
      "iteration 6100 / 12600: loss 1.339213\n",
      "iteration 6200 / 12600: loss 1.076106\n",
      "iteration 6300 / 12600: loss 1.250602\n",
      "epoch done... acc 0.504\n",
      "iteration 6400 / 12600: loss 1.138036\n",
      "iteration 6500 / 12600: loss 1.357285\n",
      "iteration 6600 / 12600: loss 1.228460\n",
      "iteration 6700 / 12600: loss 1.161000\n",
      "iteration 6800 / 12600: loss 1.248762\n",
      "epoch done... acc 0.502\n",
      "iteration 6900 / 12600: loss 1.203807\n",
      "iteration 7000 / 12600: loss 1.328293\n",
      "iteration 7100 / 12600: loss 1.099028\n",
      "iteration 7200 / 12600: loss 1.255554\n",
      "iteration 7300 / 12600: loss 1.155259\n",
      "epoch done... acc 0.512\n",
      "iteration 7400 / 12600: loss 1.140215\n",
      "iteration 7500 / 12600: loss 0.980765\n",
      "iteration 7600 / 12600: loss 1.112075\n",
      "iteration 7700 / 12600: loss 1.367201\n",
      "iteration 7800 / 12600: loss 1.174718\n",
      "epoch done... acc 0.514\n",
      "iteration 7900 / 12600: loss 1.182747\n",
      "iteration 8000 / 12600: loss 1.199728\n",
      "iteration 8100 / 12600: loss 1.382084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8200 / 12600: loss 1.303786\n",
      "iteration 8300 / 12600: loss 1.010788\n",
      "epoch done... acc 0.507\n",
      "iteration 8400 / 12600: loss 1.009244\n",
      "iteration 8500 / 12600: loss 1.159462\n",
      "iteration 8600 / 12600: loss 1.173122\n",
      "iteration 8700 / 12600: loss 1.312991\n",
      "iteration 8800 / 12600: loss 1.079488\n",
      "epoch done... acc 0.514\n",
      "iteration 8900 / 12600: loss 1.291409\n",
      "iteration 9000 / 12600: loss 1.117553\n",
      "iteration 9100 / 12600: loss 1.155131\n",
      "iteration 9200 / 12600: loss 1.263226\n",
      "iteration 9300 / 12600: loss 1.138799\n",
      "epoch done... acc 0.527\n",
      "iteration 9400 / 12600: loss 1.180747\n",
      "iteration 9500 / 12600: loss 1.204390\n",
      "iteration 9600 / 12600: loss 1.062768\n",
      "iteration 9700 / 12600: loss 1.058502\n",
      "iteration 9800 / 12600: loss 1.054394\n",
      "epoch done... acc 0.518\n",
      "iteration 9900 / 12600: loss 1.145655\n",
      "iteration 10000 / 12600: loss 1.159504\n",
      "iteration 10100 / 12600: loss 0.951667\n",
      "iteration 10200 / 12600: loss 1.230518\n",
      "epoch done... acc 0.529\n",
      "iteration 10300 / 12600: loss 1.097167\n",
      "iteration 10400 / 12600: loss 1.131781\n",
      "iteration 10500 / 12600: loss 1.042564\n",
      "iteration 10600 / 12600: loss 1.142712\n",
      "iteration 10700 / 12600: loss 1.090962\n",
      "epoch done... acc 0.529\n",
      "iteration 10800 / 12600: loss 1.225859\n",
      "iteration 10900 / 12600: loss 1.172449\n",
      "iteration 11000 / 12600: loss 1.145015\n",
      "iteration 11100 / 12600: loss 1.149947\n",
      "iteration 11200 / 12600: loss 1.087877\n",
      "epoch done... acc 0.525\n",
      "iteration 11300 / 12600: loss 1.147641\n",
      "iteration 11400 / 12600: loss 1.160470\n",
      "iteration 11500 / 12600: loss 1.195893\n",
      "iteration 11600 / 12600: loss 0.955443\n",
      "iteration 11700 / 12600: loss 0.948382\n",
      "epoch done... acc 0.528\n",
      "iteration 11800 / 12600: loss 0.922941\n",
      "iteration 11900 / 12600: loss 1.006046\n",
      "iteration 12000 / 12600: loss 1.196123\n",
      "iteration 12100 / 12600: loss 1.037834\n",
      "iteration 12200 / 12600: loss 1.058072\n",
      "epoch done... acc 0.536\n",
      "iteration 12300 / 12600: loss 0.923509\n",
      "iteration 12400 / 12600: loss 1.033197\n",
      "iteration 12500 / 12600: loss 1.077848\n",
      "Final training loss:  0.9468555363132022\n",
      "Final validation loss:  1.3752983683430575\n",
      "Final validation accuracy:  0.536\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "24 2 1 385 14000 100 0.001 0.98 0.536\n",
      "iteration 0 / 14000: loss 2.302568\n",
      "epoch done... acc 0.139\n",
      "iteration 100 / 14000: loss 2.061410\n",
      "iteration 200 / 14000: loss 1.891028\n",
      "iteration 300 / 14000: loss 1.797778\n",
      "iteration 400 / 14000: loss 1.861486\n",
      "epoch done... acc 0.395\n",
      "iteration 500 / 14000: loss 1.778586\n",
      "iteration 600 / 14000: loss 1.805630\n",
      "iteration 700 / 14000: loss 1.558391\n",
      "iteration 800 / 14000: loss 1.666188\n",
      "iteration 900 / 14000: loss 1.712651\n",
      "epoch done... acc 0.414\n",
      "iteration 1000 / 14000: loss 1.591465\n",
      "iteration 1100 / 14000: loss 1.481424\n",
      "iteration 1200 / 14000: loss 1.686384\n",
      "iteration 1300 / 14000: loss 1.544136\n",
      "iteration 1400 / 14000: loss 1.651019\n",
      "epoch done... acc 0.44\n",
      "iteration 1500 / 14000: loss 1.515671\n",
      "iteration 1600 / 14000: loss 1.513136\n",
      "iteration 1700 / 14000: loss 1.391005\n",
      "iteration 1800 / 14000: loss 1.536388\n",
      "iteration 1900 / 14000: loss 1.449343\n",
      "epoch done... acc 0.459\n",
      "iteration 2000 / 14000: loss 1.431717\n",
      "iteration 2100 / 14000: loss 1.671610\n",
      "iteration 2200 / 14000: loss 1.488754\n",
      "iteration 2300 / 14000: loss 1.411187\n",
      "iteration 2400 / 14000: loss 1.458685\n",
      "epoch done... acc 0.475\n",
      "iteration 2500 / 14000: loss 1.239210\n",
      "iteration 2600 / 14000: loss 1.742832\n",
      "iteration 2700 / 14000: loss 1.323476\n",
      "iteration 2800 / 14000: loss 1.317368\n",
      "iteration 2900 / 14000: loss 1.427561\n",
      "epoch done... acc 0.491\n",
      "iteration 3000 / 14000: loss 1.386930\n",
      "iteration 3100 / 14000: loss 1.368374\n",
      "iteration 3200 / 14000: loss 1.352435\n",
      "iteration 3300 / 14000: loss 1.311730\n",
      "iteration 3400 / 14000: loss 1.270760\n",
      "epoch done... acc 0.49\n",
      "iteration 3500 / 14000: loss 1.345673\n",
      "iteration 3600 / 14000: loss 1.516598\n",
      "iteration 3700 / 14000: loss 1.230526\n",
      "iteration 3800 / 14000: loss 1.566802\n",
      "iteration 3900 / 14000: loss 1.418835\n",
      "epoch done... acc 0.49\n",
      "iteration 4000 / 14000: loss 1.324989\n",
      "iteration 4100 / 14000: loss 1.257037\n",
      "iteration 4200 / 14000: loss 1.243437\n",
      "iteration 4300 / 14000: loss 1.324942\n",
      "iteration 4400 / 14000: loss 1.096574\n",
      "epoch done... acc 0.477\n",
      "iteration 4500 / 14000: loss 1.378591\n",
      "iteration 4600 / 14000: loss 1.347135\n",
      "iteration 4700 / 14000: loss 1.092591\n",
      "iteration 4800 / 14000: loss 1.506283\n",
      "iteration 4900 / 14000: loss 1.382595\n",
      "epoch done... acc 0.499\n",
      "iteration 5000 / 14000: loss 1.291973\n",
      "iteration 5100 / 14000: loss 1.431670\n",
      "iteration 5200 / 14000: loss 1.302700\n",
      "iteration 5300 / 14000: loss 1.440028\n",
      "epoch done... acc 0.491\n",
      "iteration 5400 / 14000: loss 1.216870\n",
      "iteration 5500 / 14000: loss 1.672791\n",
      "iteration 5600 / 14000: loss 1.159657\n",
      "iteration 5700 / 14000: loss 1.541509\n",
      "iteration 5800 / 14000: loss 1.193806\n",
      "epoch done... acc 0.501\n",
      "iteration 5900 / 14000: loss 1.376685\n",
      "iteration 6000 / 14000: loss 1.377826\n",
      "iteration 6100 / 14000: loss 1.230894\n",
      "iteration 6200 / 14000: loss 1.171579\n",
      "iteration 6300 / 14000: loss 1.351466\n",
      "epoch done... acc 0.499\n",
      "iteration 6400 / 14000: loss 1.161378\n",
      "iteration 6500 / 14000: loss 1.135799\n",
      "iteration 6600 / 14000: loss 1.297425\n",
      "iteration 6700 / 14000: loss 1.326872\n",
      "iteration 6800 / 14000: loss 1.117883\n",
      "epoch done... acc 0.5\n",
      "iteration 6900 / 14000: loss 1.019803\n",
      "iteration 7000 / 14000: loss 1.197774\n",
      "iteration 7100 / 14000: loss 1.196376\n",
      "iteration 7200 / 14000: loss 1.130777\n",
      "iteration 7300 / 14000: loss 1.224339\n",
      "epoch done... acc 0.513\n",
      "iteration 7400 / 14000: loss 1.119175\n",
      "iteration 7500 / 14000: loss 1.199197\n",
      "iteration 7600 / 14000: loss 1.183346\n",
      "iteration 7700 / 14000: loss 1.063990\n",
      "iteration 7800 / 14000: loss 1.089565\n",
      "epoch done... acc 0.525\n",
      "iteration 7900 / 14000: loss 1.620037\n",
      "iteration 8000 / 14000: loss 1.033844\n",
      "iteration 8100 / 14000: loss 0.986587\n",
      "iteration 8200 / 14000: loss 1.180418\n",
      "iteration 8300 / 14000: loss 1.053209\n",
      "epoch done... acc 0.514\n",
      "iteration 8400 / 14000: loss 1.071090\n",
      "iteration 8500 / 14000: loss 1.219408\n",
      "iteration 8600 / 14000: loss 1.034884\n",
      "iteration 8700 / 14000: loss 1.234274\n",
      "iteration 8800 / 14000: loss 1.070929\n",
      "epoch done... acc 0.517\n",
      "iteration 8900 / 14000: loss 1.152006\n",
      "iteration 9000 / 14000: loss 1.109261\n",
      "iteration 9100 / 14000: loss 1.135011\n",
      "iteration 9200 / 14000: loss 1.184100\n",
      "iteration 9300 / 14000: loss 1.011650\n",
      "epoch done... acc 0.522\n",
      "iteration 9400 / 14000: loss 0.889112\n",
      "iteration 9500 / 14000: loss 1.000279\n",
      "iteration 9600 / 14000: loss 0.951719\n",
      "iteration 9700 / 14000: loss 0.981344\n",
      "iteration 9800 / 14000: loss 1.164773\n",
      "epoch done... acc 0.525\n",
      "iteration 9900 / 14000: loss 1.057837\n",
      "iteration 10000 / 14000: loss 1.009254\n",
      "iteration 10100 / 14000: loss 1.006353\n",
      "iteration 10200 / 14000: loss 1.287276\n",
      "epoch done... acc 0.512\n",
      "iteration 10300 / 14000: loss 1.090609\n",
      "iteration 10400 / 14000: loss 0.946353\n",
      "iteration 10500 / 14000: loss 1.116713\n",
      "iteration 10600 / 14000: loss 1.144252\n",
      "iteration 10700 / 14000: loss 0.996179\n",
      "epoch done... acc 0.531\n",
      "iteration 10800 / 14000: loss 1.035525\n",
      "iteration 10900 / 14000: loss 0.934919\n",
      "iteration 11000 / 14000: loss 1.075743\n",
      "iteration 11100 / 14000: loss 0.989715\n",
      "iteration 11200 / 14000: loss 1.116431\n",
      "epoch done... acc 0.532\n",
      "iteration 11300 / 14000: loss 0.982230\n",
      "iteration 11400 / 14000: loss 0.977480\n",
      "iteration 11500 / 14000: loss 1.038459\n",
      "iteration 11600 / 14000: loss 0.987383\n",
      "iteration 11700 / 14000: loss 1.040214\n",
      "epoch done... acc 0.527\n",
      "iteration 11800 / 14000: loss 0.970918\n",
      "iteration 11900 / 14000: loss 0.967499\n",
      "iteration 12000 / 14000: loss 1.059818\n",
      "iteration 12100 / 14000: loss 0.916394\n",
      "iteration 12200 / 14000: loss 1.074764\n",
      "epoch done... acc 0.519\n",
      "iteration 12300 / 14000: loss 1.339377\n",
      "iteration 12400 / 14000: loss 1.006774\n",
      "iteration 12500 / 14000: loss 0.967209\n",
      "iteration 12600 / 14000: loss 1.125753\n",
      "iteration 12700 / 14000: loss 1.040150\n",
      "epoch done... acc 0.518\n",
      "iteration 12800 / 14000: loss 1.172970\n",
      "iteration 12900 / 14000: loss 1.187778\n",
      "iteration 13000 / 14000: loss 1.019701\n",
      "iteration 13100 / 14000: loss 1.037070\n",
      "iteration 13200 / 14000: loss 1.045690\n",
      "epoch done... acc 0.521\n",
      "iteration 13300 / 14000: loss 1.081925\n",
      "iteration 13400 / 14000: loss 1.027206\n",
      "iteration 13500 / 14000: loss 1.083775\n",
      "iteration 13600 / 14000: loss 0.823843\n",
      "iteration 13700 / 14000: loss 1.013459\n",
      "epoch done... acc 0.511\n",
      "iteration 13800 / 14000: loss 0.995436\n",
      "iteration 13900 / 14000: loss 0.776871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training loss:  0.9922260736447582\n",
      "Final validation loss:  1.3630801748865717\n",
      "Final validation accuracy:  0.511\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "25 1 0 385 14000 100 0.001 0.98 0.511\n",
      "iteration 0 / 14000: loss 2.302560\n",
      "epoch done... acc 0.157\n",
      "iteration 100 / 14000: loss 2.056215\n",
      "iteration 200 / 14000: loss 1.844904\n",
      "iteration 300 / 14000: loss 1.726464\n",
      "iteration 400 / 14000: loss 1.876520\n",
      "epoch done... acc 0.39\n",
      "iteration 500 / 14000: loss 1.722629\n",
      "iteration 600 / 14000: loss 1.861713\n",
      "iteration 700 / 14000: loss 1.593740\n",
      "iteration 800 / 14000: loss 1.806403\n",
      "iteration 900 / 14000: loss 1.879252\n",
      "epoch done... acc 0.407\n",
      "iteration 1000 / 14000: loss 1.462362\n",
      "iteration 1100 / 14000: loss 1.619004\n",
      "iteration 1200 / 14000: loss 1.587258\n",
      "iteration 1300 / 14000: loss 1.572678\n",
      "iteration 1400 / 14000: loss 1.648915\n",
      "epoch done... acc 0.449\n",
      "iteration 1500 / 14000: loss 1.477370\n",
      "iteration 1600 / 14000: loss 1.642713\n",
      "iteration 1700 / 14000: loss 1.540270\n",
      "iteration 1800 / 14000: loss 1.661165\n",
      "iteration 1900 / 14000: loss 1.496564\n",
      "epoch done... acc 0.446\n",
      "iteration 2000 / 14000: loss 1.567001\n",
      "iteration 2100 / 14000: loss 1.609789\n",
      "iteration 2200 / 14000: loss 1.335043\n",
      "iteration 2300 / 14000: loss 1.467331\n",
      "iteration 2400 / 14000: loss 1.364288\n",
      "epoch done... acc 0.477\n",
      "iteration 2500 / 14000: loss 1.523008\n",
      "iteration 2600 / 14000: loss 1.590070\n",
      "iteration 2700 / 14000: loss 1.481836\n",
      "iteration 2800 / 14000: loss 1.407996\n",
      "iteration 2900 / 14000: loss 1.336091\n",
      "epoch done... acc 0.463\n",
      "iteration 3000 / 14000: loss 1.466128\n",
      "iteration 3100 / 14000: loss 1.398723\n",
      "iteration 3200 / 14000: loss 1.365003\n",
      "iteration 3300 / 14000: loss 1.573492\n",
      "iteration 3400 / 14000: loss 1.579922\n",
      "epoch done... acc 0.473\n",
      "iteration 3500 / 14000: loss 1.326047\n",
      "iteration 3600 / 14000: loss 1.284375\n",
      "iteration 3700 / 14000: loss 1.280353\n",
      "iteration 3800 / 14000: loss 1.208303\n",
      "iteration 3900 / 14000: loss 1.403441\n",
      "epoch done... acc 0.474\n",
      "iteration 4000 / 14000: loss 1.195306\n",
      "iteration 4100 / 14000: loss 1.497787\n",
      "iteration 4200 / 14000: loss 1.412928\n",
      "iteration 4300 / 14000: loss 1.444600\n",
      "iteration 4400 / 14000: loss 1.433785\n",
      "epoch done... acc 0.489\n",
      "iteration 4500 / 14000: loss 1.524374\n",
      "iteration 4600 / 14000: loss 1.244312\n",
      "iteration 4700 / 14000: loss 1.191657\n",
      "iteration 4800 / 14000: loss 1.408519\n",
      "iteration 4900 / 14000: loss 1.205256\n",
      "epoch done... acc 0.495\n",
      "iteration 5000 / 14000: loss 1.353225\n",
      "iteration 5100 / 14000: loss 1.408357\n",
      "iteration 5200 / 14000: loss 1.311633\n",
      "iteration 5300 / 14000: loss 1.246440\n",
      "epoch done... acc 0.491\n",
      "iteration 5400 / 14000: loss 1.293694\n",
      "iteration 5500 / 14000: loss 1.298567\n",
      "iteration 5600 / 14000: loss 1.353316\n",
      "iteration 5700 / 14000: loss 1.226557\n",
      "iteration 5800 / 14000: loss 1.264607\n",
      "epoch done... acc 0.494\n",
      "iteration 5900 / 14000: loss 1.398060\n",
      "iteration 6000 / 14000: loss 1.246918\n",
      "iteration 6100 / 14000: loss 1.342656\n",
      "iteration 6200 / 14000: loss 1.167547\n",
      "iteration 6300 / 14000: loss 1.229440\n",
      "epoch done... acc 0.497\n",
      "iteration 6400 / 14000: loss 0.993722\n",
      "iteration 6500 / 14000: loss 1.242806\n",
      "iteration 6600 / 14000: loss 1.359128\n",
      "iteration 6700 / 14000: loss 1.221743\n",
      "iteration 6800 / 14000: loss 1.294032\n",
      "epoch done... acc 0.507\n",
      "iteration 6900 / 14000: loss 1.045814\n",
      "iteration 7000 / 14000: loss 1.104338\n",
      "iteration 7100 / 14000: loss 1.234903\n",
      "iteration 7200 / 14000: loss 1.137461\n",
      "iteration 7300 / 14000: loss 1.133753\n",
      "epoch done... acc 0.506\n",
      "iteration 7400 / 14000: loss 1.215827\n",
      "iteration 7500 / 14000: loss 1.260969\n",
      "iteration 7600 / 14000: loss 1.068264\n",
      "iteration 7700 / 14000: loss 0.870166\n",
      "iteration 7800 / 14000: loss 1.170828\n",
      "epoch done... acc 0.5\n",
      "iteration 7900 / 14000: loss 1.132086\n",
      "iteration 8000 / 14000: loss 1.264463\n",
      "iteration 8100 / 14000: loss 1.118064\n",
      "iteration 8200 / 14000: loss 1.182218\n",
      "iteration 8300 / 14000: loss 1.124563\n",
      "epoch done... acc 0.504\n",
      "iteration 8400 / 14000: loss 1.037408\n",
      "iteration 8500 / 14000: loss 0.964500\n",
      "iteration 8600 / 14000: loss 0.969760\n",
      "iteration 8700 / 14000: loss 1.262965\n",
      "iteration 8800 / 14000: loss 1.092622\n",
      "epoch done... acc 0.518\n",
      "iteration 8900 / 14000: loss 1.175567\n",
      "iteration 9000 / 14000: loss 1.149021\n",
      "iteration 9100 / 14000: loss 1.116697\n",
      "iteration 9200 / 14000: loss 1.145721\n",
      "iteration 9300 / 14000: loss 1.326381\n",
      "epoch done... acc 0.513\n",
      "iteration 9400 / 14000: loss 0.927561\n",
      "iteration 9500 / 14000: loss 1.020556\n",
      "iteration 9600 / 14000: loss 1.082891\n",
      "iteration 9700 / 14000: loss 1.342320\n",
      "iteration 9800 / 14000: loss 1.205442\n",
      "epoch done... acc 0.51\n",
      "iteration 9900 / 14000: loss 1.077763\n",
      "iteration 10000 / 14000: loss 1.109796\n",
      "iteration 10100 / 14000: loss 1.120442\n",
      "iteration 10200 / 14000: loss 1.208115\n",
      "epoch done... acc 0.499\n",
      "iteration 10300 / 14000: loss 1.103692\n",
      "iteration 10400 / 14000: loss 1.123330\n",
      "iteration 10500 / 14000: loss 1.031324\n",
      "iteration 10600 / 14000: loss 1.168114\n",
      "iteration 10700 / 14000: loss 0.999684\n",
      "epoch done... acc 0.507\n",
      "iteration 10800 / 14000: loss 1.154217\n",
      "iteration 10900 / 14000: loss 1.150818\n",
      "iteration 11000 / 14000: loss 1.060665\n",
      "iteration 11100 / 14000: loss 1.017670\n",
      "iteration 11200 / 14000: loss 1.115604\n",
      "epoch done... acc 0.495\n",
      "iteration 11300 / 14000: loss 1.048374\n",
      "iteration 11400 / 14000: loss 1.110116\n",
      "iteration 11500 / 14000: loss 1.183967\n",
      "iteration 11600 / 14000: loss 1.058691\n",
      "iteration 11700 / 14000: loss 1.148433\n",
      "epoch done... acc 0.523\n",
      "iteration 11800 / 14000: loss 1.220086\n",
      "iteration 11900 / 14000: loss 1.133821\n",
      "iteration 12000 / 14000: loss 0.998270\n",
      "iteration 12100 / 14000: loss 0.968498\n",
      "iteration 12200 / 14000: loss 1.093638\n",
      "epoch done... acc 0.532\n",
      "iteration 12300 / 14000: loss 0.983252\n",
      "iteration 12400 / 14000: loss 1.092092\n",
      "iteration 12500 / 14000: loss 0.929164\n",
      "iteration 12600 / 14000: loss 1.058514\n",
      "iteration 12700 / 14000: loss 1.081014\n",
      "epoch done... acc 0.513\n",
      "iteration 12800 / 14000: loss 0.844441\n",
      "iteration 12900 / 14000: loss 1.126732\n",
      "iteration 13000 / 14000: loss 1.062048\n",
      "iteration 13100 / 14000: loss 1.070763\n",
      "iteration 13200 / 14000: loss 1.071061\n",
      "epoch done... acc 0.527\n",
      "iteration 13300 / 14000: loss 1.225914\n",
      "iteration 13400 / 14000: loss 1.051748\n",
      "iteration 13500 / 14000: loss 1.116649\n",
      "iteration 13600 / 14000: loss 0.784021\n",
      "iteration 13700 / 14000: loss 1.108821\n",
      "epoch done... acc 0.525\n",
      "iteration 13800 / 14000: loss 1.020965\n",
      "iteration 13900 / 14000: loss 1.057938\n",
      "Final training loss:  1.1701895382480187\n",
      "Final validation loss:  1.363662613544922\n",
      "Final validation accuracy:  0.525\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "26 2 0 385 14000 100 0.001 0.98 0.525\n",
      "iteration 0 / 15400: loss 2.302594\n",
      "epoch done... acc 0.121\n",
      "iteration 100 / 15400: loss 1.961890\n",
      "iteration 200 / 15400: loss 1.869507\n",
      "iteration 300 / 15400: loss 1.900890\n",
      "iteration 400 / 15400: loss 1.749504\n",
      "epoch done... acc 0.396\n",
      "iteration 500 / 15400: loss 1.877699\n",
      "iteration 600 / 15400: loss 1.738010\n",
      "iteration 700 / 15400: loss 1.692771\n",
      "iteration 800 / 15400: loss 1.739926\n",
      "iteration 900 / 15400: loss 1.647166\n",
      "epoch done... acc 0.429\n",
      "iteration 1000 / 15400: loss 1.486487\n",
      "iteration 1100 / 15400: loss 1.491205\n",
      "iteration 1200 / 15400: loss 1.528624\n",
      "iteration 1300 / 15400: loss 1.681707\n",
      "iteration 1400 / 15400: loss 1.682697\n",
      "epoch done... acc 0.453\n",
      "iteration 1500 / 15400: loss 1.406369\n",
      "iteration 1600 / 15400: loss 1.494201\n",
      "iteration 1700 / 15400: loss 1.556413\n",
      "iteration 1800 / 15400: loss 1.437426\n",
      "iteration 1900 / 15400: loss 1.454751\n",
      "epoch done... acc 0.445\n",
      "iteration 2000 / 15400: loss 1.525800\n",
      "iteration 2100 / 15400: loss 1.509128\n",
      "iteration 2200 / 15400: loss 1.430225\n",
      "iteration 2300 / 15400: loss 1.417358\n",
      "iteration 2400 / 15400: loss 1.434876\n",
      "epoch done... acc 0.465\n",
      "iteration 2500 / 15400: loss 1.364885\n",
      "iteration 2600 / 15400: loss 1.454774\n",
      "iteration 2700 / 15400: loss 1.324140\n",
      "iteration 2800 / 15400: loss 1.213542\n",
      "iteration 2900 / 15400: loss 1.477496\n",
      "epoch done... acc 0.494\n",
      "iteration 3000 / 15400: loss 1.378214\n",
      "iteration 3100 / 15400: loss 1.393730\n",
      "iteration 3200 / 15400: loss 1.194156\n",
      "iteration 3300 / 15400: loss 1.682776\n",
      "iteration 3400 / 15400: loss 1.535284\n",
      "epoch done... acc 0.482\n",
      "iteration 3500 / 15400: loss 1.386904\n",
      "iteration 3600 / 15400: loss 1.168516\n",
      "iteration 3700 / 15400: loss 1.268462\n",
      "iteration 3800 / 15400: loss 1.421574\n",
      "iteration 3900 / 15400: loss 1.369454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch done... acc 0.502\n",
      "iteration 4000 / 15400: loss 1.435129\n",
      "iteration 4100 / 15400: loss 1.446170\n",
      "iteration 4200 / 15400: loss 1.399272\n",
      "iteration 4300 / 15400: loss 1.469230\n",
      "iteration 4400 / 15400: loss 1.392419\n",
      "epoch done... acc 0.494\n",
      "iteration 4500 / 15400: loss 1.227902\n",
      "iteration 4600 / 15400: loss 1.544277\n",
      "iteration 4700 / 15400: loss 1.356547\n",
      "iteration 4800 / 15400: loss 1.159073\n",
      "iteration 4900 / 15400: loss 1.325291\n",
      "epoch done... acc 0.498\n",
      "iteration 5000 / 15400: loss 1.276925\n",
      "iteration 5100 / 15400: loss 1.374809\n",
      "iteration 5200 / 15400: loss 1.315689\n",
      "iteration 5300 / 15400: loss 1.345511\n",
      "epoch done... acc 0.504\n",
      "iteration 5400 / 15400: loss 1.333827\n",
      "iteration 5500 / 15400: loss 1.184223\n",
      "iteration 5600 / 15400: loss 1.150838\n",
      "iteration 5700 / 15400: loss 1.303537\n",
      "iteration 5800 / 15400: loss 1.180310\n",
      "epoch done... acc 0.509\n",
      "iteration 5900 / 15400: loss 1.179534\n",
      "iteration 6000 / 15400: loss 1.403638\n",
      "iteration 6100 / 15400: loss 1.190903\n",
      "iteration 6200 / 15400: loss 1.267858\n",
      "iteration 6300 / 15400: loss 1.347839\n",
      "epoch done... acc 0.499\n",
      "iteration 6400 / 15400: loss 1.260457\n",
      "iteration 6500 / 15400: loss 1.380549\n",
      "iteration 6600 / 15400: loss 1.287605\n",
      "iteration 6700 / 15400: loss 1.331290\n",
      "iteration 6800 / 15400: loss 1.184894\n",
      "epoch done... acc 0.531\n",
      "iteration 6900 / 15400: loss 1.173421\n",
      "iteration 7000 / 15400: loss 1.227199\n",
      "iteration 7100 / 15400: loss 1.301794\n",
      "iteration 7200 / 15400: loss 1.248330\n",
      "iteration 7300 / 15400: loss 1.235214\n",
      "epoch done... acc 0.524\n",
      "iteration 7400 / 15400: loss 1.307926\n",
      "iteration 7500 / 15400: loss 1.181488\n",
      "iteration 7600 / 15400: loss 1.188311\n",
      "iteration 7700 / 15400: loss 1.263216\n",
      "iteration 7800 / 15400: loss 1.175033\n",
      "epoch done... acc 0.519\n",
      "iteration 7900 / 15400: loss 1.152023\n",
      "iteration 8000 / 15400: loss 1.326020\n",
      "iteration 8100 / 15400: loss 1.151758\n",
      "iteration 8200 / 15400: loss 1.024422\n",
      "iteration 8300 / 15400: loss 1.368393\n",
      "epoch done... acc 0.517\n",
      "iteration 8400 / 15400: loss 1.230191\n",
      "iteration 8500 / 15400: loss 1.346382\n",
      "iteration 8600 / 15400: loss 1.187766\n",
      "iteration 8700 / 15400: loss 1.208419\n",
      "iteration 8800 / 15400: loss 1.036482\n",
      "epoch done... acc 0.515\n",
      "iteration 8900 / 15400: loss 1.275519\n",
      "iteration 9000 / 15400: loss 1.167720\n",
      "iteration 9100 / 15400: loss 1.108844\n",
      "iteration 9200 / 15400: loss 1.274686\n",
      "iteration 9300 / 15400: loss 1.139220\n",
      "epoch done... acc 0.522\n",
      "iteration 9400 / 15400: loss 1.211901\n",
      "iteration 9500 / 15400: loss 1.027950\n",
      "iteration 9600 / 15400: loss 1.171213\n",
      "iteration 9700 / 15400: loss 1.218346\n",
      "iteration 9800 / 15400: loss 1.376425\n",
      "epoch done... acc 0.534\n",
      "iteration 9900 / 15400: loss 1.150473\n",
      "iteration 10000 / 15400: loss 1.277972\n",
      "iteration 10100 / 15400: loss 0.996487\n",
      "iteration 10200 / 15400: loss 0.909821\n",
      "epoch done... acc 0.523\n",
      "iteration 10300 / 15400: loss 0.952438\n",
      "iteration 10400 / 15400: loss 1.270929\n",
      "iteration 10500 / 15400: loss 1.191610\n",
      "iteration 10600 / 15400: loss 1.193272\n",
      "iteration 10700 / 15400: loss 1.135956\n",
      "epoch done... acc 0.53\n",
      "iteration 10800 / 15400: loss 1.031204\n",
      "iteration 10900 / 15400: loss 1.090642\n",
      "iteration 11000 / 15400: loss 1.010544\n",
      "iteration 11100 / 15400: loss 1.095426\n",
      "iteration 11200 / 15400: loss 1.215983\n",
      "epoch done... acc 0.522\n",
      "iteration 11300 / 15400: loss 1.052571\n",
      "iteration 11400 / 15400: loss 0.980023\n",
      "iteration 11500 / 15400: loss 1.106286\n",
      "iteration 11600 / 15400: loss 0.961893\n",
      "iteration 11700 / 15400: loss 1.023182\n",
      "epoch done... acc 0.531\n",
      "iteration 11800 / 15400: loss 1.133252\n",
      "iteration 11900 / 15400: loss 1.045428\n",
      "iteration 12000 / 15400: loss 1.150322\n",
      "iteration 12100 / 15400: loss 1.145884\n",
      "iteration 12200 / 15400: loss 1.063758\n",
      "epoch done... acc 0.525\n",
      "iteration 12300 / 15400: loss 1.149918\n",
      "iteration 12400 / 15400: loss 0.938908\n",
      "iteration 12500 / 15400: loss 1.023435\n",
      "iteration 12600 / 15400: loss 1.136447\n",
      "iteration 12700 / 15400: loss 0.989392\n",
      "epoch done... acc 0.524\n",
      "iteration 12800 / 15400: loss 1.050516\n",
      "iteration 12900 / 15400: loss 1.164089\n",
      "iteration 13000 / 15400: loss 0.815534\n",
      "iteration 13100 / 15400: loss 1.003299\n",
      "iteration 13200 / 15400: loss 1.068607\n",
      "epoch done... acc 0.538\n",
      "iteration 13300 / 15400: loss 0.915596\n",
      "iteration 13400 / 15400: loss 1.002927\n",
      "iteration 13500 / 15400: loss 1.068597\n",
      "iteration 13600 / 15400: loss 1.015303\n",
      "iteration 13700 / 15400: loss 1.139544\n",
      "epoch done... acc 0.527\n",
      "iteration 13800 / 15400: loss 1.242648\n",
      "iteration 13900 / 15400: loss 0.981294\n",
      "iteration 14000 / 15400: loss 0.993522\n",
      "iteration 14100 / 15400: loss 0.870779\n",
      "iteration 14200 / 15400: loss 1.011441\n",
      "epoch done... acc 0.526\n",
      "iteration 14300 / 15400: loss 0.954024\n",
      "iteration 14400 / 15400: loss 0.719426\n",
      "iteration 14500 / 15400: loss 1.019221\n",
      "iteration 14600 / 15400: loss 0.897190\n",
      "iteration 14700 / 15400: loss 1.073396\n",
      "epoch done... acc 0.523\n",
      "iteration 14800 / 15400: loss 0.930014\n",
      "iteration 14900 / 15400: loss 1.014955\n",
      "iteration 15000 / 15400: loss 0.999457\n",
      "iteration 15100 / 15400: loss 0.945768\n",
      "epoch done... acc 0.523\n",
      "iteration 15200 / 15400: loss 0.924813\n",
      "iteration 15300 / 15400: loss 1.088188\n",
      "Final training loss:  0.9165973509213227\n",
      "Final validation loss:  1.35582195322211\n",
      "Final validation accuracy:  0.523\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "27 1 1 385 14000 100 0.001 0.98 0.523\n",
      "iteration 0 / 12600: loss 2.302578\n",
      "epoch done... acc 0.189\n",
      "iteration 100 / 12600: loss 2.011929\n",
      "iteration 200 / 12600: loss 2.077956\n",
      "iteration 300 / 12600: loss 1.727339\n",
      "iteration 400 / 12600: loss 1.639199\n",
      "epoch done... acc 0.379\n",
      "iteration 500 / 12600: loss 1.827024\n",
      "iteration 600 / 12600: loss 1.756458\n",
      "iteration 700 / 12600: loss 1.781184\n",
      "iteration 800 / 12600: loss 1.746513\n",
      "iteration 900 / 12600: loss 1.758198\n",
      "epoch done... acc 0.434\n",
      "iteration 1000 / 12600: loss 1.624453\n",
      "iteration 1100 / 12600: loss 1.643718\n",
      "iteration 1200 / 12600: loss 1.576676\n",
      "iteration 1300 / 12600: loss 1.608634\n",
      "iteration 1400 / 12600: loss 1.485545\n",
      "epoch done... acc 0.453\n",
      "iteration 1500 / 12600: loss 1.477960\n",
      "iteration 1600 / 12600: loss 1.484615\n",
      "iteration 1700 / 12600: loss 1.437461\n",
      "iteration 1800 / 12600: loss 1.582029\n",
      "iteration 1900 / 12600: loss 1.539197\n",
      "epoch done... acc 0.455\n",
      "iteration 2000 / 12600: loss 1.518352\n",
      "iteration 2100 / 12600: loss 1.481108\n",
      "iteration 2200 / 12600: loss 1.445027\n",
      "iteration 2300 / 12600: loss 1.599659\n",
      "iteration 2400 / 12600: loss 1.409560\n",
      "epoch done... acc 0.47\n",
      "iteration 2500 / 12600: loss 1.439844\n",
      "iteration 2600 / 12600: loss 1.378522\n",
      "iteration 2700 / 12600: loss 1.368305\n",
      "iteration 2800 / 12600: loss 1.569884\n",
      "iteration 2900 / 12600: loss 1.615549\n",
      "epoch done... acc 0.464\n",
      "iteration 3000 / 12600: loss 1.259879\n",
      "iteration 3100 / 12600: loss 1.329665\n",
      "iteration 3200 / 12600: loss 1.346087\n",
      "iteration 3300 / 12600: loss 1.279175\n",
      "iteration 3400 / 12600: loss 1.268557\n",
      "epoch done... acc 0.477\n",
      "iteration 3500 / 12600: loss 1.420671\n",
      "iteration 3600 / 12600: loss 1.559609\n",
      "iteration 3700 / 12600: loss 1.357718\n",
      "iteration 3800 / 12600: loss 1.233065\n",
      "iteration 3900 / 12600: loss 1.428607\n",
      "epoch done... acc 0.484\n",
      "iteration 4000 / 12600: loss 1.373278\n",
      "iteration 4100 / 12600: loss 1.499267\n",
      "iteration 4200 / 12600: loss 1.339930\n",
      "iteration 4300 / 12600: loss 1.340799\n",
      "iteration 4400 / 12600: loss 1.260083\n",
      "epoch done... acc 0.475\n",
      "iteration 4500 / 12600: loss 1.286050\n",
      "iteration 4600 / 12600: loss 1.379157\n",
      "iteration 4700 / 12600: loss 1.457481\n",
      "iteration 4800 / 12600: loss 1.338247\n",
      "iteration 4900 / 12600: loss 1.507232\n",
      "epoch done... acc 0.497\n",
      "iteration 5000 / 12600: loss 1.298839\n",
      "iteration 5100 / 12600: loss 1.245936\n",
      "iteration 5200 / 12600: loss 1.151447\n",
      "iteration 5300 / 12600: loss 1.465871\n",
      "epoch done... acc 0.499\n",
      "iteration 5400 / 12600: loss 1.254683\n",
      "iteration 5500 / 12600: loss 1.335651\n",
      "iteration 5600 / 12600: loss 1.483636\n",
      "iteration 5700 / 12600: loss 1.233939\n",
      "iteration 5800 / 12600: loss 1.318281\n",
      "epoch done... acc 0.517\n",
      "iteration 5900 / 12600: loss 1.156346\n",
      "iteration 6000 / 12600: loss 1.160738\n",
      "iteration 6100 / 12600: loss 1.189239\n",
      "iteration 6200 / 12600: loss 1.238158\n",
      "iteration 6300 / 12600: loss 1.320250\n",
      "epoch done... acc 0.506\n",
      "iteration 6400 / 12600: loss 1.133270\n",
      "iteration 6500 / 12600: loss 1.192413\n",
      "iteration 6600 / 12600: loss 1.406736\n",
      "iteration 6700 / 12600: loss 1.153128\n",
      "iteration 6800 / 12600: loss 1.012720\n",
      "epoch done... acc 0.51\n",
      "iteration 6900 / 12600: loss 1.331848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7000 / 12600: loss 1.265640\n",
      "iteration 7100 / 12600: loss 1.419862\n",
      "iteration 7200 / 12600: loss 1.299411\n",
      "iteration 7300 / 12600: loss 1.186290\n",
      "epoch done... acc 0.515\n",
      "iteration 7400 / 12600: loss 1.223510\n",
      "iteration 7500 / 12600: loss 1.275561\n",
      "iteration 7600 / 12600: loss 1.277006\n",
      "iteration 7700 / 12600: loss 1.142197\n",
      "iteration 7800 / 12600: loss 1.088436\n",
      "epoch done... acc 0.513\n",
      "iteration 7900 / 12600: loss 1.232037\n",
      "iteration 8000 / 12600: loss 1.204455\n",
      "iteration 8100 / 12600: loss 1.072771\n",
      "iteration 8200 / 12600: loss 1.094931\n",
      "iteration 8300 / 12600: loss 1.074667\n",
      "epoch done... acc 0.516\n",
      "iteration 8400 / 12600: loss 1.185009\n",
      "iteration 8500 / 12600: loss 1.196537\n",
      "iteration 8600 / 12600: loss 1.046452\n",
      "iteration 8700 / 12600: loss 1.359646\n",
      "iteration 8800 / 12600: loss 1.041178\n",
      "epoch done... acc 0.51\n",
      "iteration 8900 / 12600: loss 1.342359\n",
      "iteration 9000 / 12600: loss 1.115142\n",
      "iteration 9100 / 12600: loss 1.054834\n",
      "iteration 9200 / 12600: loss 1.041330\n",
      "iteration 9300 / 12600: loss 1.029350\n",
      "epoch done... acc 0.504\n",
      "iteration 9400 / 12600: loss 1.127902\n",
      "iteration 9500 / 12600: loss 1.136590\n",
      "iteration 9600 / 12600: loss 1.065412\n",
      "iteration 9700 / 12600: loss 1.232853\n",
      "iteration 9800 / 12600: loss 0.925927\n",
      "epoch done... acc 0.516\n",
      "iteration 9900 / 12600: loss 1.188781\n",
      "iteration 10000 / 12600: loss 1.165280\n",
      "iteration 10100 / 12600: loss 1.088206\n",
      "iteration 10200 / 12600: loss 1.157291\n",
      "epoch done... acc 0.528\n",
      "iteration 10300 / 12600: loss 1.006954\n",
      "iteration 10400 / 12600: loss 1.092230\n",
      "iteration 10500 / 12600: loss 1.007807\n",
      "iteration 10600 / 12600: loss 0.988485\n",
      "iteration 10700 / 12600: loss 1.071560\n",
      "epoch done... acc 0.524\n",
      "iteration 10800 / 12600: loss 1.054810\n",
      "iteration 10900 / 12600: loss 1.208804\n",
      "iteration 11000 / 12600: loss 1.184838\n",
      "iteration 11100 / 12600: loss 1.272081\n",
      "iteration 11200 / 12600: loss 1.040924\n",
      "epoch done... acc 0.527\n",
      "iteration 11300 / 12600: loss 1.093007\n",
      "iteration 11400 / 12600: loss 1.101436\n",
      "iteration 11500 / 12600: loss 0.916540\n",
      "iteration 11600 / 12600: loss 1.101064\n",
      "iteration 11700 / 12600: loss 1.031150\n",
      "epoch done... acc 0.532\n",
      "iteration 11800 / 12600: loss 0.964824\n",
      "iteration 11900 / 12600: loss 0.964058\n",
      "iteration 12000 / 12600: loss 0.885375\n",
      "iteration 12100 / 12600: loss 1.029341\n",
      "iteration 12200 / 12600: loss 0.935999\n",
      "epoch done... acc 0.527\n",
      "iteration 12300 / 12600: loss 0.977084\n",
      "iteration 12400 / 12600: loss 1.040938\n",
      "iteration 12500 / 12600: loss 1.150043\n",
      "Final training loss:  0.9158155040928218\n",
      "Final validation loss:  1.3933327630294263\n",
      "Final validation accuracy:  0.527\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "28 2 1 385 14000 100 0.001 0.98 0.527\n",
      "iteration 0 / 14000: loss 2.302606\n",
      "epoch done... acc 0.138\n",
      "iteration 100 / 14000: loss 2.044995\n",
      "iteration 200 / 14000: loss 1.997791\n",
      "iteration 300 / 14000: loss 1.915764\n",
      "iteration 400 / 14000: loss 1.726111\n",
      "epoch done... acc 0.387\n",
      "iteration 500 / 14000: loss 1.717821\n",
      "iteration 600 / 14000: loss 1.589744\n",
      "iteration 700 / 14000: loss 1.633450\n",
      "iteration 800 / 14000: loss 1.712101\n",
      "iteration 900 / 14000: loss 1.670025\n",
      "epoch done... acc 0.429\n",
      "iteration 1000 / 14000: loss 1.708350\n",
      "iteration 1100 / 14000: loss 1.466930\n",
      "iteration 1200 / 14000: loss 1.554641\n",
      "iteration 1300 / 14000: loss 1.436796\n",
      "iteration 1400 / 14000: loss 1.461552\n",
      "epoch done... acc 0.449\n",
      "iteration 1500 / 14000: loss 1.484750\n",
      "iteration 1600 / 14000: loss 1.778605\n",
      "iteration 1700 / 14000: loss 1.513040\n",
      "iteration 1800 / 14000: loss 1.506051\n",
      "iteration 1900 / 14000: loss 1.448202\n",
      "epoch done... acc 0.466\n",
      "iteration 2000 / 14000: loss 1.668000\n",
      "iteration 2100 / 14000: loss 1.643671\n",
      "iteration 2200 / 14000: loss 1.512533\n",
      "iteration 2300 / 14000: loss 1.459564\n",
      "iteration 2400 / 14000: loss 1.540379\n",
      "epoch done... acc 0.467\n",
      "iteration 2500 / 14000: loss 1.380971\n",
      "iteration 2600 / 14000: loss 1.516578\n",
      "iteration 2700 / 14000: loss 1.354445\n",
      "iteration 2800 / 14000: loss 1.773789\n",
      "iteration 2900 / 14000: loss 1.448869\n",
      "epoch done... acc 0.48\n",
      "iteration 3000 / 14000: loss 1.527170\n",
      "iteration 3100 / 14000: loss 1.487596\n",
      "iteration 3200 / 14000: loss 1.393748\n",
      "iteration 3300 / 14000: loss 1.397772\n",
      "iteration 3400 / 14000: loss 1.314913\n",
      "epoch done... acc 0.491\n",
      "iteration 3500 / 14000: loss 1.316288\n",
      "iteration 3600 / 14000: loss 1.302112\n",
      "iteration 3700 / 14000: loss 1.387535\n",
      "iteration 3800 / 14000: loss 1.412277\n",
      "iteration 3900 / 14000: loss 1.367682\n",
      "epoch done... acc 0.487\n",
      "iteration 4000 / 14000: loss 1.596439\n",
      "iteration 4100 / 14000: loss 1.338448\n",
      "iteration 4200 / 14000: loss 1.272533\n",
      "iteration 4300 / 14000: loss 1.397979\n",
      "iteration 4400 / 14000: loss 1.336742\n",
      "epoch done... acc 0.488\n",
      "iteration 4500 / 14000: loss 1.211544\n",
      "iteration 4600 / 14000: loss 1.336686\n",
      "iteration 4700 / 14000: loss 1.276928\n",
      "iteration 4800 / 14000: loss 1.309542\n",
      "iteration 4900 / 14000: loss 1.239538\n",
      "epoch done... acc 0.502\n",
      "iteration 5000 / 14000: loss 1.352728\n",
      "iteration 5100 / 14000: loss 1.309780\n",
      "iteration 5200 / 14000: loss 1.295966\n",
      "iteration 5300 / 14000: loss 1.224964\n",
      "epoch done... acc 0.504\n",
      "iteration 5400 / 14000: loss 1.279394\n",
      "iteration 5500 / 14000: loss 1.242528\n",
      "iteration 5600 / 14000: loss 1.396378\n",
      "iteration 5700 / 14000: loss 1.085866\n",
      "iteration 5800 / 14000: loss 1.263068\n",
      "epoch done... acc 0.504\n",
      "iteration 5900 / 14000: loss 1.321172\n",
      "iteration 6000 / 14000: loss 1.226829\n",
      "iteration 6100 / 14000: loss 1.366074\n",
      "iteration 6200 / 14000: loss 1.245586\n",
      "iteration 6300 / 14000: loss 1.426924\n",
      "epoch done... acc 0.507\n",
      "iteration 6400 / 14000: loss 1.340786\n",
      "iteration 6500 / 14000: loss 1.246811\n",
      "iteration 6600 / 14000: loss 1.375642\n",
      "iteration 6700 / 14000: loss 1.234741\n",
      "iteration 6800 / 14000: loss 1.162034\n",
      "epoch done... acc 0.503\n",
      "iteration 6900 / 14000: loss 1.349018\n",
      "iteration 7000 / 14000: loss 1.109247\n",
      "iteration 7100 / 14000: loss 1.110754\n",
      "iteration 7200 / 14000: loss 1.213490\n",
      "iteration 7300 / 14000: loss 1.381869\n",
      "epoch done... acc 0.511\n",
      "iteration 7400 / 14000: loss 1.209064\n",
      "iteration 7500 / 14000: loss 1.028429\n",
      "iteration 7600 / 14000: loss 1.219936\n",
      "iteration 7700 / 14000: loss 1.140856\n",
      "iteration 7800 / 14000: loss 1.219755\n",
      "epoch done... acc 0.503\n",
      "iteration 7900 / 14000: loss 1.138941\n",
      "iteration 8000 / 14000: loss 1.177494\n",
      "iteration 8100 / 14000: loss 1.180423\n",
      "iteration 8200 / 14000: loss 1.142340\n",
      "iteration 8300 / 14000: loss 1.160483\n",
      "epoch done... acc 0.519\n",
      "iteration 8400 / 14000: loss 1.094362\n",
      "iteration 8500 / 14000: loss 1.194743\n",
      "iteration 8600 / 14000: loss 1.164840\n",
      "iteration 8700 / 14000: loss 1.310269\n",
      "iteration 8800 / 14000: loss 1.025305\n",
      "epoch done... acc 0.508\n",
      "iteration 8900 / 14000: loss 1.313167\n",
      "iteration 9000 / 14000: loss 0.940039\n",
      "iteration 9100 / 14000: loss 1.127136\n",
      "iteration 9200 / 14000: loss 1.146735\n",
      "iteration 9300 / 14000: loss 0.985526\n",
      "epoch done... acc 0.507\n",
      "iteration 9400 / 14000: loss 1.162817\n",
      "iteration 9500 / 14000: loss 1.150723\n",
      "iteration 9600 / 14000: loss 1.120322\n",
      "iteration 9700 / 14000: loss 1.097869\n",
      "iteration 9800 / 14000: loss 1.099176\n",
      "epoch done... acc 0.527\n",
      "iteration 9900 / 14000: loss 1.250702\n",
      "iteration 10000 / 14000: loss 1.027799\n",
      "iteration 10100 / 14000: loss 1.110441\n",
      "iteration 10200 / 14000: loss 1.090103\n",
      "epoch done... acc 0.525\n",
      "iteration 10300 / 14000: loss 1.203379\n",
      "iteration 10400 / 14000: loss 1.164989\n",
      "iteration 10500 / 14000: loss 0.872353\n",
      "iteration 10600 / 14000: loss 1.266612\n",
      "iteration 10700 / 14000: loss 1.000326\n",
      "epoch done... acc 0.529\n",
      "iteration 10800 / 14000: loss 1.016720\n",
      "iteration 10900 / 14000: loss 0.974118\n",
      "iteration 11000 / 14000: loss 1.082480\n",
      "iteration 11100 / 14000: loss 1.059182\n",
      "iteration 11200 / 14000: loss 1.234948\n",
      "epoch done... acc 0.521\n",
      "iteration 11300 / 14000: loss 1.020944\n",
      "iteration 11400 / 14000: loss 1.135176\n",
      "iteration 11500 / 14000: loss 1.128243\n",
      "iteration 11600 / 14000: loss 1.068967\n",
      "iteration 11700 / 14000: loss 0.909537\n",
      "epoch done... acc 0.524\n",
      "iteration 11800 / 14000: loss 0.962105\n",
      "iteration 11900 / 14000: loss 1.315953\n",
      "iteration 12000 / 14000: loss 1.028132\n",
      "iteration 12100 / 14000: loss 1.078815\n",
      "iteration 12200 / 14000: loss 1.032040\n",
      "epoch done... acc 0.516\n",
      "iteration 12300 / 14000: loss 0.959999\n",
      "iteration 12400 / 14000: loss 0.998559\n",
      "iteration 12500 / 14000: loss 1.032208\n",
      "iteration 12600 / 14000: loss 1.068591\n",
      "iteration 12700 / 14000: loss 1.012215\n",
      "epoch done... acc 0.531\n",
      "iteration 12800 / 14000: loss 1.097971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 12900 / 14000: loss 1.007342\n",
      "iteration 13000 / 14000: loss 1.023125\n",
      "iteration 13100 / 14000: loss 0.998255\n",
      "iteration 13200 / 14000: loss 0.917130\n",
      "epoch done... acc 0.522\n",
      "iteration 13300 / 14000: loss 1.047590\n",
      "iteration 13400 / 14000: loss 1.102546\n",
      "iteration 13500 / 14000: loss 0.956646\n",
      "iteration 13600 / 14000: loss 0.924326\n",
      "iteration 13700 / 14000: loss 0.954448\n",
      "epoch done... acc 0.533\n",
      "iteration 13800 / 14000: loss 0.976279\n",
      "iteration 13900 / 14000: loss 1.021177\n",
      "Final training loss:  1.161042021678889\n",
      "Final validation loss:  1.3668726532749276\n",
      "Final validation accuracy:  0.533\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "29 1 0 385 14000 100 0.001 0.98 0.533\n",
      "iteration 0 / 14000: loss 2.302587\n",
      "epoch done... acc 0.134\n",
      "iteration 100 / 14000: loss 2.058984\n",
      "iteration 200 / 14000: loss 1.950534\n",
      "iteration 300 / 14000: loss 1.910988\n",
      "iteration 400 / 14000: loss 1.697482\n",
      "epoch done... acc 0.386\n",
      "iteration 500 / 14000: loss 1.733179\n",
      "iteration 600 / 14000: loss 1.691874\n",
      "iteration 700 / 14000: loss 1.566980\n",
      "iteration 800 / 14000: loss 1.674899\n",
      "iteration 900 / 14000: loss 1.537864\n",
      "epoch done... acc 0.431\n",
      "iteration 1000 / 14000: loss 1.731385\n",
      "iteration 1100 / 14000: loss 1.767318\n",
      "iteration 1200 / 14000: loss 1.665462\n",
      "iteration 1300 / 14000: loss 1.731983\n",
      "iteration 1400 / 14000: loss 1.436277\n",
      "epoch done... acc 0.446\n",
      "iteration 1500 / 14000: loss 1.589831\n",
      "iteration 1600 / 14000: loss 1.771571\n",
      "iteration 1700 / 14000: loss 1.572171\n",
      "iteration 1800 / 14000: loss 1.492796\n",
      "iteration 1900 / 14000: loss 1.492825\n",
      "epoch done... acc 0.462\n",
      "iteration 2000 / 14000: loss 1.598076\n",
      "iteration 2100 / 14000: loss 1.486730\n",
      "iteration 2200 / 14000: loss 1.432482\n",
      "iteration 2300 / 14000: loss 1.363577\n",
      "iteration 2400 / 14000: loss 1.437864\n",
      "epoch done... acc 0.469\n",
      "iteration 2500 / 14000: loss 1.372717\n",
      "iteration 2600 / 14000: loss 1.634748\n",
      "iteration 2700 / 14000: loss 1.435903\n",
      "iteration 2800 / 14000: loss 1.324374\n",
      "iteration 2900 / 14000: loss 1.429499\n",
      "epoch done... acc 0.484\n",
      "iteration 3000 / 14000: loss 1.425337\n",
      "iteration 3100 / 14000: loss 1.431172\n",
      "iteration 3200 / 14000: loss 1.482302\n",
      "iteration 3300 / 14000: loss 1.261899\n",
      "iteration 3400 / 14000: loss 1.316732\n",
      "epoch done... acc 0.478\n",
      "iteration 3500 / 14000: loss 1.310089\n",
      "iteration 3600 / 14000: loss 1.354640\n",
      "iteration 3700 / 14000: loss 1.198140\n",
      "iteration 3800 / 14000: loss 1.422331\n",
      "iteration 3900 / 14000: loss 1.431170\n",
      "epoch done... acc 0.496\n",
      "iteration 4000 / 14000: loss 1.437177\n",
      "iteration 4100 / 14000: loss 1.369798\n",
      "iteration 4200 / 14000: loss 1.314052\n",
      "iteration 4300 / 14000: loss 1.428233\n",
      "iteration 4400 / 14000: loss 1.381664\n",
      "epoch done... acc 0.493\n",
      "iteration 4500 / 14000: loss 1.138492\n",
      "iteration 4600 / 14000: loss 1.412051\n",
      "iteration 4700 / 14000: loss 1.364667\n",
      "iteration 4800 / 14000: loss 1.356995\n",
      "iteration 4900 / 14000: loss 1.125935\n",
      "epoch done... acc 0.512\n",
      "iteration 5000 / 14000: loss 1.104300\n",
      "iteration 5100 / 14000: loss 1.389191\n",
      "iteration 5200 / 14000: loss 1.378490\n",
      "iteration 5300 / 14000: loss 1.363064\n",
      "epoch done... acc 0.503\n",
      "iteration 5400 / 14000: loss 1.202485\n",
      "iteration 5500 / 14000: loss 1.198303\n",
      "iteration 5600 / 14000: loss 1.295692\n",
      "iteration 5700 / 14000: loss 1.331305\n",
      "iteration 5800 / 14000: loss 1.188995\n",
      "epoch done... acc 0.504\n",
      "iteration 5900 / 14000: loss 1.227490\n",
      "iteration 6000 / 14000: loss 1.274780\n",
      "iteration 6100 / 14000: loss 1.284739\n",
      "iteration 6200 / 14000: loss 1.193251\n",
      "iteration 6300 / 14000: loss 1.227096\n",
      "epoch done... acc 0.528\n",
      "iteration 6400 / 14000: loss 1.442384\n",
      "iteration 6500 / 14000: loss 1.128595\n",
      "iteration 6600 / 14000: loss 1.269470\n",
      "iteration 6700 / 14000: loss 1.124737\n",
      "iteration 6800 / 14000: loss 1.184261\n",
      "epoch done... acc 0.504\n",
      "iteration 6900 / 14000: loss 1.276731\n",
      "iteration 7000 / 14000: loss 1.098852\n",
      "iteration 7100 / 14000: loss 1.104322\n",
      "iteration 7200 / 14000: loss 1.233079\n",
      "iteration 7300 / 14000: loss 1.206467\n",
      "epoch done... acc 0.514\n",
      "iteration 7400 / 14000: loss 1.400645\n",
      "iteration 7500 / 14000: loss 1.248185\n",
      "iteration 7600 / 14000: loss 1.165910\n",
      "iteration 7700 / 14000: loss 1.012444\n",
      "iteration 7800 / 14000: loss 1.139871\n",
      "epoch done... acc 0.514\n",
      "iteration 7900 / 14000: loss 1.274371\n",
      "iteration 8000 / 14000: loss 0.964000\n",
      "iteration 8100 / 14000: loss 1.180759\n",
      "iteration 8200 / 14000: loss 1.032421\n",
      "iteration 8300 / 14000: loss 1.256430\n",
      "epoch done... acc 0.523\n",
      "iteration 8400 / 14000: loss 1.127108\n",
      "iteration 8500 / 14000: loss 1.232959\n",
      "iteration 8600 / 14000: loss 1.282440\n",
      "iteration 8700 / 14000: loss 1.277678\n",
      "iteration 8800 / 14000: loss 1.147639\n",
      "epoch done... acc 0.54\n",
      "iteration 8900 / 14000: loss 1.229164\n",
      "iteration 9000 / 14000: loss 1.254860\n",
      "iteration 9100 / 14000: loss 1.314717\n",
      "iteration 9200 / 14000: loss 1.103068\n",
      "iteration 9300 / 14000: loss 1.107534\n",
      "epoch done... acc 0.529\n",
      "iteration 9400 / 14000: loss 1.248757\n",
      "iteration 9500 / 14000: loss 1.005307\n",
      "iteration 9600 / 14000: loss 1.135307\n",
      "iteration 9700 / 14000: loss 1.120165\n",
      "iteration 9800 / 14000: loss 1.116429\n",
      "epoch done... acc 0.527\n",
      "iteration 9900 / 14000: loss 1.202139\n",
      "iteration 10000 / 14000: loss 1.383156\n",
      "iteration 10100 / 14000: loss 1.152082\n",
      "iteration 10200 / 14000: loss 1.030800\n",
      "epoch done... acc 0.521\n",
      "iteration 10300 / 14000: loss 1.108592\n",
      "iteration 10400 / 14000: loss 1.084963\n",
      "iteration 10500 / 14000: loss 1.114452\n",
      "iteration 10600 / 14000: loss 1.040268\n",
      "iteration 10700 / 14000: loss 1.080726\n",
      "epoch done... acc 0.537\n",
      "iteration 10800 / 14000: loss 1.098433\n",
      "iteration 10900 / 14000: loss 1.027221\n",
      "iteration 11000 / 14000: loss 1.098489\n",
      "iteration 11100 / 14000: loss 1.044517\n",
      "iteration 11200 / 14000: loss 1.157581\n",
      "epoch done... acc 0.516\n",
      "iteration 11300 / 14000: loss 1.025042\n",
      "iteration 11400 / 14000: loss 1.147498\n",
      "iteration 11500 / 14000: loss 1.182802\n",
      "iteration 11600 / 14000: loss 1.009197\n",
      "iteration 11700 / 14000: loss 1.058256\n",
      "epoch done... acc 0.535\n",
      "iteration 11800 / 14000: loss 1.143649\n",
      "iteration 11900 / 14000: loss 1.012761\n",
      "iteration 12000 / 14000: loss 1.076289\n",
      "iteration 12100 / 14000: loss 1.072135\n",
      "iteration 12200 / 14000: loss 1.116732\n",
      "epoch done... acc 0.526\n",
      "iteration 12300 / 14000: loss 1.010869\n",
      "iteration 12400 / 14000: loss 1.044189\n",
      "iteration 12500 / 14000: loss 1.159587\n",
      "iteration 12600 / 14000: loss 1.000338\n",
      "iteration 12700 / 14000: loss 1.039864\n",
      "epoch done... acc 0.535\n",
      "iteration 12800 / 14000: loss 1.082798\n",
      "iteration 12900 / 14000: loss 1.097798\n",
      "iteration 13000 / 14000: loss 1.087304\n",
      "iteration 13100 / 14000: loss 1.118366\n",
      "iteration 13200 / 14000: loss 1.211633\n",
      "epoch done... acc 0.519\n",
      "iteration 13300 / 14000: loss 1.204355\n",
      "iteration 13400 / 14000: loss 0.996459\n",
      "iteration 13500 / 14000: loss 1.205561\n",
      "iteration 13600 / 14000: loss 1.019631\n",
      "iteration 13700 / 14000: loss 1.184989\n",
      "epoch done... acc 0.522\n",
      "iteration 13800 / 14000: loss 0.886371\n",
      "iteration 13900 / 14000: loss 0.951775\n",
      "Final training loss:  1.0665353864639915\n",
      "Final validation loss:  1.372987530363149\n",
      "Final validation accuracy:  0.522\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "30 2 0 385 14000 100 0.001 0.98 0.522\n",
      "iteration 0 / 15400: loss 2.302573\n",
      "epoch done... acc 0.12\n",
      "iteration 100 / 15400: loss 2.015445\n",
      "iteration 200 / 15400: loss 1.820166\n",
      "iteration 300 / 15400: loss 1.923635\n",
      "iteration 400 / 15400: loss 1.753360\n",
      "epoch done... acc 0.383\n",
      "iteration 500 / 15400: loss 1.776016\n",
      "iteration 600 / 15400: loss 1.648202\n",
      "iteration 700 / 15400: loss 1.682641\n",
      "iteration 800 / 15400: loss 1.665557\n",
      "iteration 900 / 15400: loss 1.554667\n",
      "epoch done... acc 0.428\n",
      "iteration 1000 / 15400: loss 1.576792\n",
      "iteration 1100 / 15400: loss 1.473800\n",
      "iteration 1200 / 15400: loss 1.392485\n",
      "iteration 1300 / 15400: loss 1.581068\n",
      "iteration 1400 / 15400: loss 1.558130\n",
      "epoch done... acc 0.445\n",
      "iteration 1500 / 15400: loss 1.682059\n",
      "iteration 1600 / 15400: loss 1.575412\n",
      "iteration 1700 / 15400: loss 1.626262\n",
      "iteration 1800 / 15400: loss 1.412363\n",
      "iteration 1900 / 15400: loss 1.522290\n",
      "epoch done... acc 0.455\n",
      "iteration 2000 / 15400: loss 1.485583\n",
      "iteration 2100 / 15400: loss 1.461200\n",
      "iteration 2200 / 15400: loss 1.598591\n",
      "iteration 2300 / 15400: loss 1.525349\n",
      "iteration 2400 / 15400: loss 1.382549\n",
      "epoch done... acc 0.467\n",
      "iteration 2500 / 15400: loss 1.496818\n",
      "iteration 2600 / 15400: loss 1.569128\n",
      "iteration 2700 / 15400: loss 1.478670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2800 / 15400: loss 1.552096\n",
      "iteration 2900 / 15400: loss 1.355819\n",
      "epoch done... acc 0.471\n",
      "iteration 3000 / 15400: loss 1.457716\n",
      "iteration 3100 / 15400: loss 1.415866\n",
      "iteration 3200 / 15400: loss 1.322014\n",
      "iteration 3300 / 15400: loss 1.419573\n",
      "iteration 3400 / 15400: loss 1.511188\n",
      "epoch done... acc 0.475\n",
      "iteration 3500 / 15400: loss 1.317804\n",
      "iteration 3600 / 15400: loss 1.355428\n",
      "iteration 3700 / 15400: loss 1.298082\n",
      "iteration 3800 / 15400: loss 1.302592\n",
      "iteration 3900 / 15400: loss 1.418805\n",
      "epoch done... acc 0.478\n",
      "iteration 4000 / 15400: loss 1.330348\n",
      "iteration 4100 / 15400: loss 1.335545\n",
      "iteration 4200 / 15400: loss 1.410580\n",
      "iteration 4300 / 15400: loss 1.223182\n",
      "iteration 4400 / 15400: loss 1.444762\n",
      "epoch done... acc 0.497\n",
      "iteration 4500 / 15400: loss 1.372136\n",
      "iteration 4600 / 15400: loss 1.396303\n",
      "iteration 4700 / 15400: loss 1.285510\n",
      "iteration 4800 / 15400: loss 1.213973\n",
      "iteration 4900 / 15400: loss 1.395784\n",
      "epoch done... acc 0.488\n",
      "iteration 5000 / 15400: loss 1.338829\n",
      "iteration 5100 / 15400: loss 1.380472\n",
      "iteration 5200 / 15400: loss 1.058883\n",
      "iteration 5300 / 15400: loss 1.277225\n",
      "epoch done... acc 0.497\n",
      "iteration 5400 / 15400: loss 1.322900\n",
      "iteration 5500 / 15400: loss 1.317027\n",
      "iteration 5600 / 15400: loss 1.356759\n",
      "iteration 5700 / 15400: loss 1.508543\n",
      "iteration 5800 / 15400: loss 1.325914\n",
      "epoch done... acc 0.493\n",
      "iteration 5900 / 15400: loss 1.354316\n",
      "iteration 6000 / 15400: loss 1.156415\n",
      "iteration 6100 / 15400: loss 1.237583\n",
      "iteration 6200 / 15400: loss 1.389301\n",
      "iteration 6300 / 15400: loss 1.311306\n",
      "epoch done... acc 0.518\n",
      "iteration 6400 / 15400: loss 1.356968\n",
      "iteration 6500 / 15400: loss 1.485880\n",
      "iteration 6600 / 15400: loss 1.239964\n",
      "iteration 6700 / 15400: loss 1.181547\n",
      "iteration 6800 / 15400: loss 1.336312\n",
      "epoch done... acc 0.516\n",
      "iteration 6900 / 15400: loss 1.080721\n",
      "iteration 7000 / 15400: loss 1.323169\n",
      "iteration 7100 / 15400: loss 1.375255\n",
      "iteration 7200 / 15400: loss 1.078482\n",
      "iteration 7300 / 15400: loss 1.244560\n",
      "epoch done... acc 0.508\n",
      "iteration 7400 / 15400: loss 1.257441\n",
      "iteration 7500 / 15400: loss 1.223658\n",
      "iteration 7600 / 15400: loss 1.175590\n",
      "iteration 7700 / 15400: loss 1.178755\n",
      "iteration 7800 / 15400: loss 1.066429\n",
      "epoch done... acc 0.496\n",
      "iteration 7900 / 15400: loss 0.994351\n",
      "iteration 8000 / 15400: loss 1.264200\n",
      "iteration 8100 / 15400: loss 1.136098\n",
      "iteration 8200 / 15400: loss 1.212961\n",
      "iteration 8300 / 15400: loss 1.150657\n",
      "epoch done... acc 0.512\n",
      "iteration 8400 / 15400: loss 1.069914\n",
      "iteration 8500 / 15400: loss 1.203900\n",
      "iteration 8600 / 15400: loss 1.196868\n",
      "iteration 8700 / 15400: loss 1.008749\n",
      "iteration 8800 / 15400: loss 0.956638\n",
      "epoch done... acc 0.52\n",
      "iteration 8900 / 15400: loss 1.067436\n",
      "iteration 9000 / 15400: loss 1.118262\n",
      "iteration 9100 / 15400: loss 1.175553\n",
      "iteration 9200 / 15400: loss 1.235982\n",
      "iteration 9300 / 15400: loss 1.164762\n",
      "epoch done... acc 0.523\n",
      "iteration 9400 / 15400: loss 1.178856\n",
      "iteration 9500 / 15400: loss 1.195430\n",
      "iteration 9600 / 15400: loss 1.174107\n",
      "iteration 9700 / 15400: loss 1.267626\n",
      "iteration 9800 / 15400: loss 1.340488\n",
      "epoch done... acc 0.516\n",
      "iteration 9900 / 15400: loss 1.109266\n",
      "iteration 10000 / 15400: loss 1.072823\n",
      "iteration 10100 / 15400: loss 1.196568\n",
      "iteration 10200 / 15400: loss 1.201573\n",
      "epoch done... acc 0.53\n",
      "iteration 10300 / 15400: loss 1.186019\n",
      "iteration 10400 / 15400: loss 1.200382\n",
      "iteration 10500 / 15400: loss 1.059829\n",
      "iteration 10600 / 15400: loss 1.221822\n",
      "iteration 10700 / 15400: loss 1.128344\n",
      "epoch done... acc 0.516\n",
      "iteration 10800 / 15400: loss 1.173660\n",
      "iteration 10900 / 15400: loss 1.129552\n",
      "iteration 11000 / 15400: loss 1.126328\n",
      "iteration 11100 / 15400: loss 1.060769\n",
      "iteration 11200 / 15400: loss 1.108713\n",
      "epoch done... acc 0.524\n",
      "iteration 11300 / 15400: loss 0.917342\n",
      "iteration 11400 / 15400: loss 1.108792\n",
      "iteration 11500 / 15400: loss 1.213295\n",
      "iteration 11600 / 15400: loss 1.045487\n",
      "iteration 11700 / 15400: loss 0.968276\n",
      "epoch done... acc 0.528\n",
      "iteration 11800 / 15400: loss 1.132463\n",
      "iteration 11900 / 15400: loss 1.127958\n",
      "iteration 12000 / 15400: loss 1.116207\n",
      "iteration 12100 / 15400: loss 0.958955\n",
      "iteration 12200 / 15400: loss 0.954918\n",
      "epoch done... acc 0.527\n",
      "iteration 12300 / 15400: loss 1.165375\n",
      "iteration 12400 / 15400: loss 1.022974\n",
      "iteration 12500 / 15400: loss 1.099816\n",
      "iteration 12600 / 15400: loss 0.957872\n",
      "iteration 12700 / 15400: loss 1.083223\n",
      "epoch done... acc 0.513\n",
      "iteration 12800 / 15400: loss 1.007300\n",
      "iteration 12900 / 15400: loss 1.252852\n",
      "iteration 13000 / 15400: loss 1.026240\n",
      "iteration 13100 / 15400: loss 0.997453\n",
      "iteration 13200 / 15400: loss 0.933778\n",
      "epoch done... acc 0.53\n",
      "iteration 13300 / 15400: loss 1.123313\n",
      "iteration 13400 / 15400: loss 1.067297\n",
      "iteration 13500 / 15400: loss 1.063505\n",
      "iteration 13600 / 15400: loss 0.953741\n",
      "iteration 13700 / 15400: loss 0.912206\n",
      "epoch done... acc 0.531\n",
      "iteration 13800 / 15400: loss 0.863536\n",
      "iteration 13900 / 15400: loss 0.936017\n",
      "iteration 14000 / 15400: loss 1.027386\n",
      "iteration 14100 / 15400: loss 1.131958\n",
      "iteration 14200 / 15400: loss 1.140427\n",
      "epoch done... acc 0.522\n",
      "iteration 14300 / 15400: loss 1.217180\n",
      "iteration 14400 / 15400: loss 1.024407\n",
      "iteration 14500 / 15400: loss 0.981168\n",
      "iteration 14600 / 15400: loss 0.914110\n",
      "iteration 14700 / 15400: loss 1.150780\n",
      "epoch done... acc 0.534\n",
      "iteration 14800 / 15400: loss 0.955270\n",
      "iteration 14900 / 15400: loss 1.062178\n",
      "iteration 15000 / 15400: loss 1.005749\n",
      "iteration 15100 / 15400: loss 0.976068\n",
      "epoch done... acc 0.52\n",
      "iteration 15200 / 15400: loss 0.940523\n",
      "iteration 15300 / 15400: loss 0.929926\n",
      "Final training loss:  0.9707937395970105\n",
      "Final validation loss:  1.3799049413955977\n",
      "Final validation accuracy:  0.52\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "31 1 1 385 14000 100 0.001 0.98 0.52\n",
      "iteration 0 / 12600: loss 2.302612\n",
      "epoch done... acc 0.136\n",
      "iteration 100 / 12600: loss 2.059237\n",
      "iteration 200 / 12600: loss 1.929875\n",
      "iteration 300 / 12600: loss 1.922259\n",
      "iteration 400 / 12600: loss 1.724784\n",
      "epoch done... acc 0.389\n",
      "iteration 500 / 12600: loss 1.932019\n",
      "iteration 600 / 12600: loss 1.741406\n",
      "iteration 700 / 12600: loss 1.794371\n",
      "iteration 800 / 12600: loss 1.621351\n",
      "iteration 900 / 12600: loss 1.659680\n",
      "epoch done... acc 0.422\n",
      "iteration 1000 / 12600: loss 1.700506\n",
      "iteration 1100 / 12600: loss 1.457086\n",
      "iteration 1200 / 12600: loss 1.610638\n",
      "iteration 1300 / 12600: loss 1.500476\n",
      "iteration 1400 / 12600: loss 1.566975\n",
      "epoch done... acc 0.453\n",
      "iteration 1500 / 12600: loss 1.434220\n",
      "iteration 1600 / 12600: loss 1.646677\n",
      "iteration 1700 / 12600: loss 1.464416\n",
      "iteration 1800 / 12600: loss 1.725997\n",
      "iteration 1900 / 12600: loss 1.654507\n",
      "epoch done... acc 0.454\n",
      "iteration 2000 / 12600: loss 1.492434\n",
      "iteration 2100 / 12600: loss 1.502880\n",
      "iteration 2200 / 12600: loss 1.531336\n",
      "iteration 2300 / 12600: loss 1.330748\n",
      "iteration 2400 / 12600: loss 1.363201\n",
      "epoch done... acc 0.456\n",
      "iteration 2500 / 12600: loss 1.494220\n",
      "iteration 2600 / 12600: loss 1.368090\n",
      "iteration 2700 / 12600: loss 1.363623\n",
      "iteration 2800 / 12600: loss 1.412772\n",
      "iteration 2900 / 12600: loss 1.343846\n",
      "epoch done... acc 0.481\n",
      "iteration 3000 / 12600: loss 1.437286\n",
      "iteration 3100 / 12600: loss 1.404878\n",
      "iteration 3200 / 12600: loss 1.396842\n",
      "iteration 3300 / 12600: loss 1.291615\n",
      "iteration 3400 / 12600: loss 1.460120\n",
      "epoch done... acc 0.489\n",
      "iteration 3500 / 12600: loss 1.387919\n",
      "iteration 3600 / 12600: loss 1.465219\n",
      "iteration 3700 / 12600: loss 1.384882\n",
      "iteration 3800 / 12600: loss 1.140987\n",
      "iteration 3900 / 12600: loss 1.218652\n",
      "epoch done... acc 0.48\n",
      "iteration 4000 / 12600: loss 1.175768\n",
      "iteration 4100 / 12600: loss 1.338104\n",
      "iteration 4200 / 12600: loss 1.490700\n",
      "iteration 4300 / 12600: loss 1.208125\n",
      "iteration 4400 / 12600: loss 1.259865\n",
      "epoch done... acc 0.493\n",
      "iteration 4500 / 12600: loss 1.345466\n",
      "iteration 4600 / 12600: loss 1.291554\n",
      "iteration 4700 / 12600: loss 1.374509\n",
      "iteration 4800 / 12600: loss 1.129717\n",
      "iteration 4900 / 12600: loss 1.285546\n",
      "epoch done... acc 0.501\n",
      "iteration 5000 / 12600: loss 1.322285\n",
      "iteration 5100 / 12600: loss 1.256177\n",
      "iteration 5200 / 12600: loss 1.198797\n",
      "iteration 5300 / 12600: loss 1.514032\n",
      "epoch done... acc 0.503\n",
      "iteration 5400 / 12600: loss 1.301759\n",
      "iteration 5500 / 12600: loss 1.106889\n",
      "iteration 5600 / 12600: loss 1.356500\n",
      "iteration 5700 / 12600: loss 1.190734\n",
      "iteration 5800 / 12600: loss 1.238375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch done... acc 0.507\n",
      "iteration 5900 / 12600: loss 1.355943\n",
      "iteration 6000 / 12600: loss 1.253168\n",
      "iteration 6100 / 12600: loss 1.135151\n",
      "iteration 6200 / 12600: loss 1.293508\n",
      "iteration 6300 / 12600: loss 1.297616\n",
      "epoch done... acc 0.517\n",
      "iteration 6400 / 12600: loss 1.320618\n",
      "iteration 6500 / 12600: loss 1.257609\n",
      "iteration 6600 / 12600: loss 1.192068\n",
      "iteration 6700 / 12600: loss 1.158898\n",
      "iteration 6800 / 12600: loss 1.421828\n",
      "epoch done... acc 0.505\n",
      "iteration 6900 / 12600: loss 1.002317\n",
      "iteration 7000 / 12600: loss 1.316721\n",
      "iteration 7100 / 12600: loss 1.081240\n",
      "iteration 7200 / 12600: loss 1.182188\n",
      "iteration 7300 / 12600: loss 1.184864\n",
      "epoch done... acc 0.516\n",
      "iteration 7400 / 12600: loss 1.313822\n",
      "iteration 7500 / 12600: loss 1.206000\n",
      "iteration 7600 / 12600: loss 1.331092\n",
      "iteration 7700 / 12600: loss 1.079602\n",
      "iteration 7800 / 12600: loss 1.171310\n",
      "epoch done... acc 0.526\n",
      "iteration 7900 / 12600: loss 1.154512\n",
      "iteration 8000 / 12600: loss 1.228960\n",
      "iteration 8100 / 12600: loss 1.240980\n",
      "iteration 8200 / 12600: loss 1.144798\n",
      "iteration 8300 / 12600: loss 1.247600\n",
      "epoch done... acc 0.511\n",
      "iteration 8400 / 12600: loss 1.124295\n",
      "iteration 8500 / 12600: loss 1.002907\n",
      "iteration 8600 / 12600: loss 1.200095\n",
      "iteration 8700 / 12600: loss 1.107059\n",
      "iteration 8800 / 12600: loss 1.055675\n",
      "epoch done... acc 0.526\n",
      "iteration 8900 / 12600: loss 1.085692\n",
      "iteration 9000 / 12600: loss 1.258500\n",
      "iteration 9100 / 12600: loss 1.232372\n",
      "iteration 9200 / 12600: loss 1.106906\n",
      "iteration 9300 / 12600: loss 1.020787\n",
      "epoch done... acc 0.531\n",
      "iteration 9400 / 12600: loss 1.222195\n",
      "iteration 9500 / 12600: loss 1.073827\n",
      "iteration 9600 / 12600: loss 1.060253\n",
      "iteration 9700 / 12600: loss 1.200940\n",
      "iteration 9800 / 12600: loss 1.169940\n",
      "epoch done... acc 0.533\n",
      "iteration 9900 / 12600: loss 1.098033\n",
      "iteration 10000 / 12600: loss 1.061555\n",
      "iteration 10100 / 12600: loss 0.980166\n",
      "iteration 10200 / 12600: loss 1.009519\n",
      "epoch done... acc 0.547\n",
      "iteration 10300 / 12600: loss 1.153196\n",
      "iteration 10400 / 12600: loss 1.012107\n",
      "iteration 10500 / 12600: loss 1.077009\n",
      "iteration 10600 / 12600: loss 1.220195\n",
      "iteration 10700 / 12600: loss 1.013892\n",
      "epoch done... acc 0.535\n",
      "iteration 10800 / 12600: loss 1.043427\n",
      "iteration 10900 / 12600: loss 1.139531\n",
      "iteration 11000 / 12600: loss 1.242776\n",
      "iteration 11100 / 12600: loss 0.962573\n",
      "iteration 11200 / 12600: loss 1.094632\n",
      "epoch done... acc 0.53\n",
      "iteration 11300 / 12600: loss 0.911411\n",
      "iteration 11400 / 12600: loss 0.996704\n",
      "iteration 11500 / 12600: loss 1.095416\n",
      "iteration 11600 / 12600: loss 1.047862\n",
      "iteration 11700 / 12600: loss 1.194309\n",
      "epoch done... acc 0.534\n",
      "iteration 11800 / 12600: loss 1.194026\n",
      "iteration 11900 / 12600: loss 1.228891\n",
      "iteration 12000 / 12600: loss 1.022355\n",
      "iteration 12100 / 12600: loss 0.967251\n",
      "iteration 12200 / 12600: loss 1.050284\n",
      "epoch done... acc 0.547\n",
      "iteration 12300 / 12600: loss 1.030147\n",
      "iteration 12400 / 12600: loss 1.047079\n",
      "iteration 12500 / 12600: loss 1.207555\n",
      "Final training loss:  1.0238604097902795\n",
      "Final validation loss:  1.3548950649947569\n",
      "Final validation accuracy:  0.547\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "32 1 1 385 12600 100 0.001 0.98 0.547\n",
      "iteration 0 / 11340: loss 2.302582\n",
      "epoch done... acc 0.178\n",
      "iteration 100 / 11340: loss 2.023895\n",
      "iteration 200 / 11340: loss 2.034757\n",
      "iteration 300 / 11340: loss 1.804438\n",
      "iteration 400 / 11340: loss 1.924161\n",
      "epoch done... acc 0.379\n",
      "iteration 500 / 11340: loss 1.795253\n",
      "iteration 600 / 11340: loss 1.763970\n",
      "iteration 700 / 11340: loss 1.739744\n",
      "iteration 800 / 11340: loss 1.613821\n",
      "iteration 900 / 11340: loss 1.701348\n",
      "epoch done... acc 0.414\n",
      "iteration 1000 / 11340: loss 1.722514\n",
      "iteration 1100 / 11340: loss 1.517867\n",
      "iteration 1200 / 11340: loss 1.508615\n",
      "iteration 1300 / 11340: loss 1.488134\n",
      "iteration 1400 / 11340: loss 1.504957\n",
      "epoch done... acc 0.432\n",
      "iteration 1500 / 11340: loss 1.536252\n",
      "iteration 1600 / 11340: loss 1.505804\n",
      "iteration 1700 / 11340: loss 1.572485\n",
      "iteration 1800 / 11340: loss 1.502597\n",
      "iteration 1900 / 11340: loss 1.493176\n",
      "epoch done... acc 0.451\n",
      "iteration 2000 / 11340: loss 1.559163\n",
      "iteration 2100 / 11340: loss 1.421179\n",
      "iteration 2200 / 11340: loss 1.527624\n",
      "iteration 2300 / 11340: loss 1.316116\n",
      "iteration 2400 / 11340: loss 1.382567\n",
      "epoch done... acc 0.471\n",
      "iteration 2500 / 11340: loss 1.601649\n",
      "iteration 2600 / 11340: loss 1.409876\n",
      "iteration 2700 / 11340: loss 1.545254\n",
      "iteration 2800 / 11340: loss 1.491160\n",
      "iteration 2900 / 11340: loss 1.485681\n",
      "epoch done... acc 0.465\n",
      "iteration 3000 / 11340: loss 1.310444\n",
      "iteration 3100 / 11340: loss 1.458689\n",
      "iteration 3200 / 11340: loss 1.417492\n",
      "iteration 3300 / 11340: loss 1.384458\n",
      "iteration 3400 / 11340: loss 1.149402\n",
      "epoch done... acc 0.48\n",
      "iteration 3500 / 11340: loss 1.394634\n",
      "iteration 3600 / 11340: loss 1.400400\n",
      "iteration 3700 / 11340: loss 1.245706\n",
      "iteration 3800 / 11340: loss 1.460660\n",
      "iteration 3900 / 11340: loss 1.379985\n",
      "epoch done... acc 0.477\n",
      "iteration 4000 / 11340: loss 1.312649\n",
      "iteration 4100 / 11340: loss 1.165852\n",
      "iteration 4200 / 11340: loss 1.329780\n",
      "iteration 4300 / 11340: loss 1.335739\n",
      "iteration 4400 / 11340: loss 1.200285\n",
      "epoch done... acc 0.473\n",
      "iteration 4500 / 11340: loss 1.469867\n",
      "iteration 4600 / 11340: loss 1.176903\n",
      "iteration 4700 / 11340: loss 1.229182\n",
      "iteration 4800 / 11340: loss 1.360222\n",
      "iteration 4900 / 11340: loss 1.329823\n",
      "epoch done... acc 0.485\n",
      "iteration 5000 / 11340: loss 1.224476\n",
      "iteration 5100 / 11340: loss 1.127388\n",
      "iteration 5200 / 11340: loss 1.332767\n",
      "iteration 5300 / 11340: loss 1.211842\n",
      "epoch done... acc 0.486\n",
      "iteration 5400 / 11340: loss 1.419523\n",
      "iteration 5500 / 11340: loss 1.256192\n",
      "iteration 5600 / 11340: loss 1.406742\n",
      "iteration 5700 / 11340: loss 1.340626\n",
      "iteration 5800 / 11340: loss 1.209892\n",
      "epoch done... acc 0.49\n",
      "iteration 5900 / 11340: loss 1.384466\n",
      "iteration 6000 / 11340: loss 1.268615\n",
      "iteration 6100 / 11340: loss 1.480841\n",
      "iteration 6200 / 11340: loss 1.103208\n",
      "iteration 6300 / 11340: loss 1.174144\n",
      "epoch done... acc 0.497\n",
      "iteration 6400 / 11340: loss 0.996740\n",
      "iteration 6500 / 11340: loss 1.273902\n",
      "iteration 6600 / 11340: loss 1.150543\n",
      "iteration 6700 / 11340: loss 1.263160\n",
      "iteration 6800 / 11340: loss 1.209049\n",
      "epoch done... acc 0.509\n",
      "iteration 6900 / 11340: loss 1.135769\n",
      "iteration 7000 / 11340: loss 1.430770\n",
      "iteration 7100 / 11340: loss 1.123805\n",
      "iteration 7200 / 11340: loss 1.093184\n",
      "iteration 7300 / 11340: loss 1.150849\n",
      "epoch done... acc 0.496\n",
      "iteration 7400 / 11340: loss 1.080822\n",
      "iteration 7500 / 11340: loss 1.367451\n",
      "iteration 7600 / 11340: loss 1.163477\n",
      "iteration 7700 / 11340: loss 1.108950\n",
      "iteration 7800 / 11340: loss 1.029768\n",
      "epoch done... acc 0.509\n",
      "iteration 7900 / 11340: loss 1.138439\n",
      "iteration 8000 / 11340: loss 1.067185\n",
      "iteration 8100 / 11340: loss 1.215704\n",
      "iteration 8200 / 11340: loss 1.089013\n",
      "iteration 8300 / 11340: loss 1.129863\n",
      "epoch done... acc 0.521\n",
      "iteration 8400 / 11340: loss 1.249667\n",
      "iteration 8500 / 11340: loss 1.247513\n",
      "iteration 8600 / 11340: loss 1.123540\n",
      "iteration 8700 / 11340: loss 1.137067\n",
      "iteration 8800 / 11340: loss 1.115844\n",
      "epoch done... acc 0.517\n",
      "iteration 8900 / 11340: loss 1.074082\n",
      "iteration 9000 / 11340: loss 1.185398\n",
      "iteration 9100 / 11340: loss 1.228344\n",
      "iteration 9200 / 11340: loss 1.096233\n",
      "iteration 9300 / 11340: loss 1.213414\n",
      "epoch done... acc 0.508\n",
      "iteration 9400 / 11340: loss 1.162539\n",
      "iteration 9500 / 11340: loss 1.065128\n",
      "iteration 9600 / 11340: loss 1.114421\n",
      "iteration 9700 / 11340: loss 1.133864\n",
      "iteration 9800 / 11340: loss 1.014876\n",
      "epoch done... acc 0.531\n",
      "iteration 9900 / 11340: loss 1.014208\n",
      "iteration 10000 / 11340: loss 1.219923\n",
      "iteration 10100 / 11340: loss 1.374925\n",
      "iteration 10200 / 11340: loss 1.136939\n",
      "epoch done... acc 0.508\n",
      "iteration 10300 / 11340: loss 1.240100\n",
      "iteration 10400 / 11340: loss 1.180485\n",
      "iteration 10500 / 11340: loss 1.118593\n",
      "iteration 10600 / 11340: loss 1.275208\n",
      "iteration 10700 / 11340: loss 1.330961\n",
      "epoch done... acc 0.515\n",
      "iteration 10800 / 11340: loss 1.222927\n",
      "iteration 10900 / 11340: loss 1.163915\n",
      "iteration 11000 / 11340: loss 1.085068\n",
      "iteration 11100 / 11340: loss 1.053715\n",
      "iteration 11200 / 11340: loss 1.103740\n",
      "epoch done... acc 0.52\n",
      "iteration 11300 / 11340: loss 1.170729\n",
      "Final training loss:  1.1286373206593487\n",
      "Final validation loss:  1.3797242695218122\n",
      "Final validation accuracy:  0.52\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "33 2 1 385 12600 100 0.001 0.98 0.52\n",
      "iteration 0 / 12600: loss 2.302473\n",
      "epoch done... acc 0.176\n",
      "iteration 100 / 12600: loss 1.937227\n",
      "iteration 200 / 12600: loss 2.007979\n",
      "iteration 300 / 12600: loss 1.898440\n",
      "iteration 400 / 12600: loss 1.713366\n",
      "epoch done... acc 0.402\n",
      "iteration 500 / 12600: loss 1.708480\n",
      "iteration 600 / 12600: loss 1.741254\n",
      "iteration 700 / 12600: loss 1.637255\n",
      "iteration 800 / 12600: loss 1.611860\n",
      "iteration 900 / 12600: loss 1.596990\n",
      "epoch done... acc 0.427\n",
      "iteration 1000 / 12600: loss 1.506028\n",
      "iteration 1100 / 12600: loss 1.441428\n",
      "iteration 1200 / 12600: loss 1.590733\n",
      "iteration 1300 / 12600: loss 1.595719\n",
      "iteration 1400 / 12600: loss 1.578624\n",
      "epoch done... acc 0.458\n",
      "iteration 1500 / 12600: loss 1.432583\n",
      "iteration 1600 / 12600: loss 1.664592\n",
      "iteration 1700 / 12600: loss 1.303371\n",
      "iteration 1800 / 12600: loss 1.467809\n",
      "iteration 1900 / 12600: loss 1.715920\n",
      "epoch done... acc 0.462\n",
      "iteration 2000 / 12600: loss 1.496110\n",
      "iteration 2100 / 12600: loss 1.487975\n",
      "iteration 2200 / 12600: loss 1.352955\n",
      "iteration 2300 / 12600: loss 1.446118\n",
      "iteration 2400 / 12600: loss 1.324610\n",
      "epoch done... acc 0.478\n",
      "iteration 2500 / 12600: loss 1.344133\n",
      "iteration 2600 / 12600: loss 1.542700\n",
      "iteration 2700 / 12600: loss 1.487486\n",
      "iteration 2800 / 12600: loss 1.452407\n",
      "iteration 2900 / 12600: loss 1.371120\n",
      "epoch done... acc 0.473\n",
      "iteration 3000 / 12600: loss 1.454748\n",
      "iteration 3100 / 12600: loss 1.348031\n",
      "iteration 3200 / 12600: loss 1.358069\n",
      "iteration 3300 / 12600: loss 1.440743\n",
      "iteration 3400 / 12600: loss 1.469092\n",
      "epoch done... acc 0.491\n",
      "iteration 3500 / 12600: loss 1.371015\n",
      "iteration 3600 / 12600: loss 1.440329\n",
      "iteration 3700 / 12600: loss 1.341235\n",
      "iteration 3800 / 12600: loss 1.425395\n",
      "iteration 3900 / 12600: loss 1.347315\n",
      "epoch done... acc 0.488\n",
      "iteration 4000 / 12600: loss 1.290766\n",
      "iteration 4100 / 12600: loss 1.404971\n",
      "iteration 4200 / 12600: loss 1.272430\n",
      "iteration 4300 / 12600: loss 1.289976\n",
      "iteration 4400 / 12600: loss 1.532992\n",
      "epoch done... acc 0.48\n",
      "iteration 4500 / 12600: loss 1.300825\n",
      "iteration 4600 / 12600: loss 1.245630\n",
      "iteration 4700 / 12600: loss 1.125687\n",
      "iteration 4800 / 12600: loss 1.255525\n",
      "iteration 4900 / 12600: loss 1.344593\n",
      "epoch done... acc 0.508\n",
      "iteration 5000 / 12600: loss 1.308142\n",
      "iteration 5100 / 12600: loss 1.236540\n",
      "iteration 5200 / 12600: loss 1.308338\n",
      "iteration 5300 / 12600: loss 1.133412\n",
      "epoch done... acc 0.501\n",
      "iteration 5400 / 12600: loss 1.319517\n",
      "iteration 5500 / 12600: loss 1.344207\n",
      "iteration 5600 / 12600: loss 1.245481\n",
      "iteration 5700 / 12600: loss 1.086887\n",
      "iteration 5800 / 12600: loss 1.202982\n",
      "epoch done... acc 0.515\n",
      "iteration 5900 / 12600: loss 1.209828\n",
      "iteration 6000 / 12600: loss 1.136752\n",
      "iteration 6100 / 12600: loss 1.255088\n",
      "iteration 6200 / 12600: loss 1.050574\n",
      "iteration 6300 / 12600: loss 1.189297\n",
      "epoch done... acc 0.526\n",
      "iteration 6400 / 12600: loss 1.130120\n",
      "iteration 6500 / 12600: loss 1.242118\n",
      "iteration 6600 / 12600: loss 1.263407\n",
      "iteration 6700 / 12600: loss 1.316299\n",
      "iteration 6800 / 12600: loss 1.191849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch done... acc 0.516\n",
      "iteration 6900 / 12600: loss 1.378268\n",
      "iteration 7000 / 12600: loss 1.151633\n",
      "iteration 7100 / 12600: loss 1.234299\n",
      "iteration 7200 / 12600: loss 1.068416\n",
      "iteration 7300 / 12600: loss 1.108257\n",
      "epoch done... acc 0.5\n",
      "iteration 7400 / 12600: loss 1.180161\n",
      "iteration 7500 / 12600: loss 1.113441\n",
      "iteration 7600 / 12600: loss 1.264064\n",
      "iteration 7700 / 12600: loss 1.290637\n",
      "iteration 7800 / 12600: loss 1.399365\n",
      "epoch done... acc 0.51\n",
      "iteration 7900 / 12600: loss 1.143873\n",
      "iteration 8000 / 12600: loss 1.230659\n",
      "iteration 8100 / 12600: loss 1.054904\n",
      "iteration 8200 / 12600: loss 1.006171\n",
      "iteration 8300 / 12600: loss 1.130653\n",
      "epoch done... acc 0.496\n",
      "iteration 8400 / 12600: loss 1.139080\n",
      "iteration 8500 / 12600: loss 1.113962\n",
      "iteration 8600 / 12600: loss 1.251283\n",
      "iteration 8700 / 12600: loss 1.079732\n",
      "iteration 8800 / 12600: loss 1.151623\n",
      "epoch done... acc 0.51\n",
      "iteration 8900 / 12600: loss 1.164184\n",
      "iteration 9000 / 12600: loss 1.061180\n",
      "iteration 9100 / 12600: loss 1.115204\n",
      "iteration 9200 / 12600: loss 1.030351\n",
      "iteration 9300 / 12600: loss 0.991623\n",
      "epoch done... acc 0.513\n",
      "iteration 9400 / 12600: loss 1.177989\n",
      "iteration 9500 / 12600: loss 1.125989\n",
      "iteration 9600 / 12600: loss 1.058171\n",
      "iteration 9700 / 12600: loss 1.079873\n",
      "iteration 9800 / 12600: loss 0.976591\n",
      "epoch done... acc 0.512\n",
      "iteration 9900 / 12600: loss 1.154958\n",
      "iteration 10000 / 12600: loss 0.943286\n",
      "iteration 10100 / 12600: loss 1.055973\n",
      "iteration 10200 / 12600: loss 1.206243\n",
      "epoch done... acc 0.507\n",
      "iteration 10300 / 12600: loss 1.194619\n",
      "iteration 10400 / 12600: loss 0.838939\n",
      "iteration 10500 / 12600: loss 1.050597\n",
      "iteration 10600 / 12600: loss 1.102788\n",
      "iteration 10700 / 12600: loss 1.053342\n",
      "epoch done... acc 0.498\n",
      "iteration 10800 / 12600: loss 1.047653\n",
      "iteration 10900 / 12600: loss 1.140579\n",
      "iteration 11000 / 12600: loss 0.938369\n",
      "iteration 11100 / 12600: loss 1.052605\n",
      "iteration 11200 / 12600: loss 1.139387\n",
      "epoch done... acc 0.52\n",
      "iteration 11300 / 12600: loss 1.035698\n",
      "iteration 11400 / 12600: loss 1.086649\n",
      "iteration 11500 / 12600: loss 1.012063\n",
      "iteration 11600 / 12600: loss 1.079825\n",
      "iteration 11700 / 12600: loss 0.884176\n",
      "epoch done... acc 0.535\n",
      "iteration 11800 / 12600: loss 1.041481\n",
      "iteration 11900 / 12600: loss 1.079783\n",
      "iteration 12000 / 12600: loss 1.131821\n",
      "iteration 12100 / 12600: loss 1.227818\n",
      "iteration 12200 / 12600: loss 1.218085\n",
      "epoch done... acc 0.524\n",
      "iteration 12300 / 12600: loss 1.387210\n",
      "iteration 12400 / 12600: loss 1.007429\n",
      "iteration 12500 / 12600: loss 1.024881\n",
      "Final training loss:  1.0903729092828864\n",
      "Final validation loss:  1.377147142138372\n",
      "Final validation accuracy:  0.524\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "34 1 0 385 12600 100 0.001 0.98 0.524\n",
      "iteration 0 / 12600: loss 2.302566\n",
      "epoch done... acc 0.163\n",
      "iteration 100 / 12600: loss 2.005899\n",
      "iteration 200 / 12600: loss 1.899753\n",
      "iteration 300 / 12600: loss 1.876596\n",
      "iteration 400 / 12600: loss 1.840960\n",
      "epoch done... acc 0.37\n",
      "iteration 500 / 12600: loss 1.756472\n",
      "iteration 600 / 12600: loss 1.721924\n",
      "iteration 700 / 12600: loss 1.656564\n",
      "iteration 800 / 12600: loss 1.867098\n",
      "iteration 900 / 12600: loss 1.687625\n",
      "epoch done... acc 0.416\n",
      "iteration 1000 / 12600: loss 1.644622\n",
      "iteration 1100 / 12600: loss 1.604399\n",
      "iteration 1200 / 12600: loss 1.697685\n",
      "iteration 1300 / 12600: loss 1.641183\n",
      "iteration 1400 / 12600: loss 1.686129\n",
      "epoch done... acc 0.453\n",
      "iteration 1500 / 12600: loss 1.567828\n",
      "iteration 1600 / 12600: loss 1.673727\n",
      "iteration 1700 / 12600: loss 1.560428\n",
      "iteration 1800 / 12600: loss 1.585453\n",
      "iteration 1900 / 12600: loss 1.386867\n",
      "epoch done... acc 0.453\n",
      "iteration 2000 / 12600: loss 1.508211\n",
      "iteration 2100 / 12600: loss 1.524027\n",
      "iteration 2200 / 12600: loss 1.445600\n",
      "iteration 2300 / 12600: loss 1.584914\n",
      "iteration 2400 / 12600: loss 1.380126\n",
      "epoch done... acc 0.472\n",
      "iteration 2500 / 12600: loss 1.446180\n",
      "iteration 2600 / 12600: loss 1.458177\n",
      "iteration 2700 / 12600: loss 1.588072\n",
      "iteration 2800 / 12600: loss 1.401181\n",
      "iteration 2900 / 12600: loss 1.373505\n",
      "epoch done... acc 0.468\n",
      "iteration 3000 / 12600: loss 1.726851\n",
      "iteration 3100 / 12600: loss 1.362831\n",
      "iteration 3200 / 12600: loss 1.277811\n",
      "iteration 3300 / 12600: loss 1.277474\n",
      "iteration 3400 / 12600: loss 1.410596\n",
      "epoch done... acc 0.466\n",
      "iteration 3500 / 12600: loss 1.549703\n",
      "iteration 3600 / 12600: loss 1.484932\n",
      "iteration 3700 / 12600: loss 1.358395\n",
      "iteration 3800 / 12600: loss 1.662874\n",
      "iteration 3900 / 12600: loss 1.320774\n",
      "epoch done... acc 0.492\n",
      "iteration 4000 / 12600: loss 1.433448\n",
      "iteration 4100 / 12600: loss 1.412103\n",
      "iteration 4200 / 12600: loss 1.360553\n",
      "iteration 4300 / 12600: loss 1.315146\n",
      "iteration 4400 / 12600: loss 1.286151\n",
      "epoch done... acc 0.482\n",
      "iteration 4500 / 12600: loss 1.519447\n",
      "iteration 4600 / 12600: loss 1.428889\n",
      "iteration 4700 / 12600: loss 1.311521\n",
      "iteration 4800 / 12600: loss 1.336454\n",
      "iteration 4900 / 12600: loss 1.191729\n",
      "epoch done... acc 0.501\n",
      "iteration 5000 / 12600: loss 1.225136\n",
      "iteration 5100 / 12600: loss 1.231277\n",
      "iteration 5200 / 12600: loss 1.412979\n",
      "iteration 5300 / 12600: loss 1.290623\n",
      "epoch done... acc 0.502\n",
      "iteration 5400 / 12600: loss 1.415022\n",
      "iteration 5500 / 12600: loss 1.458318\n",
      "iteration 5600 / 12600: loss 1.257033\n",
      "iteration 5700 / 12600: loss 1.402334\n",
      "iteration 5800 / 12600: loss 1.256036\n",
      "epoch done... acc 0.503\n",
      "iteration 5900 / 12600: loss 1.361708\n",
      "iteration 6000 / 12600: loss 1.261672\n",
      "iteration 6100 / 12600: loss 1.283747\n",
      "iteration 6200 / 12600: loss 1.323642\n",
      "iteration 6300 / 12600: loss 1.215436\n",
      "epoch done... acc 0.523\n",
      "iteration 6400 / 12600: loss 1.331436\n",
      "iteration 6500 / 12600: loss 1.339372\n",
      "iteration 6600 / 12600: loss 1.143941\n",
      "iteration 6700 / 12600: loss 1.149484\n",
      "iteration 6800 / 12600: loss 1.279326\n",
      "epoch done... acc 0.505\n",
      "iteration 6900 / 12600: loss 1.366149\n",
      "iteration 7000 / 12600: loss 1.169768\n",
      "iteration 7100 / 12600: loss 1.199913\n",
      "iteration 7200 / 12600: loss 1.020300\n",
      "iteration 7300 / 12600: loss 1.327069\n",
      "epoch done... acc 0.521\n",
      "iteration 7400 / 12600: loss 1.289746\n",
      "iteration 7500 / 12600: loss 1.238591\n",
      "iteration 7600 / 12600: loss 1.279333\n",
      "iteration 7700 / 12600: loss 1.288545\n",
      "iteration 7800 / 12600: loss 1.277196\n",
      "epoch done... acc 0.504\n",
      "iteration 7900 / 12600: loss 1.316463\n",
      "iteration 8000 / 12600: loss 1.142803\n",
      "iteration 8100 / 12600: loss 1.234702\n",
      "iteration 8200 / 12600: loss 1.122825\n",
      "iteration 8300 / 12600: loss 1.221616\n",
      "epoch done... acc 0.51\n",
      "iteration 8400 / 12600: loss 1.389229\n",
      "iteration 8500 / 12600: loss 1.254445\n",
      "iteration 8600 / 12600: loss 1.293991\n",
      "iteration 8700 / 12600: loss 1.164925\n",
      "iteration 8800 / 12600: loss 1.097541\n",
      "epoch done... acc 0.516\n",
      "iteration 8900 / 12600: loss 1.267056\n",
      "iteration 9000 / 12600: loss 1.255352\n",
      "iteration 9100 / 12600: loss 1.085201\n",
      "iteration 9200 / 12600: loss 1.153618\n",
      "iteration 9300 / 12600: loss 1.237459\n",
      "epoch done... acc 0.522\n",
      "iteration 9400 / 12600: loss 1.143460\n",
      "iteration 9500 / 12600: loss 0.967907\n",
      "iteration 9600 / 12600: loss 1.216447\n",
      "iteration 9700 / 12600: loss 1.079336\n",
      "iteration 9800 / 12600: loss 1.177593\n",
      "epoch done... acc 0.525\n",
      "iteration 9900 / 12600: loss 1.028965\n",
      "iteration 10000 / 12600: loss 1.204523\n",
      "iteration 10100 / 12600: loss 1.087311\n",
      "iteration 10200 / 12600: loss 1.247859\n",
      "epoch done... acc 0.512\n",
      "iteration 10300 / 12600: loss 0.986862\n",
      "iteration 10400 / 12600: loss 0.951405\n",
      "iteration 10500 / 12600: loss 1.237847\n",
      "iteration 10600 / 12600: loss 1.080637\n",
      "iteration 10700 / 12600: loss 1.000665\n",
      "epoch done... acc 0.522\n",
      "iteration 10800 / 12600: loss 1.051194\n",
      "iteration 10900 / 12600: loss 1.048170\n",
      "iteration 11000 / 12600: loss 1.152777\n",
      "iteration 11100 / 12600: loss 0.947713\n",
      "iteration 11200 / 12600: loss 1.135118\n",
      "epoch done... acc 0.513\n",
      "iteration 11300 / 12600: loss 1.031880\n",
      "iteration 11400 / 12600: loss 1.086745\n",
      "iteration 11500 / 12600: loss 0.993979\n",
      "iteration 11600 / 12600: loss 1.109995\n",
      "iteration 11700 / 12600: loss 1.081713\n",
      "epoch done... acc 0.52\n",
      "iteration 11800 / 12600: loss 1.159628\n",
      "iteration 11900 / 12600: loss 1.182307\n",
      "iteration 12000 / 12600: loss 1.140206\n",
      "iteration 12100 / 12600: loss 0.996501\n",
      "iteration 12200 / 12600: loss 0.968847\n",
      "epoch done... acc 0.512\n",
      "iteration 12300 / 12600: loss 1.072180\n",
      "iteration 12400 / 12600: loss 1.136426\n",
      "iteration 12500 / 12600: loss 1.275722\n",
      "Final training loss:  1.1758892373954704\n",
      "Final validation loss:  1.361681261945389\n",
      "Final validation accuracy:  0.512\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "35 2 0 385 12600 100 0.001 0.98 0.512\n",
      "iteration 0 / 13860: loss 2.302582\n",
      "epoch done... acc 0.172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 13860: loss 1.994971\n",
      "iteration 200 / 13860: loss 1.923406\n",
      "iteration 300 / 13860: loss 1.814366\n",
      "iteration 400 / 13860: loss 1.689003\n",
      "epoch done... acc 0.387\n",
      "iteration 500 / 13860: loss 1.752158\n",
      "iteration 600 / 13860: loss 1.864190\n",
      "iteration 700 / 13860: loss 1.696834\n",
      "iteration 800 / 13860: loss 1.752438\n",
      "iteration 900 / 13860: loss 1.499533\n",
      "epoch done... acc 0.429\n",
      "iteration 1000 / 13860: loss 1.519825\n",
      "iteration 1100 / 13860: loss 1.752925\n",
      "iteration 1200 / 13860: loss 1.566022\n",
      "iteration 1300 / 13860: loss 1.700694\n",
      "iteration 1400 / 13860: loss 1.528126\n",
      "epoch done... acc 0.441\n",
      "iteration 1500 / 13860: loss 1.423084\n",
      "iteration 1600 / 13860: loss 1.743043\n",
      "iteration 1700 / 13860: loss 1.513184\n",
      "iteration 1800 / 13860: loss 1.546767\n",
      "iteration 1900 / 13860: loss 1.494185\n",
      "epoch done... acc 0.457\n",
      "iteration 2000 / 13860: loss 1.382029\n",
      "iteration 2100 / 13860: loss 1.396494\n",
      "iteration 2200 / 13860: loss 1.461512\n",
      "iteration 2300 / 13860: loss 1.602424\n",
      "iteration 2400 / 13860: loss 1.309929\n",
      "epoch done... acc 0.466\n",
      "iteration 2500 / 13860: loss 1.730558\n",
      "iteration 2600 / 13860: loss 1.264757\n",
      "iteration 2700 / 13860: loss 1.293913\n",
      "iteration 2800 / 13860: loss 1.585423\n",
      "iteration 2900 / 13860: loss 1.352151\n",
      "epoch done... acc 0.463\n",
      "iteration 3000 / 13860: loss 1.422398\n",
      "iteration 3100 / 13860: loss 1.407846\n",
      "iteration 3200 / 13860: loss 1.228538\n",
      "iteration 3300 / 13860: loss 1.465704\n",
      "iteration 3400 / 13860: loss 1.368301\n",
      "epoch done... acc 0.489\n",
      "iteration 3500 / 13860: loss 1.371468\n",
      "iteration 3600 / 13860: loss 1.376635\n",
      "iteration 3700 / 13860: loss 1.208716\n",
      "iteration 3800 / 13860: loss 1.429484\n",
      "iteration 3900 / 13860: loss 1.285892\n",
      "epoch done... acc 0.497\n",
      "iteration 4000 / 13860: loss 1.446340\n",
      "iteration 4100 / 13860: loss 1.299421\n",
      "iteration 4200 / 13860: loss 1.355310\n",
      "iteration 4300 / 13860: loss 1.556281\n",
      "iteration 4400 / 13860: loss 1.242513\n",
      "epoch done... acc 0.497\n",
      "iteration 4500 / 13860: loss 1.507598\n",
      "iteration 4600 / 13860: loss 1.350833\n",
      "iteration 4700 / 13860: loss 1.108088\n",
      "iteration 4800 / 13860: loss 1.364750\n",
      "iteration 4900 / 13860: loss 1.392639\n",
      "epoch done... acc 0.493\n",
      "iteration 5000 / 13860: loss 1.369832\n",
      "iteration 5100 / 13860: loss 1.183651\n",
      "iteration 5200 / 13860: loss 1.324708\n",
      "iteration 5300 / 13860: loss 1.250236\n",
      "epoch done... acc 0.489\n",
      "iteration 5400 / 13860: loss 1.231673\n",
      "iteration 5500 / 13860: loss 1.264185\n",
      "iteration 5600 / 13860: loss 1.351401\n",
      "iteration 5700 / 13860: loss 1.238412\n",
      "iteration 5800 / 13860: loss 1.159110\n",
      "epoch done... acc 0.502\n",
      "iteration 5900 / 13860: loss 1.342581\n",
      "iteration 6000 / 13860: loss 1.217403\n",
      "iteration 6100 / 13860: loss 1.432282\n",
      "iteration 6200 / 13860: loss 1.185922\n",
      "iteration 6300 / 13860: loss 1.273318\n",
      "epoch done... acc 0.508\n",
      "iteration 6400 / 13860: loss 1.244614\n",
      "iteration 6500 / 13860: loss 1.149997\n",
      "iteration 6600 / 13860: loss 1.223653\n",
      "iteration 6700 / 13860: loss 1.314421\n",
      "iteration 6800 / 13860: loss 1.209855\n",
      "epoch done... acc 0.488\n",
      "iteration 6900 / 13860: loss 1.330506\n",
      "iteration 7000 / 13860: loss 1.255974\n",
      "iteration 7100 / 13860: loss 1.131852\n",
      "iteration 7200 / 13860: loss 1.358055\n",
      "iteration 7300 / 13860: loss 1.233602\n",
      "epoch done... acc 0.511\n",
      "iteration 7400 / 13860: loss 1.255824\n",
      "iteration 7500 / 13860: loss 1.189378\n",
      "iteration 7600 / 13860: loss 1.101861\n",
      "iteration 7700 / 13860: loss 1.346377\n",
      "iteration 7800 / 13860: loss 1.121644\n",
      "epoch done... acc 0.526\n",
      "iteration 7900 / 13860: loss 1.109104\n",
      "iteration 8000 / 13860: loss 1.070303\n",
      "iteration 8100 / 13860: loss 1.157527\n",
      "iteration 8200 / 13860: loss 1.090367\n",
      "iteration 8300 / 13860: loss 1.250306\n",
      "epoch done... acc 0.525\n",
      "iteration 8400 / 13860: loss 1.214690\n",
      "iteration 8500 / 13860: loss 1.052712\n",
      "iteration 8600 / 13860: loss 1.285371\n",
      "iteration 8700 / 13860: loss 1.135869\n",
      "iteration 8800 / 13860: loss 0.951175\n",
      "epoch done... acc 0.526\n",
      "iteration 8900 / 13860: loss 1.313692\n",
      "iteration 9000 / 13860: loss 1.299626\n",
      "iteration 9100 / 13860: loss 1.225026\n",
      "iteration 9200 / 13860: loss 1.024203\n",
      "iteration 9300 / 13860: loss 1.147842\n",
      "epoch done... acc 0.529\n",
      "iteration 9400 / 13860: loss 1.129824\n",
      "iteration 9500 / 13860: loss 1.054144\n",
      "iteration 9600 / 13860: loss 1.219564\n",
      "iteration 9700 / 13860: loss 1.195287\n",
      "iteration 9800 / 13860: loss 1.156517\n",
      "epoch done... acc 0.514\n",
      "iteration 9900 / 13860: loss 1.019328\n",
      "iteration 10000 / 13860: loss 1.134306\n",
      "iteration 10100 / 13860: loss 1.014029\n",
      "iteration 10200 / 13860: loss 1.067458\n",
      "epoch done... acc 0.527\n",
      "iteration 10300 / 13860: loss 1.220612\n",
      "iteration 10400 / 13860: loss 1.106800\n",
      "iteration 10500 / 13860: loss 1.172561\n",
      "iteration 10600 / 13860: loss 1.020370\n",
      "iteration 10700 / 13860: loss 1.324330\n",
      "epoch done... acc 0.528\n",
      "iteration 10800 / 13860: loss 1.106949\n",
      "iteration 10900 / 13860: loss 1.028502\n",
      "iteration 11000 / 13860: loss 1.066531\n",
      "iteration 11100 / 13860: loss 1.098343\n",
      "iteration 11200 / 13860: loss 1.052163\n",
      "epoch done... acc 0.521\n",
      "iteration 11300 / 13860: loss 0.941559\n",
      "iteration 11400 / 13860: loss 1.109161\n",
      "iteration 11500 / 13860: loss 1.064736\n",
      "iteration 11600 / 13860: loss 0.990811\n",
      "iteration 11700 / 13860: loss 1.311594\n",
      "epoch done... acc 0.539\n",
      "iteration 11800 / 13860: loss 1.105053\n",
      "iteration 11900 / 13860: loss 1.006619\n",
      "iteration 12000 / 13860: loss 1.214521\n",
      "iteration 12100 / 13860: loss 0.907955\n",
      "iteration 12200 / 13860: loss 0.975727\n",
      "epoch done... acc 0.543\n",
      "iteration 12300 / 13860: loss 0.957177\n",
      "iteration 12400 / 13860: loss 0.958636\n",
      "iteration 12500 / 13860: loss 1.065198\n",
      "iteration 12600 / 13860: loss 1.049839\n",
      "iteration 12700 / 13860: loss 0.970704\n",
      "epoch done... acc 0.545\n",
      "iteration 12800 / 13860: loss 1.046727\n",
      "iteration 12900 / 13860: loss 1.125534\n",
      "iteration 13000 / 13860: loss 1.111395\n",
      "iteration 13100 / 13860: loss 1.038098\n",
      "iteration 13200 / 13860: loss 1.169118\n",
      "epoch done... acc 0.523\n",
      "iteration 13300 / 13860: loss 0.979950\n",
      "iteration 13400 / 13860: loss 1.101382\n",
      "iteration 13500 / 13860: loss 1.086524\n",
      "iteration 13600 / 13860: loss 0.975155\n",
      "iteration 13700 / 13860: loss 1.002317\n",
      "epoch done... acc 0.516\n",
      "iteration 13800 / 13860: loss 1.003200\n",
      "Final training loss:  1.0296568130791923\n",
      "Final validation loss:  1.3522316238184573\n",
      "Final validation accuracy:  0.516\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "36 1 1 385 12600 100 0.001 0.98 0.516\n",
      "iteration 0 / 11340: loss 2.302635\n",
      "epoch done... acc 0.173\n",
      "iteration 100 / 11340: loss 1.990875\n",
      "iteration 200 / 11340: loss 1.743478\n",
      "iteration 300 / 11340: loss 1.904985\n",
      "iteration 400 / 11340: loss 1.756809\n",
      "epoch done... acc 0.383\n",
      "iteration 500 / 11340: loss 1.727954\n",
      "iteration 600 / 11340: loss 1.905351\n",
      "iteration 700 / 11340: loss 1.661183\n",
      "iteration 800 / 11340: loss 1.727501\n",
      "iteration 900 / 11340: loss 1.611003\n",
      "epoch done... acc 0.432\n",
      "iteration 1000 / 11340: loss 1.571258\n",
      "iteration 1100 / 11340: loss 1.597164\n",
      "iteration 1200 / 11340: loss 1.601277\n",
      "iteration 1300 / 11340: loss 1.674779\n",
      "iteration 1400 / 11340: loss 1.602180\n",
      "epoch done... acc 0.448\n",
      "iteration 1500 / 11340: loss 1.423320\n",
      "iteration 1600 / 11340: loss 1.550822\n",
      "iteration 1700 / 11340: loss 1.638380\n",
      "iteration 1800 / 11340: loss 1.312781\n",
      "iteration 1900 / 11340: loss 1.513813\n",
      "epoch done... acc 0.448\n",
      "iteration 2000 / 11340: loss 1.344265\n",
      "iteration 2100 / 11340: loss 1.473863\n",
      "iteration 2200 / 11340: loss 1.629807\n",
      "iteration 2300 / 11340: loss 1.566423\n",
      "iteration 2400 / 11340: loss 1.510407\n",
      "epoch done... acc 0.467\n",
      "iteration 2500 / 11340: loss 1.436737\n",
      "iteration 2600 / 11340: loss 1.282526\n",
      "iteration 2700 / 11340: loss 1.465678\n",
      "iteration 2800 / 11340: loss 1.350372\n",
      "iteration 2900 / 11340: loss 1.466553\n",
      "epoch done... acc 0.474\n",
      "iteration 3000 / 11340: loss 1.269210\n",
      "iteration 3100 / 11340: loss 1.371925\n",
      "iteration 3200 / 11340: loss 1.429904\n",
      "iteration 3300 / 11340: loss 1.381867\n",
      "iteration 3400 / 11340: loss 1.427102\n",
      "epoch done... acc 0.475\n",
      "iteration 3500 / 11340: loss 1.566502\n",
      "iteration 3600 / 11340: loss 1.476733\n",
      "iteration 3700 / 11340: loss 1.487803\n",
      "iteration 3800 / 11340: loss 1.571483\n",
      "iteration 3900 / 11340: loss 1.295729\n",
      "epoch done... acc 0.488\n",
      "iteration 4000 / 11340: loss 1.471485\n",
      "iteration 4100 / 11340: loss 1.239088\n",
      "iteration 4200 / 11340: loss 1.523841\n",
      "iteration 4300 / 11340: loss 1.349775\n",
      "iteration 4400 / 11340: loss 1.372957\n",
      "epoch done... acc 0.49\n",
      "iteration 4500 / 11340: loss 1.519886\n",
      "iteration 4600 / 11340: loss 1.559942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4700 / 11340: loss 1.391863\n",
      "iteration 4800 / 11340: loss 1.518071\n",
      "iteration 4900 / 11340: loss 1.273643\n",
      "epoch done... acc 0.506\n",
      "iteration 5000 / 11340: loss 1.337275\n",
      "iteration 5100 / 11340: loss 1.319957\n",
      "iteration 5200 / 11340: loss 1.285882\n",
      "iteration 5300 / 11340: loss 1.269847\n",
      "epoch done... acc 0.502\n",
      "iteration 5400 / 11340: loss 1.265333\n",
      "iteration 5500 / 11340: loss 1.136568\n",
      "iteration 5600 / 11340: loss 1.182873\n",
      "iteration 5700 / 11340: loss 1.340640\n",
      "iteration 5800 / 11340: loss 1.328429\n",
      "epoch done... acc 0.5\n",
      "iteration 5900 / 11340: loss 1.367619\n",
      "iteration 6000 / 11340: loss 1.183242\n",
      "iteration 6100 / 11340: loss 1.127474\n",
      "iteration 6200 / 11340: loss 1.366603\n",
      "iteration 6300 / 11340: loss 1.177452\n",
      "epoch done... acc 0.509\n",
      "iteration 6400 / 11340: loss 1.137550\n",
      "iteration 6500 / 11340: loss 1.209499\n",
      "iteration 6600 / 11340: loss 1.252947\n",
      "iteration 6700 / 11340: loss 1.284050\n",
      "iteration 6800 / 11340: loss 1.439848\n",
      "epoch done... acc 0.512\n",
      "iteration 6900 / 11340: loss 1.192012\n",
      "iteration 7000 / 11340: loss 1.227751\n",
      "iteration 7100 / 11340: loss 1.129093\n",
      "iteration 7200 / 11340: loss 1.176511\n",
      "iteration 7300 / 11340: loss 1.240786\n",
      "epoch done... acc 0.497\n",
      "iteration 7400 / 11340: loss 1.001265\n",
      "iteration 7500 / 11340: loss 1.268846\n",
      "iteration 7600 / 11340: loss 1.312432\n",
      "iteration 7700 / 11340: loss 1.339488\n",
      "iteration 7800 / 11340: loss 1.110549\n",
      "epoch done... acc 0.509\n",
      "iteration 7900 / 11340: loss 1.122220\n",
      "iteration 8000 / 11340: loss 1.132388\n",
      "iteration 8100 / 11340: loss 1.172476\n",
      "iteration 8200 / 11340: loss 1.055943\n",
      "iteration 8300 / 11340: loss 1.295958\n",
      "epoch done... acc 0.504\n",
      "iteration 8400 / 11340: loss 1.161232\n",
      "iteration 8500 / 11340: loss 0.919110\n",
      "iteration 8600 / 11340: loss 1.265767\n",
      "iteration 8700 / 11340: loss 1.167607\n",
      "iteration 8800 / 11340: loss 1.196816\n",
      "epoch done... acc 0.511\n",
      "iteration 8900 / 11340: loss 1.281720\n",
      "iteration 9000 / 11340: loss 1.171136\n",
      "iteration 9100 / 11340: loss 1.028291\n",
      "iteration 9200 / 11340: loss 1.079725\n",
      "iteration 9300 / 11340: loss 1.018202\n",
      "epoch done... acc 0.504\n",
      "iteration 9400 / 11340: loss 1.403047\n",
      "iteration 9500 / 11340: loss 1.042926\n",
      "iteration 9600 / 11340: loss 1.048471\n",
      "iteration 9700 / 11340: loss 1.221422\n",
      "iteration 9800 / 11340: loss 1.012468\n",
      "epoch done... acc 0.507\n",
      "iteration 9900 / 11340: loss 0.997827\n",
      "iteration 10000 / 11340: loss 1.220935\n",
      "iteration 10100 / 11340: loss 1.275771\n",
      "iteration 10200 / 11340: loss 1.128918\n",
      "epoch done... acc 0.516\n",
      "iteration 10300 / 11340: loss 0.976827\n",
      "iteration 10400 / 11340: loss 1.114433\n",
      "iteration 10500 / 11340: loss 1.157721\n",
      "iteration 10600 / 11340: loss 1.075550\n",
      "iteration 10700 / 11340: loss 1.149287\n",
      "epoch done... acc 0.532\n",
      "iteration 10800 / 11340: loss 1.228482\n",
      "iteration 10900 / 11340: loss 1.156457\n",
      "iteration 11000 / 11340: loss 0.981653\n",
      "iteration 11100 / 11340: loss 0.999311\n",
      "iteration 11200 / 11340: loss 0.998711\n",
      "epoch done... acc 0.526\n",
      "iteration 11300 / 11340: loss 1.069722\n",
      "Final training loss:  0.862100005970138\n",
      "Final validation loss:  1.3629407607494903\n",
      "Final validation accuracy:  0.526\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "37 2 1 385 12600 100 0.001 0.98 0.526\n",
      "iteration 0 / 12600: loss 2.302701\n",
      "epoch done... acc 0.151\n",
      "iteration 100 / 12600: loss 2.039501\n",
      "iteration 200 / 12600: loss 2.047113\n",
      "iteration 300 / 12600: loss 1.774061\n",
      "iteration 400 / 12600: loss 1.846111\n",
      "epoch done... acc 0.379\n",
      "iteration 500 / 12600: loss 1.670248\n",
      "iteration 600 / 12600: loss 1.755985\n",
      "iteration 700 / 12600: loss 1.627154\n",
      "iteration 800 / 12600: loss 1.582607\n",
      "iteration 900 / 12600: loss 1.622408\n",
      "epoch done... acc 0.422\n",
      "iteration 1000 / 12600: loss 1.603472\n",
      "iteration 1100 / 12600: loss 1.634327\n",
      "iteration 1200 / 12600: loss 1.713675\n",
      "iteration 1300 / 12600: loss 1.589037\n",
      "iteration 1400 / 12600: loss 1.543092\n",
      "epoch done... acc 0.451\n",
      "iteration 1500 / 12600: loss 1.558017\n",
      "iteration 1600 / 12600: loss 1.664815\n",
      "iteration 1700 / 12600: loss 1.630416\n",
      "iteration 1800 / 12600: loss 1.487449\n",
      "iteration 1900 / 12600: loss 1.283348\n",
      "epoch done... acc 0.45\n",
      "iteration 2000 / 12600: loss 1.466514\n",
      "iteration 2100 / 12600: loss 1.499990\n",
      "iteration 2200 / 12600: loss 1.467008\n",
      "iteration 2300 / 12600: loss 1.401760\n",
      "iteration 2400 / 12600: loss 1.423500\n",
      "epoch done... acc 0.473\n",
      "iteration 2500 / 12600: loss 1.361677\n",
      "iteration 2600 / 12600: loss 1.500741\n",
      "iteration 2700 / 12600: loss 1.598345\n",
      "iteration 2800 / 12600: loss 1.636373\n",
      "iteration 2900 / 12600: loss 1.469145\n",
      "epoch done... acc 0.481\n",
      "iteration 3000 / 12600: loss 1.409736\n",
      "iteration 3100 / 12600: loss 1.509363\n",
      "iteration 3200 / 12600: loss 1.275912\n",
      "iteration 3300 / 12600: loss 1.265104\n",
      "iteration 3400 / 12600: loss 1.473971\n",
      "epoch done... acc 0.497\n",
      "iteration 3500 / 12600: loss 1.301026\n",
      "iteration 3600 / 12600: loss 1.342054\n",
      "iteration 3700 / 12600: loss 1.334680\n",
      "iteration 3800 / 12600: loss 1.386288\n",
      "iteration 3900 / 12600: loss 1.492430\n",
      "epoch done... acc 0.496\n",
      "iteration 4000 / 12600: loss 1.233616\n",
      "iteration 4100 / 12600: loss 1.273944\n",
      "iteration 4200 / 12600: loss 1.340656\n",
      "iteration 4300 / 12600: loss 1.332348\n",
      "iteration 4400 / 12600: loss 1.284805\n",
      "epoch done... acc 0.487\n",
      "iteration 4500 / 12600: loss 1.166957\n",
      "iteration 4600 / 12600: loss 1.282233\n",
      "iteration 4700 / 12600: loss 1.245781\n",
      "iteration 4800 / 12600: loss 1.165984\n",
      "iteration 4900 / 12600: loss 1.259579\n",
      "epoch done... acc 0.503\n",
      "iteration 5000 / 12600: loss 1.444320\n",
      "iteration 5100 / 12600: loss 1.302459\n",
      "iteration 5200 / 12600: loss 1.227248\n",
      "iteration 5300 / 12600: loss 1.334028\n",
      "epoch done... acc 0.502\n",
      "iteration 5400 / 12600: loss 1.135379\n",
      "iteration 5500 / 12600: loss 1.096791\n",
      "iteration 5600 / 12600: loss 1.141645\n",
      "iteration 5700 / 12600: loss 1.345533\n",
      "iteration 5800 / 12600: loss 1.301867\n",
      "epoch done... acc 0.513\n",
      "iteration 5900 / 12600: loss 1.301876\n",
      "iteration 6000 / 12600: loss 1.332037\n",
      "iteration 6100 / 12600: loss 1.298146\n",
      "iteration 6200 / 12600: loss 1.155268\n",
      "iteration 6300 / 12600: loss 1.458602\n",
      "epoch done... acc 0.519\n",
      "iteration 6400 / 12600: loss 1.212769\n",
      "iteration 6500 / 12600: loss 1.134483\n",
      "iteration 6600 / 12600: loss 1.041563\n",
      "iteration 6700 / 12600: loss 1.251409\n",
      "iteration 6800 / 12600: loss 1.189796\n",
      "epoch done... acc 0.507\n",
      "iteration 6900 / 12600: loss 1.179064\n",
      "iteration 7000 / 12600: loss 1.107111\n",
      "iteration 7100 / 12600: loss 1.157589\n",
      "iteration 7200 / 12600: loss 1.210860\n",
      "iteration 7300 / 12600: loss 1.076972\n",
      "epoch done... acc 0.507\n",
      "iteration 7400 / 12600: loss 1.130844\n",
      "iteration 7500 / 12600: loss 1.157711\n",
      "iteration 7600 / 12600: loss 1.293155\n",
      "iteration 7700 / 12600: loss 1.249846\n",
      "iteration 7800 / 12600: loss 1.239386\n",
      "epoch done... acc 0.504\n",
      "iteration 7900 / 12600: loss 1.116156\n",
      "iteration 8000 / 12600: loss 1.030447\n",
      "iteration 8100 / 12600: loss 1.228840\n",
      "iteration 8200 / 12600: loss 1.144650\n",
      "iteration 8300 / 12600: loss 1.238113\n",
      "epoch done... acc 0.505\n",
      "iteration 8400 / 12600: loss 1.099010\n",
      "iteration 8500 / 12600: loss 1.168591\n",
      "iteration 8600 / 12600: loss 1.106654\n",
      "iteration 8700 / 12600: loss 1.094413\n",
      "iteration 8800 / 12600: loss 1.176535\n",
      "epoch done... acc 0.528\n",
      "iteration 8900 / 12600: loss 1.371266\n",
      "iteration 9000 / 12600: loss 1.228589\n",
      "iteration 9100 / 12600: loss 1.135721\n",
      "iteration 9200 / 12600: loss 1.180554\n",
      "iteration 9300 / 12600: loss 1.096484\n",
      "epoch done... acc 0.511\n",
      "iteration 9400 / 12600: loss 1.141394\n",
      "iteration 9500 / 12600: loss 1.032956\n",
      "iteration 9600 / 12600: loss 1.131073\n",
      "iteration 9700 / 12600: loss 1.014680\n",
      "iteration 9800 / 12600: loss 1.284699\n",
      "epoch done... acc 0.52\n",
      "iteration 9900 / 12600: loss 1.203816\n",
      "iteration 10000 / 12600: loss 1.100224\n",
      "iteration 10100 / 12600: loss 1.166246\n",
      "iteration 10200 / 12600: loss 1.013559\n",
      "epoch done... acc 0.516\n",
      "iteration 10300 / 12600: loss 1.145609\n",
      "iteration 10400 / 12600: loss 0.994277\n",
      "iteration 10500 / 12600: loss 1.072167\n",
      "iteration 10600 / 12600: loss 1.135829\n",
      "iteration 10700 / 12600: loss 1.038042\n",
      "epoch done... acc 0.518\n",
      "iteration 10800 / 12600: loss 1.187955\n",
      "iteration 10900 / 12600: loss 1.086521\n",
      "iteration 11000 / 12600: loss 1.262123\n",
      "iteration 11100 / 12600: loss 1.114394\n",
      "iteration 11200 / 12600: loss 1.259787\n",
      "epoch done... acc 0.518\n",
      "iteration 11300 / 12600: loss 0.941291\n",
      "iteration 11400 / 12600: loss 1.099834\n",
      "iteration 11500 / 12600: loss 1.109285\n",
      "iteration 11600 / 12600: loss 0.956918\n",
      "iteration 11700 / 12600: loss 0.909189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch done... acc 0.525\n",
      "iteration 11800 / 12600: loss 1.228999\n",
      "iteration 11900 / 12600: loss 1.142126\n",
      "iteration 12000 / 12600: loss 1.067786\n",
      "iteration 12100 / 12600: loss 1.180716\n",
      "iteration 12200 / 12600: loss 1.155061\n",
      "epoch done... acc 0.534\n",
      "iteration 12300 / 12600: loss 1.028301\n",
      "iteration 12400 / 12600: loss 1.061438\n",
      "iteration 12500 / 12600: loss 0.983190\n",
      "Final training loss:  1.2249861768503556\n",
      "Final validation loss:  1.3625575339460057\n",
      "Final validation accuracy:  0.534\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "38 1 0 385 12600 100 0.001 0.98 0.534\n",
      "iteration 0 / 12600: loss 2.302691\n",
      "epoch done... acc 0.139\n",
      "iteration 100 / 12600: loss 2.053784\n",
      "iteration 200 / 12600: loss 1.853382\n",
      "iteration 300 / 12600: loss 1.949174\n",
      "iteration 400 / 12600: loss 1.736329\n",
      "epoch done... acc 0.379\n",
      "iteration 500 / 12600: loss 1.635421\n",
      "iteration 600 / 12600: loss 1.764473\n",
      "iteration 700 / 12600: loss 1.645084\n",
      "iteration 800 / 12600: loss 1.841051\n",
      "iteration 900 / 12600: loss 1.656393\n",
      "epoch done... acc 0.417\n",
      "iteration 1000 / 12600: loss 1.647397\n",
      "iteration 1100 / 12600: loss 1.782124\n",
      "iteration 1200 / 12600: loss 1.451408\n",
      "iteration 1300 / 12600: loss 1.674817\n",
      "iteration 1400 / 12600: loss 1.712826\n",
      "epoch done... acc 0.452\n",
      "iteration 1500 / 12600: loss 1.668570\n",
      "iteration 1600 / 12600: loss 1.555440\n",
      "iteration 1700 / 12600: loss 1.589110\n",
      "iteration 1800 / 12600: loss 1.521563\n",
      "iteration 1900 / 12600: loss 1.399820\n",
      "epoch done... acc 0.45\n",
      "iteration 2000 / 12600: loss 1.487119\n",
      "iteration 2100 / 12600: loss 1.453976\n",
      "iteration 2200 / 12600: loss 1.432690\n",
      "iteration 2300 / 12600: loss 1.488675\n",
      "iteration 2400 / 12600: loss 1.323705\n",
      "epoch done... acc 0.464\n",
      "iteration 2500 / 12600: loss 1.550646\n",
      "iteration 2600 / 12600: loss 1.529992\n",
      "iteration 2700 / 12600: loss 1.361229\n",
      "iteration 2800 / 12600: loss 1.599781\n",
      "iteration 2900 / 12600: loss 1.347400\n",
      "epoch done... acc 0.474\n",
      "iteration 3000 / 12600: loss 1.568293\n",
      "iteration 3100 / 12600: loss 1.378179\n",
      "iteration 3200 / 12600: loss 1.533147\n",
      "iteration 3300 / 12600: loss 1.407787\n",
      "iteration 3400 / 12600: loss 1.443598\n",
      "epoch done... acc 0.488\n",
      "iteration 3500 / 12600: loss 1.454696\n",
      "iteration 3600 / 12600: loss 1.460984\n",
      "iteration 3700 / 12600: loss 1.292257\n",
      "iteration 3800 / 12600: loss 1.374530\n",
      "iteration 3900 / 12600: loss 1.263369\n",
      "epoch done... acc 0.49\n",
      "iteration 4000 / 12600: loss 1.300576\n",
      "iteration 4100 / 12600: loss 1.389977\n",
      "iteration 4200 / 12600: loss 1.268900\n",
      "iteration 4300 / 12600: loss 1.363311\n",
      "iteration 4400 / 12600: loss 1.402390\n",
      "epoch done... acc 0.486\n",
      "iteration 4500 / 12600: loss 1.322934\n",
      "iteration 4600 / 12600: loss 1.470653\n",
      "iteration 4700 / 12600: loss 1.473900\n",
      "iteration 4800 / 12600: loss 1.363836\n",
      "iteration 4900 / 12600: loss 1.356098\n",
      "epoch done... acc 0.489\n",
      "iteration 5000 / 12600: loss 1.364626\n",
      "iteration 5100 / 12600: loss 1.160246\n",
      "iteration 5200 / 12600: loss 1.212247\n",
      "iteration 5300 / 12600: loss 1.224823\n",
      "epoch done... acc 0.507\n",
      "iteration 5400 / 12600: loss 1.467790\n",
      "iteration 5500 / 12600: loss 1.229499\n",
      "iteration 5600 / 12600: loss 1.246550\n",
      "iteration 5700 / 12600: loss 1.067327\n",
      "iteration 5800 / 12600: loss 1.272437\n",
      "epoch done... acc 0.503\n",
      "iteration 5900 / 12600: loss 1.390012\n",
      "iteration 6000 / 12600: loss 1.298842\n",
      "iteration 6100 / 12600: loss 1.151035\n",
      "iteration 6200 / 12600: loss 1.158301\n",
      "iteration 6300 / 12600: loss 1.310591\n",
      "epoch done... acc 0.519\n",
      "iteration 6400 / 12600: loss 1.088213\n",
      "iteration 6500 / 12600: loss 1.128887\n",
      "iteration 6600 / 12600: loss 1.164127\n",
      "iteration 6700 / 12600: loss 1.444785\n",
      "iteration 6800 / 12600: loss 1.270959\n",
      "epoch done... acc 0.503\n",
      "iteration 6900 / 12600: loss 1.280297\n",
      "iteration 7000 / 12600: loss 1.249388\n",
      "iteration 7100 / 12600: loss 1.189503\n",
      "iteration 7200 / 12600: loss 1.222587\n",
      "iteration 7300 / 12600: loss 1.092608\n",
      "epoch done... acc 0.508\n",
      "iteration 7400 / 12600: loss 1.267789\n",
      "iteration 7500 / 12600: loss 1.097757\n",
      "iteration 7600 / 12600: loss 1.023515\n",
      "iteration 7700 / 12600: loss 1.277969\n",
      "iteration 7800 / 12600: loss 1.303971\n",
      "epoch done... acc 0.51\n",
      "iteration 7900 / 12600: loss 1.217626\n",
      "iteration 8000 / 12600: loss 1.215099\n",
      "iteration 8100 / 12600: loss 1.263279\n",
      "iteration 8200 / 12600: loss 1.134353\n",
      "iteration 8300 / 12600: loss 1.151143\n",
      "epoch done... acc 0.503\n",
      "iteration 8400 / 12600: loss 1.284962\n",
      "iteration 8500 / 12600: loss 0.861787\n",
      "iteration 8600 / 12600: loss 1.084212\n",
      "iteration 8700 / 12600: loss 1.371512\n",
      "iteration 8800 / 12600: loss 1.235935\n",
      "epoch done... acc 0.512\n",
      "iteration 8900 / 12600: loss 1.017095\n",
      "iteration 9000 / 12600: loss 1.013216\n",
      "iteration 9100 / 12600: loss 1.148255\n",
      "iteration 9200 / 12600: loss 1.099579\n",
      "iteration 9300 / 12600: loss 1.281395\n",
      "epoch done... acc 0.516\n",
      "iteration 9400 / 12600: loss 1.194277\n",
      "iteration 9500 / 12600: loss 1.162467\n",
      "iteration 9600 / 12600: loss 0.960947\n",
      "iteration 9700 / 12600: loss 1.130346\n",
      "iteration 9800 / 12600: loss 1.167037\n",
      "epoch done... acc 0.515\n",
      "iteration 9900 / 12600: loss 0.985001\n",
      "iteration 10000 / 12600: loss 1.223411\n",
      "iteration 10100 / 12600: loss 1.249024\n",
      "iteration 10200 / 12600: loss 1.375038\n",
      "epoch done... acc 0.516\n",
      "iteration 10300 / 12600: loss 1.130058\n",
      "iteration 10400 / 12600: loss 1.155306\n",
      "iteration 10500 / 12600: loss 1.242270\n",
      "iteration 10600 / 12600: loss 1.146374\n",
      "iteration 10700 / 12600: loss 1.257124\n",
      "epoch done... acc 0.509\n",
      "iteration 10800 / 12600: loss 1.110208\n",
      "iteration 10900 / 12600: loss 1.129822\n",
      "iteration 11000 / 12600: loss 1.085152\n",
      "iteration 11100 / 12600: loss 1.113842\n",
      "iteration 11200 / 12600: loss 1.113894\n",
      "epoch done... acc 0.509\n",
      "iteration 11300 / 12600: loss 1.002419\n",
      "iteration 11400 / 12600: loss 1.066491\n",
      "iteration 11500 / 12600: loss 1.093756\n",
      "iteration 11600 / 12600: loss 1.152599\n",
      "iteration 11700 / 12600: loss 1.134624\n",
      "epoch done... acc 0.528\n",
      "iteration 11800 / 12600: loss 1.043317\n",
      "iteration 11900 / 12600: loss 0.952854\n",
      "iteration 12000 / 12600: loss 1.044115\n",
      "iteration 12100 / 12600: loss 1.045140\n",
      "iteration 12200 / 12600: loss 0.978936\n",
      "epoch done... acc 0.519\n",
      "iteration 12300 / 12600: loss 1.096789\n",
      "iteration 12400 / 12600: loss 1.142757\n",
      "iteration 12500 / 12600: loss 1.001482\n",
      "Final training loss:  0.9236569588342065\n",
      "Final validation loss:  1.37360561133093\n",
      "Final validation accuracy:  0.519\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "39 2 0 385 12600 100 0.001 0.98 0.519\n",
      "iteration 0 / 13860: loss 2.302529\n",
      "epoch done... acc 0.125\n",
      "iteration 100 / 13860: loss 1.998478\n",
      "iteration 200 / 13860: loss 1.808830\n",
      "iteration 300 / 13860: loss 1.739031\n",
      "iteration 400 / 13860: loss 1.805173\n",
      "epoch done... acc 0.378\n",
      "iteration 500 / 13860: loss 1.858393\n",
      "iteration 600 / 13860: loss 1.752789\n",
      "iteration 700 / 13860: loss 1.676389\n",
      "iteration 800 / 13860: loss 1.752004\n",
      "iteration 900 / 13860: loss 1.601471\n",
      "epoch done... acc 0.426\n",
      "iteration 1000 / 13860: loss 1.464753\n",
      "iteration 1100 / 13860: loss 1.614470\n",
      "iteration 1200 / 13860: loss 1.590318\n",
      "iteration 1300 / 13860: loss 1.519391\n",
      "iteration 1400 / 13860: loss 1.618111\n",
      "epoch done... acc 0.451\n",
      "iteration 1500 / 13860: loss 1.643047\n",
      "iteration 1600 / 13860: loss 1.541544\n",
      "iteration 1700 / 13860: loss 1.436608\n",
      "iteration 1800 / 13860: loss 1.539210\n",
      "iteration 1900 / 13860: loss 1.555550\n",
      "epoch done... acc 0.472\n",
      "iteration 2000 / 13860: loss 1.587569\n",
      "iteration 2100 / 13860: loss 1.546307\n",
      "iteration 2200 / 13860: loss 1.579956\n",
      "iteration 2300 / 13860: loss 1.516159\n",
      "iteration 2400 / 13860: loss 1.446038\n",
      "epoch done... acc 0.466\n",
      "iteration 2500 / 13860: loss 1.436964\n",
      "iteration 2600 / 13860: loss 1.573438\n",
      "iteration 2700 / 13860: loss 1.366165\n",
      "iteration 2800 / 13860: loss 1.529440\n",
      "iteration 2900 / 13860: loss 1.391098\n",
      "epoch done... acc 0.479\n",
      "iteration 3000 / 13860: loss 1.408006\n",
      "iteration 3100 / 13860: loss 1.479802\n",
      "iteration 3200 / 13860: loss 1.429050\n",
      "iteration 3300 / 13860: loss 1.365916\n",
      "iteration 3400 / 13860: loss 1.275180\n",
      "epoch done... acc 0.486\n",
      "iteration 3500 / 13860: loss 1.343121\n",
      "iteration 3600 / 13860: loss 1.444989\n",
      "iteration 3700 / 13860: loss 1.510248\n",
      "iteration 3800 / 13860: loss 1.406407\n",
      "iteration 3900 / 13860: loss 1.317317\n",
      "epoch done... acc 0.48\n",
      "iteration 4000 / 13860: loss 1.218521\n",
      "iteration 4100 / 13860: loss 1.406968\n",
      "iteration 4200 / 13860: loss 1.336312\n",
      "iteration 4300 / 13860: loss 1.446606\n",
      "iteration 4400 / 13860: loss 1.593240\n",
      "epoch done... acc 0.493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4500 / 13860: loss 1.333281\n",
      "iteration 4600 / 13860: loss 1.355087\n",
      "iteration 4700 / 13860: loss 1.407700\n",
      "iteration 4800 / 13860: loss 1.264083\n",
      "iteration 4900 / 13860: loss 1.306830\n",
      "epoch done... acc 0.495\n",
      "iteration 5000 / 13860: loss 1.281483\n",
      "iteration 5100 / 13860: loss 1.377689\n",
      "iteration 5200 / 13860: loss 1.308858\n",
      "iteration 5300 / 13860: loss 1.430880\n",
      "epoch done... acc 0.514\n",
      "iteration 5400 / 13860: loss 1.251301\n",
      "iteration 5500 / 13860: loss 1.281039\n",
      "iteration 5600 / 13860: loss 1.501875\n",
      "iteration 5700 / 13860: loss 1.184625\n",
      "iteration 5800 / 13860: loss 1.312361\n",
      "epoch done... acc 0.509\n",
      "iteration 5900 / 13860: loss 1.513246\n",
      "iteration 6000 / 13860: loss 1.220307\n",
      "iteration 6100 / 13860: loss 1.411521\n",
      "iteration 6200 / 13860: loss 1.089640\n",
      "iteration 6300 / 13860: loss 1.136622\n",
      "epoch done... acc 0.534\n",
      "iteration 6400 / 13860: loss 1.301557\n",
      "iteration 6500 / 13860: loss 1.384118\n",
      "iteration 6600 / 13860: loss 1.305316\n",
      "iteration 6700 / 13860: loss 1.169628\n",
      "iteration 6800 / 13860: loss 1.151192\n",
      "epoch done... acc 0.523\n",
      "iteration 6900 / 13860: loss 1.291938\n",
      "iteration 7000 / 13860: loss 1.196159\n",
      "iteration 7100 / 13860: loss 1.375843\n",
      "iteration 7200 / 13860: loss 1.160369\n",
      "iteration 7300 / 13860: loss 1.242878\n",
      "epoch done... acc 0.515\n",
      "iteration 7400 / 13860: loss 1.175540\n",
      "iteration 7500 / 13860: loss 1.215668\n",
      "iteration 7600 / 13860: loss 1.233479\n",
      "iteration 7700 / 13860: loss 1.211262\n",
      "iteration 7800 / 13860: loss 1.047125\n",
      "epoch done... acc 0.523\n",
      "iteration 7900 / 13860: loss 1.163477\n",
      "iteration 8000 / 13860: loss 1.235008\n",
      "iteration 8100 / 13860: loss 1.230991\n",
      "iteration 8200 / 13860: loss 1.202562\n",
      "iteration 8300 / 13860: loss 1.219792\n",
      "epoch done... acc 0.517\n",
      "iteration 8400 / 13860: loss 1.166994\n",
      "iteration 8500 / 13860: loss 1.205407\n",
      "iteration 8600 / 13860: loss 1.242933\n",
      "iteration 8700 / 13860: loss 1.155324\n",
      "iteration 8800 / 13860: loss 1.116747\n",
      "epoch done... acc 0.519\n",
      "iteration 8900 / 13860: loss 1.286439\n",
      "iteration 9000 / 13860: loss 1.119160\n",
      "iteration 9100 / 13860: loss 1.132012\n",
      "iteration 9200 / 13860: loss 0.945057\n",
      "iteration 9300 / 13860: loss 1.103209\n",
      "epoch done... acc 0.509\n",
      "iteration 9400 / 13860: loss 1.193400\n",
      "iteration 9500 / 13860: loss 1.203024\n",
      "iteration 9600 / 13860: loss 1.186979\n",
      "iteration 9700 / 13860: loss 1.393212\n",
      "iteration 9800 / 13860: loss 1.270762\n",
      "epoch done... acc 0.534\n",
      "iteration 9900 / 13860: loss 1.071924\n",
      "iteration 10000 / 13860: loss 1.062269\n",
      "iteration 10100 / 13860: loss 1.205356\n",
      "iteration 10200 / 13860: loss 1.065999\n",
      "epoch done... acc 0.527\n",
      "iteration 10300 / 13860: loss 1.221446\n",
      "iteration 10400 / 13860: loss 1.090091\n",
      "iteration 10500 / 13860: loss 1.013191\n",
      "iteration 10600 / 13860: loss 1.153827\n",
      "iteration 10700 / 13860: loss 0.897233\n",
      "epoch done... acc 0.535\n",
      "iteration 10800 / 13860: loss 1.092678\n",
      "iteration 10900 / 13860: loss 1.017360\n",
      "iteration 11000 / 13860: loss 1.096694\n",
      "iteration 11100 / 13860: loss 1.043649\n",
      "iteration 11200 / 13860: loss 1.185517\n",
      "epoch done... acc 0.537\n",
      "iteration 11300 / 13860: loss 1.145698\n",
      "iteration 11400 / 13860: loss 1.263614\n",
      "iteration 11500 / 13860: loss 1.083215\n",
      "iteration 11600 / 13860: loss 1.242941\n",
      "iteration 11700 / 13860: loss 0.983164\n",
      "epoch done... acc 0.535\n",
      "iteration 11800 / 13860: loss 1.055538\n",
      "iteration 11900 / 13860: loss 1.211742\n",
      "iteration 12000 / 13860: loss 1.099216\n",
      "iteration 12100 / 13860: loss 1.046134\n",
      "iteration 12200 / 13860: loss 1.158890\n",
      "epoch done... acc 0.533\n",
      "iteration 12300 / 13860: loss 1.127460\n",
      "iteration 12400 / 13860: loss 0.970937\n",
      "iteration 12500 / 13860: loss 1.109888\n",
      "iteration 12600 / 13860: loss 1.085141\n",
      "iteration 12700 / 13860: loss 1.037246\n",
      "epoch done... acc 0.523\n",
      "iteration 12800 / 13860: loss 1.089843\n",
      "iteration 12900 / 13860: loss 0.954502\n",
      "iteration 13000 / 13860: loss 1.083538\n",
      "iteration 13100 / 13860: loss 1.111679\n",
      "iteration 13200 / 13860: loss 1.219884\n",
      "epoch done... acc 0.538\n",
      "iteration 13300 / 13860: loss 1.093583\n",
      "iteration 13400 / 13860: loss 0.981930\n",
      "iteration 13500 / 13860: loss 1.108077\n",
      "iteration 13600 / 13860: loss 1.032248\n",
      "iteration 13700 / 13860: loss 1.013076\n",
      "epoch done... acc 0.532\n",
      "iteration 13800 / 13860: loss 1.123716\n",
      "Final training loss:  1.1121971389358085\n",
      "Final validation loss:  1.3532912513243978\n",
      "Final validation accuracy:  0.532\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "40 1 1 385 12600 100 0.001 0.98 0.532\n",
      "iteration 0 / 11340: loss 2.302577\n",
      "epoch done... acc 0.147\n",
      "iteration 100 / 11340: loss 2.045574\n",
      "iteration 200 / 11340: loss 1.908605\n",
      "iteration 300 / 11340: loss 1.852541\n",
      "iteration 400 / 11340: loss 1.906050\n",
      "epoch done... acc 0.383\n",
      "iteration 500 / 11340: loss 1.740205\n",
      "iteration 600 / 11340: loss 1.814246\n",
      "iteration 700 / 11340: loss 1.741722\n",
      "iteration 800 / 11340: loss 1.545930\n",
      "iteration 900 / 11340: loss 1.543365\n",
      "epoch done... acc 0.433\n",
      "iteration 1000 / 11340: loss 1.549197\n",
      "iteration 1100 / 11340: loss 1.637167\n",
      "iteration 1200 / 11340: loss 1.531246\n",
      "iteration 1300 / 11340: loss 1.550873\n",
      "iteration 1400 / 11340: loss 1.645492\n",
      "epoch done... acc 0.455\n",
      "iteration 1500 / 11340: loss 1.818361\n",
      "iteration 1600 / 11340: loss 1.522442\n",
      "iteration 1700 / 11340: loss 1.617323\n",
      "iteration 1800 / 11340: loss 1.464837\n",
      "iteration 1900 / 11340: loss 1.442096\n",
      "epoch done... acc 0.462\n",
      "iteration 2000 / 11340: loss 1.531411\n",
      "iteration 2100 / 11340: loss 1.450081\n",
      "iteration 2200 / 11340: loss 1.601147\n",
      "iteration 2300 / 11340: loss 1.395220\n",
      "iteration 2400 / 11340: loss 1.600798\n",
      "epoch done... acc 0.469\n",
      "iteration 2500 / 11340: loss 1.557504\n",
      "iteration 2600 / 11340: loss 1.432933\n",
      "iteration 2700 / 11340: loss 1.445493\n",
      "iteration 2800 / 11340: loss 1.378846\n",
      "iteration 2900 / 11340: loss 1.550659\n",
      "epoch done... acc 0.47\n",
      "iteration 3000 / 11340: loss 1.316770\n",
      "iteration 3100 / 11340: loss 1.381328\n",
      "iteration 3200 / 11340: loss 1.348756\n",
      "iteration 3300 / 11340: loss 1.512588\n",
      "iteration 3400 / 11340: loss 1.323409\n",
      "epoch done... acc 0.483\n",
      "iteration 3500 / 11340: loss 1.465520\n",
      "iteration 3600 / 11340: loss 1.323721\n",
      "iteration 3700 / 11340: loss 1.503629\n",
      "iteration 3800 / 11340: loss 1.490487\n",
      "iteration 3900 / 11340: loss 1.518072\n",
      "epoch done... acc 0.486\n",
      "iteration 4000 / 11340: loss 1.315069\n",
      "iteration 4100 / 11340: loss 1.377048\n",
      "iteration 4200 / 11340: loss 1.321583\n",
      "iteration 4300 / 11340: loss 1.351425\n",
      "iteration 4400 / 11340: loss 1.278538\n",
      "epoch done... acc 0.492\n",
      "iteration 4500 / 11340: loss 1.190036\n",
      "iteration 4600 / 11340: loss 1.285645\n",
      "iteration 4700 / 11340: loss 1.216847\n",
      "iteration 4800 / 11340: loss 1.275201\n",
      "iteration 4900 / 11340: loss 1.173656\n",
      "epoch done... acc 0.503\n",
      "iteration 5000 / 11340: loss 1.210538\n",
      "iteration 5100 / 11340: loss 1.240033\n",
      "iteration 5200 / 11340: loss 1.535705\n",
      "iteration 5300 / 11340: loss 1.192806\n",
      "epoch done... acc 0.493\n",
      "iteration 5400 / 11340: loss 1.394594\n",
      "iteration 5500 / 11340: loss 1.203562\n",
      "iteration 5600 / 11340: loss 1.321474\n",
      "iteration 5700 / 11340: loss 1.368434\n",
      "iteration 5800 / 11340: loss 1.207114\n",
      "epoch done... acc 0.509\n",
      "iteration 5900 / 11340: loss 1.253729\n",
      "iteration 6000 / 11340: loss 1.023578\n",
      "iteration 6100 / 11340: loss 1.364565\n",
      "iteration 6200 / 11340: loss 1.225872\n",
      "iteration 6300 / 11340: loss 1.311123\n",
      "epoch done... acc 0.505\n",
      "iteration 6400 / 11340: loss 1.208697\n",
      "iteration 6500 / 11340: loss 1.241414\n",
      "iteration 6600 / 11340: loss 1.296348\n",
      "iteration 6700 / 11340: loss 1.342079\n",
      "iteration 6800 / 11340: loss 1.256338\n",
      "epoch done... acc 0.507\n",
      "iteration 6900 / 11340: loss 1.041286\n",
      "iteration 7000 / 11340: loss 1.255089\n",
      "iteration 7100 / 11340: loss 1.080155\n",
      "iteration 7200 / 11340: loss 1.210638\n",
      "iteration 7300 / 11340: loss 1.219214\n",
      "epoch done... acc 0.518\n",
      "iteration 7400 / 11340: loss 1.176688\n",
      "iteration 7500 / 11340: loss 1.218086\n",
      "iteration 7600 / 11340: loss 0.913207\n",
      "iteration 7700 / 11340: loss 1.185904\n",
      "iteration 7800 / 11340: loss 1.291908\n",
      "epoch done... acc 0.51\n",
      "iteration 7900 / 11340: loss 1.147424\n",
      "iteration 8000 / 11340: loss 1.161087\n",
      "iteration 8100 / 11340: loss 1.313351\n",
      "iteration 8200 / 11340: loss 1.218787\n",
      "iteration 8300 / 11340: loss 1.391224\n",
      "epoch done... acc 0.51\n",
      "iteration 8400 / 11340: loss 1.216559\n",
      "iteration 8500 / 11340: loss 1.387299\n",
      "iteration 8600 / 11340: loss 1.110227\n",
      "iteration 8700 / 11340: loss 1.143710\n",
      "iteration 8800 / 11340: loss 1.038192\n",
      "epoch done... acc 0.509\n",
      "iteration 8900 / 11340: loss 1.132987\n",
      "iteration 9000 / 11340: loss 1.142444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9100 / 11340: loss 1.123784\n",
      "iteration 9200 / 11340: loss 1.254561\n",
      "iteration 9300 / 11340: loss 1.260991\n",
      "epoch done... acc 0.516\n",
      "iteration 9400 / 11340: loss 1.141100\n",
      "iteration 9500 / 11340: loss 1.143681\n",
      "iteration 9600 / 11340: loss 1.189831\n",
      "iteration 9700 / 11340: loss 1.075176\n",
      "iteration 9800 / 11340: loss 1.211624\n",
      "epoch done... acc 0.513\n",
      "iteration 9900 / 11340: loss 1.203123\n",
      "iteration 10000 / 11340: loss 1.089473\n",
      "iteration 10100 / 11340: loss 1.010620\n",
      "iteration 10200 / 11340: loss 1.148691\n",
      "epoch done... acc 0.501\n",
      "iteration 10300 / 11340: loss 1.179327\n",
      "iteration 10400 / 11340: loss 0.970760\n",
      "iteration 10500 / 11340: loss 1.502622\n",
      "iteration 10600 / 11340: loss 1.125874\n",
      "iteration 10700 / 11340: loss 1.240723\n",
      "epoch done... acc 0.511\n",
      "iteration 10800 / 11340: loss 1.181946\n",
      "iteration 10900 / 11340: loss 1.142993\n",
      "iteration 11000 / 11340: loss 1.189051\n",
      "iteration 11100 / 11340: loss 1.161856\n",
      "iteration 11200 / 11340: loss 1.117374\n",
      "epoch done... acc 0.512\n",
      "iteration 11300 / 11340: loss 1.239127\n",
      "Final training loss:  0.9702437314043099\n",
      "Final validation loss:  1.3837264188401954\n",
      "Final validation accuracy:  0.512\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "41 2 1 385 12600 100 0.001 0.98 0.512\n",
      "iteration 0 / 12600: loss 2.302641\n",
      "epoch done... acc 0.165\n",
      "iteration 100 / 12600: loss 2.127591\n",
      "iteration 200 / 12600: loss 1.768328\n",
      "iteration 300 / 12600: loss 1.948549\n",
      "iteration 400 / 12600: loss 1.599102\n",
      "epoch done... acc 0.381\n",
      "iteration 500 / 12600: loss 1.866640\n",
      "iteration 600 / 12600: loss 1.712607\n",
      "iteration 700 / 12600: loss 1.842022\n",
      "iteration 800 / 12600: loss 1.738177\n",
      "iteration 900 / 12600: loss 1.804372\n",
      "epoch done... acc 0.42\n",
      "iteration 1000 / 12600: loss 1.568737\n",
      "iteration 1100 / 12600: loss 1.517011\n",
      "iteration 1200 / 12600: loss 1.687777\n",
      "iteration 1300 / 12600: loss 1.624658\n",
      "iteration 1400 / 12600: loss 1.728814\n",
      "epoch done... acc 0.449\n",
      "iteration 1500 / 12600: loss 1.534720\n",
      "iteration 1600 / 12600: loss 1.458597\n",
      "iteration 1700 / 12600: loss 1.584150\n",
      "iteration 1800 / 12600: loss 1.631031\n",
      "iteration 1900 / 12600: loss 1.620169\n",
      "epoch done... acc 0.459\n",
      "iteration 2000 / 12600: loss 1.485145\n",
      "iteration 2100 / 12600: loss 1.375230\n",
      "iteration 2200 / 12600: loss 1.541699\n",
      "iteration 2300 / 12600: loss 1.584687\n",
      "iteration 2400 / 12600: loss 1.405734\n",
      "epoch done... acc 0.464\n",
      "iteration 2500 / 12600: loss 1.356845\n",
      "iteration 2600 / 12600: loss 1.390973\n",
      "iteration 2700 / 12600: loss 1.347378\n",
      "iteration 2800 / 12600: loss 1.406843\n",
      "iteration 2900 / 12600: loss 1.506673\n",
      "epoch done... acc 0.459\n",
      "iteration 3000 / 12600: loss 1.558894\n",
      "iteration 3100 / 12600: loss 1.518591\n",
      "iteration 3200 / 12600: loss 1.415174\n",
      "iteration 3300 / 12600: loss 1.343454\n",
      "iteration 3400 / 12600: loss 1.323810\n",
      "epoch done... acc 0.472\n",
      "iteration 3500 / 12600: loss 1.347689\n",
      "iteration 3600 / 12600: loss 1.201398\n",
      "iteration 3700 / 12600: loss 1.423959\n",
      "iteration 3800 / 12600: loss 1.518756\n",
      "iteration 3900 / 12600: loss 1.286720\n",
      "epoch done... acc 0.478\n",
      "iteration 4000 / 12600: loss 1.211141\n",
      "iteration 4100 / 12600: loss 1.486553\n",
      "iteration 4200 / 12600: loss 1.427961\n",
      "iteration 4300 / 12600: loss 1.251671\n",
      "iteration 4400 / 12600: loss 1.227837\n",
      "epoch done... acc 0.488\n",
      "iteration 4500 / 12600: loss 1.210154\n",
      "iteration 4600 / 12600: loss 1.269747\n",
      "iteration 4700 / 12600: loss 1.286150\n",
      "iteration 4800 / 12600: loss 1.248056\n",
      "iteration 4900 / 12600: loss 1.430359\n",
      "epoch done... acc 0.498\n",
      "iteration 5000 / 12600: loss 1.199830\n",
      "iteration 5100 / 12600: loss 1.266027\n",
      "iteration 5200 / 12600: loss 1.261231\n",
      "iteration 5300 / 12600: loss 1.439448\n",
      "epoch done... acc 0.503\n",
      "iteration 5400 / 12600: loss 1.319128\n",
      "iteration 5500 / 12600: loss 1.422614\n",
      "iteration 5600 / 12600: loss 1.357717\n",
      "iteration 5700 / 12600: loss 1.257115\n",
      "iteration 5800 / 12600: loss 1.097301\n",
      "epoch done... acc 0.494\n",
      "iteration 5900 / 12600: loss 1.292187\n",
      "iteration 6000 / 12600: loss 1.199517\n",
      "iteration 6100 / 12600: loss 1.398125\n",
      "iteration 6200 / 12600: loss 1.154334\n",
      "iteration 6300 / 12600: loss 1.267194\n",
      "epoch done... acc 0.494\n",
      "iteration 6400 / 12600: loss 1.204401\n",
      "iteration 6500 / 12600: loss 1.085130\n",
      "iteration 6600 / 12600: loss 1.145574\n",
      "iteration 6700 / 12600: loss 1.047821\n",
      "iteration 6800 / 12600: loss 1.241635\n",
      "epoch done... acc 0.504\n",
      "iteration 6900 / 12600: loss 1.263395\n",
      "iteration 7000 / 12600: loss 1.280246\n",
      "iteration 7100 / 12600: loss 1.378204\n",
      "iteration 7200 / 12600: loss 1.239675\n",
      "iteration 7300 / 12600: loss 1.233462\n",
      "epoch done... acc 0.505\n",
      "iteration 7400 / 12600: loss 1.158481\n",
      "iteration 7500 / 12600: loss 1.130861\n",
      "iteration 7600 / 12600: loss 1.150318\n",
      "iteration 7700 / 12600: loss 1.147293\n",
      "iteration 7800 / 12600: loss 1.183100\n",
      "epoch done... acc 0.522\n",
      "iteration 7900 / 12600: loss 1.077097\n",
      "iteration 8000 / 12600: loss 1.135887\n",
      "iteration 8100 / 12600: loss 1.121297\n",
      "iteration 8200 / 12600: loss 1.118856\n",
      "iteration 8300 / 12600: loss 1.135919\n",
      "epoch done... acc 0.516\n",
      "iteration 8400 / 12600: loss 1.241641\n",
      "iteration 8500 / 12600: loss 1.162936\n",
      "iteration 8600 / 12600: loss 1.154463\n",
      "iteration 8700 / 12600: loss 1.256183\n",
      "iteration 8800 / 12600: loss 1.130507\n",
      "epoch done... acc 0.519\n",
      "iteration 8900 / 12600: loss 0.996250\n",
      "iteration 9000 / 12600: loss 1.084959\n",
      "iteration 9100 / 12600: loss 1.121360\n",
      "iteration 9200 / 12600: loss 1.096210\n",
      "iteration 9300 / 12600: loss 1.098967\n",
      "epoch done... acc 0.525\n",
      "iteration 9400 / 12600: loss 1.127620\n",
      "iteration 9500 / 12600: loss 1.259476\n",
      "iteration 9600 / 12600: loss 1.277201\n",
      "iteration 9700 / 12600: loss 0.986735\n",
      "iteration 9800 / 12600: loss 1.211355\n",
      "epoch done... acc 0.53\n",
      "iteration 9900 / 12600: loss 1.114493\n",
      "iteration 10000 / 12600: loss 1.209521\n",
      "iteration 10100 / 12600: loss 1.104870\n",
      "iteration 10200 / 12600: loss 1.034526\n",
      "epoch done... acc 0.541\n",
      "iteration 10300 / 12600: loss 1.185353\n",
      "iteration 10400 / 12600: loss 0.909813\n",
      "iteration 10500 / 12600: loss 1.153908\n",
      "iteration 10600 / 12600: loss 1.072950\n",
      "iteration 10700 / 12600: loss 1.112004\n",
      "epoch done... acc 0.53\n",
      "iteration 10800 / 12600: loss 1.023113\n",
      "iteration 10900 / 12600: loss 0.984588\n",
      "iteration 11000 / 12600: loss 0.878483\n",
      "iteration 11100 / 12600: loss 1.037348\n",
      "iteration 11200 / 12600: loss 0.975400\n",
      "epoch done... acc 0.527\n",
      "iteration 11300 / 12600: loss 0.951695\n",
      "iteration 11400 / 12600: loss 1.135779\n",
      "iteration 11500 / 12600: loss 1.107405\n",
      "iteration 11600 / 12600: loss 1.157277\n",
      "iteration 11700 / 12600: loss 0.930164\n",
      "epoch done... acc 0.517\n",
      "iteration 11800 / 12600: loss 1.261159\n",
      "iteration 11900 / 12600: loss 1.017193\n",
      "iteration 12000 / 12600: loss 0.944570\n",
      "iteration 12100 / 12600: loss 1.011337\n",
      "iteration 12200 / 12600: loss 0.963984\n",
      "epoch done... acc 0.537\n",
      "iteration 12300 / 12600: loss 1.011031\n",
      "iteration 12400 / 12600: loss 1.021956\n",
      "iteration 12500 / 12600: loss 1.194021\n",
      "Final training loss:  1.1345288762288268\n",
      "Final validation loss:  1.3309065955431052\n",
      "Final validation accuracy:  0.537\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "42 1 0 385 12600 100 0.001 0.98 0.537\n",
      "iteration 0 / 12600: loss 2.302487\n",
      "epoch done... acc 0.104\n",
      "iteration 100 / 12600: loss 2.022393\n",
      "iteration 200 / 12600: loss 1.866681\n",
      "iteration 300 / 12600: loss 1.924324\n",
      "iteration 400 / 12600: loss 1.745340\n",
      "epoch done... acc 0.374\n",
      "iteration 500 / 12600: loss 1.850007\n",
      "iteration 600 / 12600: loss 1.849969\n",
      "iteration 700 / 12600: loss 1.698881\n",
      "iteration 800 / 12600: loss 1.556977\n",
      "iteration 900 / 12600: loss 1.625070\n",
      "epoch done... acc 0.414\n",
      "iteration 1000 / 12600: loss 1.462628\n",
      "iteration 1100 / 12600: loss 1.555195\n",
      "iteration 1200 / 12600: loss 1.658159\n",
      "iteration 1300 / 12600: loss 1.487782\n",
      "iteration 1400 / 12600: loss 1.705742\n",
      "epoch done... acc 0.456\n",
      "iteration 1500 / 12600: loss 1.649243\n",
      "iteration 1600 / 12600: loss 1.537337\n",
      "iteration 1700 / 12600: loss 1.336450\n",
      "iteration 1800 / 12600: loss 1.573949\n",
      "iteration 1900 / 12600: loss 1.447104\n",
      "epoch done... acc 0.454\n",
      "iteration 2000 / 12600: loss 1.490498\n",
      "iteration 2100 / 12600: loss 1.434341\n",
      "iteration 2200 / 12600: loss 1.345446\n",
      "iteration 2300 / 12600: loss 1.439925\n",
      "iteration 2400 / 12600: loss 1.537387\n",
      "epoch done... acc 0.462\n",
      "iteration 2500 / 12600: loss 1.573999\n",
      "iteration 2600 / 12600: loss 1.455011\n",
      "iteration 2700 / 12600: loss 1.508561\n",
      "iteration 2800 / 12600: loss 1.321579\n",
      "iteration 2900 / 12600: loss 1.401587\n",
      "epoch done... acc 0.466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3000 / 12600: loss 1.531734\n",
      "iteration 3100 / 12600: loss 1.427915\n",
      "iteration 3200 / 12600: loss 1.382195\n",
      "iteration 3300 / 12600: loss 1.366324\n",
      "iteration 3400 / 12600: loss 1.354226\n",
      "epoch done... acc 0.487\n",
      "iteration 3500 / 12600: loss 1.353662\n",
      "iteration 3600 / 12600: loss 1.398464\n",
      "iteration 3700 / 12600: loss 1.248031\n",
      "iteration 3800 / 12600: loss 1.331210\n",
      "iteration 3900 / 12600: loss 1.486216\n",
      "epoch done... acc 0.486\n",
      "iteration 4000 / 12600: loss 1.341672\n",
      "iteration 4100 / 12600: loss 1.375430\n",
      "iteration 4200 / 12600: loss 1.497238\n",
      "iteration 4300 / 12600: loss 1.228629\n",
      "iteration 4400 / 12600: loss 1.315593\n",
      "epoch done... acc 0.498\n",
      "iteration 4500 / 12600: loss 1.359851\n",
      "iteration 4600 / 12600: loss 1.248467\n",
      "iteration 4700 / 12600: loss 1.218565\n",
      "iteration 4800 / 12600: loss 1.289115\n",
      "iteration 4900 / 12600: loss 1.502941\n",
      "epoch done... acc 0.491\n",
      "iteration 5000 / 12600: loss 1.527204\n",
      "iteration 5100 / 12600: loss 1.369645\n",
      "iteration 5200 / 12600: loss 1.059785\n",
      "iteration 5300 / 12600: loss 1.349956\n",
      "epoch done... acc 0.506\n",
      "iteration 5400 / 12600: loss 1.252106\n",
      "iteration 5500 / 12600: loss 1.209515\n",
      "iteration 5600 / 12600: loss 1.455946\n",
      "iteration 5700 / 12600: loss 1.247286\n",
      "iteration 5800 / 12600: loss 1.419549\n",
      "epoch done... acc 0.496\n",
      "iteration 5900 / 12600: loss 1.186828\n",
      "iteration 6000 / 12600: loss 1.234853\n",
      "iteration 6100 / 12600: loss 1.238049\n",
      "iteration 6200 / 12600: loss 1.309866\n",
      "iteration 6300 / 12600: loss 1.290441\n",
      "epoch done... acc 0.497\n",
      "iteration 6400 / 12600: loss 1.270897\n",
      "iteration 6500 / 12600: loss 1.346481\n",
      "iteration 6600 / 12600: loss 1.237922\n",
      "iteration 6700 / 12600: loss 1.224164\n",
      "iteration 6800 / 12600: loss 1.428645\n",
      "epoch done... acc 0.501\n",
      "iteration 6900 / 12600: loss 1.233965\n",
      "iteration 7000 / 12600: loss 1.230596\n",
      "iteration 7100 / 12600: loss 1.164434\n",
      "iteration 7200 / 12600: loss 1.237187\n",
      "iteration 7300 / 12600: loss 1.315695\n",
      "epoch done... acc 0.502\n",
      "iteration 7400 / 12600: loss 1.103775\n",
      "iteration 7500 / 12600: loss 0.982482\n",
      "iteration 7600 / 12600: loss 1.000557\n",
      "iteration 7700 / 12600: loss 1.212876\n",
      "iteration 7800 / 12600: loss 1.066517\n",
      "epoch done... acc 0.5\n",
      "iteration 7900 / 12600: loss 1.209699\n",
      "iteration 8000 / 12600: loss 1.105730\n",
      "iteration 8100 / 12600: loss 1.271506\n",
      "iteration 8200 / 12600: loss 1.159608\n",
      "iteration 8300 / 12600: loss 1.230960\n",
      "epoch done... acc 0.499\n",
      "iteration 8400 / 12600: loss 1.037569\n",
      "iteration 8500 / 12600: loss 1.065295\n",
      "iteration 8600 / 12600: loss 1.017073\n",
      "iteration 8700 / 12600: loss 0.977308\n",
      "iteration 8800 / 12600: loss 0.979334\n",
      "epoch done... acc 0.515\n",
      "iteration 8900 / 12600: loss 1.015206\n",
      "iteration 9000 / 12600: loss 1.194785\n",
      "iteration 9100 / 12600: loss 1.290858\n",
      "iteration 9200 / 12600: loss 1.259157\n",
      "iteration 9300 / 12600: loss 1.119790\n",
      "epoch done... acc 0.518\n",
      "iteration 9400 / 12600: loss 1.211112\n",
      "iteration 9500 / 12600: loss 1.250338\n",
      "iteration 9600 / 12600: loss 0.923216\n",
      "iteration 9700 / 12600: loss 1.161421\n",
      "iteration 9800 / 12600: loss 1.160830\n",
      "epoch done... acc 0.502\n",
      "iteration 9900 / 12600: loss 1.176948\n",
      "iteration 10000 / 12600: loss 1.277384\n",
      "iteration 10100 / 12600: loss 1.272384\n",
      "iteration 10200 / 12600: loss 1.164571\n",
      "epoch done... acc 0.5\n",
      "iteration 10300 / 12600: loss 1.226242\n",
      "iteration 10400 / 12600: loss 1.173068\n",
      "iteration 10500 / 12600: loss 1.250719\n",
      "iteration 10600 / 12600: loss 1.031594\n",
      "iteration 10700 / 12600: loss 1.183018\n",
      "epoch done... acc 0.514\n",
      "iteration 10800 / 12600: loss 1.187489\n",
      "iteration 10900 / 12600: loss 1.261311\n",
      "iteration 11000 / 12600: loss 1.171875\n",
      "iteration 11100 / 12600: loss 1.149008\n",
      "iteration 11200 / 12600: loss 1.179121\n",
      "epoch done... acc 0.515\n",
      "iteration 11300 / 12600: loss 1.147420\n",
      "iteration 11400 / 12600: loss 1.073296\n",
      "iteration 11500 / 12600: loss 1.211354\n",
      "iteration 11600 / 12600: loss 1.135594\n",
      "iteration 11700 / 12600: loss 1.112504\n",
      "epoch done... acc 0.497\n",
      "iteration 11800 / 12600: loss 1.093073\n",
      "iteration 11900 / 12600: loss 1.194484\n",
      "iteration 12000 / 12600: loss 0.991882\n",
      "iteration 12100 / 12600: loss 1.001901\n",
      "iteration 12200 / 12600: loss 1.143928\n",
      "epoch done... acc 0.513\n",
      "iteration 12300 / 12600: loss 1.037776\n",
      "iteration 12400 / 12600: loss 1.256876\n",
      "iteration 12500 / 12600: loss 1.162914\n",
      "Final training loss:  0.9633333973099496\n",
      "Final validation loss:  1.3702877990102367\n",
      "Final validation accuracy:  0.513\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "43 2 0 385 12600 100 0.001 0.98 0.513\n",
      "iteration 0 / 13860: loss 2.302564\n",
      "epoch done... acc 0.09\n",
      "iteration 100 / 13860: loss 2.052774\n",
      "iteration 200 / 13860: loss 1.866846\n",
      "iteration 300 / 13860: loss 1.814871\n",
      "iteration 400 / 13860: loss 1.869160\n",
      "epoch done... acc 0.387\n",
      "iteration 500 / 13860: loss 1.682860\n",
      "iteration 600 / 13860: loss 1.758654\n",
      "iteration 700 / 13860: loss 1.654845\n",
      "iteration 800 / 13860: loss 1.600778\n",
      "iteration 900 / 13860: loss 1.656723\n",
      "epoch done... acc 0.425\n",
      "iteration 1000 / 13860: loss 1.785906\n",
      "iteration 1100 / 13860: loss 1.683353\n",
      "iteration 1200 / 13860: loss 1.447031\n",
      "iteration 1300 / 13860: loss 1.596914\n",
      "iteration 1400 / 13860: loss 1.538688\n",
      "epoch done... acc 0.444\n",
      "iteration 1500 / 13860: loss 1.391277\n",
      "iteration 1600 / 13860: loss 1.628400\n",
      "iteration 1700 / 13860: loss 1.415142\n",
      "iteration 1800 / 13860: loss 1.522846\n",
      "iteration 1900 / 13860: loss 1.507023\n",
      "epoch done... acc 0.471\n",
      "iteration 2000 / 13860: loss 1.568878\n",
      "iteration 2100 / 13860: loss 1.280502\n",
      "iteration 2200 / 13860: loss 1.610571\n",
      "iteration 2300 / 13860: loss 1.415788\n",
      "iteration 2400 / 13860: loss 1.495704\n",
      "epoch done... acc 0.457\n",
      "iteration 2500 / 13860: loss 1.329783\n",
      "iteration 2600 / 13860: loss 1.543808\n",
      "iteration 2700 / 13860: loss 1.453212\n",
      "iteration 2800 / 13860: loss 1.448892\n",
      "iteration 2900 / 13860: loss 1.504227\n",
      "epoch done... acc 0.476\n",
      "iteration 3000 / 13860: loss 1.146373\n",
      "iteration 3100 / 13860: loss 1.315783\n",
      "iteration 3200 / 13860: loss 1.456936\n",
      "iteration 3300 / 13860: loss 1.470338\n",
      "iteration 3400 / 13860: loss 1.529517\n",
      "epoch done... acc 0.488\n",
      "iteration 3500 / 13860: loss 1.456009\n",
      "iteration 3600 / 13860: loss 1.306701\n",
      "iteration 3700 / 13860: loss 1.505366\n",
      "iteration 3800 / 13860: loss 1.316755\n",
      "iteration 3900 / 13860: loss 1.354432\n",
      "epoch done... acc 0.491\n",
      "iteration 4000 / 13860: loss 1.409461\n",
      "iteration 4100 / 13860: loss 1.405688\n",
      "iteration 4200 / 13860: loss 1.333655\n",
      "iteration 4300 / 13860: loss 1.345503\n",
      "iteration 4400 / 13860: loss 1.351327\n",
      "epoch done... acc 0.484\n",
      "iteration 4500 / 13860: loss 1.382690\n",
      "iteration 4600 / 13860: loss 1.419721\n",
      "iteration 4700 / 13860: loss 1.304950\n",
      "iteration 4800 / 13860: loss 1.376491\n",
      "iteration 4900 / 13860: loss 1.468777\n",
      "epoch done... acc 0.496\n",
      "iteration 5000 / 13860: loss 1.339551\n",
      "iteration 5100 / 13860: loss 1.238224\n",
      "iteration 5200 / 13860: loss 1.437877\n",
      "iteration 5300 / 13860: loss 1.285697\n",
      "epoch done... acc 0.504\n",
      "iteration 5400 / 13860: loss 1.268016\n",
      "iteration 5500 / 13860: loss 1.291124\n",
      "iteration 5600 / 13860: loss 1.349663\n",
      "iteration 5700 / 13860: loss 1.277818\n",
      "iteration 5800 / 13860: loss 1.170500\n",
      "epoch done... acc 0.497\n",
      "iteration 5900 / 13860: loss 1.329976\n",
      "iteration 6000 / 13860: loss 1.096387\n",
      "iteration 6100 / 13860: loss 1.176142\n",
      "iteration 6200 / 13860: loss 1.075613\n",
      "iteration 6300 / 13860: loss 1.199112\n",
      "epoch done... acc 0.505\n",
      "iteration 6400 / 13860: loss 1.291151\n",
      "iteration 6500 / 13860: loss 1.346493\n",
      "iteration 6600 / 13860: loss 1.155980\n",
      "iteration 6700 / 13860: loss 1.206251\n",
      "iteration 6800 / 13860: loss 1.256411\n",
      "epoch done... acc 0.518\n",
      "iteration 6900 / 13860: loss 1.219728\n",
      "iteration 7000 / 13860: loss 1.117122\n",
      "iteration 7100 / 13860: loss 1.285369\n",
      "iteration 7200 / 13860: loss 1.221092\n",
      "iteration 7300 / 13860: loss 1.125897\n",
      "epoch done... acc 0.503\n",
      "iteration 7400 / 13860: loss 1.175688\n",
      "iteration 7500 / 13860: loss 1.323346\n",
      "iteration 7600 / 13860: loss 1.138315\n",
      "iteration 7700 / 13860: loss 1.144559\n",
      "iteration 7800 / 13860: loss 1.390388\n",
      "epoch done... acc 0.508\n",
      "iteration 7900 / 13860: loss 1.221351\n",
      "iteration 8000 / 13860: loss 1.445067\n",
      "iteration 8100 / 13860: loss 1.220080\n",
      "iteration 8200 / 13860: loss 1.186380\n",
      "iteration 8300 / 13860: loss 1.210997\n",
      "epoch done... acc 0.518\n",
      "iteration 8400 / 13860: loss 1.195040\n",
      "iteration 8500 / 13860: loss 1.106195\n",
      "iteration 8600 / 13860: loss 1.315936\n",
      "iteration 8700 / 13860: loss 1.162152\n",
      "iteration 8800 / 13860: loss 1.139077\n",
      "epoch done... acc 0.507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 8900 / 13860: loss 1.167424\n",
      "iteration 9000 / 13860: loss 1.033635\n",
      "iteration 9100 / 13860: loss 1.023334\n",
      "iteration 9200 / 13860: loss 1.277576\n",
      "iteration 9300 / 13860: loss 1.336955\n",
      "epoch done... acc 0.513\n",
      "iteration 9400 / 13860: loss 1.054382\n",
      "iteration 9500 / 13860: loss 1.113275\n",
      "iteration 9600 / 13860: loss 1.301566\n",
      "iteration 9700 / 13860: loss 1.324712\n",
      "iteration 9800 / 13860: loss 1.270235\n",
      "epoch done... acc 0.514\n",
      "iteration 9900 / 13860: loss 1.088755\n",
      "iteration 10000 / 13860: loss 1.145553\n",
      "iteration 10100 / 13860: loss 1.127254\n",
      "iteration 10200 / 13860: loss 1.175155\n",
      "epoch done... acc 0.528\n",
      "iteration 10300 / 13860: loss 1.016456\n",
      "iteration 10400 / 13860: loss 1.145207\n",
      "iteration 10500 / 13860: loss 1.185275\n",
      "iteration 10600 / 13860: loss 1.356098\n",
      "iteration 10700 / 13860: loss 0.995868\n",
      "epoch done... acc 0.513\n",
      "iteration 10800 / 13860: loss 1.252135\n",
      "iteration 10900 / 13860: loss 1.164373\n",
      "iteration 11000 / 13860: loss 1.107803\n",
      "iteration 11100 / 13860: loss 1.283041\n",
      "iteration 11200 / 13860: loss 1.083909\n",
      "epoch done... acc 0.512\n",
      "iteration 11300 / 13860: loss 1.068862\n",
      "iteration 11400 / 13860: loss 1.032501\n",
      "iteration 11500 / 13860: loss 1.202931\n",
      "iteration 11600 / 13860: loss 1.110314\n",
      "iteration 11700 / 13860: loss 1.008364\n",
      "epoch done... acc 0.515\n",
      "iteration 11800 / 13860: loss 1.337772\n",
      "iteration 11900 / 13860: loss 1.106386\n",
      "iteration 12000 / 13860: loss 0.942157\n",
      "iteration 12100 / 13860: loss 1.096613\n",
      "iteration 12200 / 13860: loss 1.108296\n",
      "epoch done... acc 0.531\n",
      "iteration 12300 / 13860: loss 1.209906\n",
      "iteration 12400 / 13860: loss 1.085970\n",
      "iteration 12500 / 13860: loss 1.099958\n",
      "iteration 12600 / 13860: loss 1.175918\n",
      "iteration 12700 / 13860: loss 0.948671\n",
      "epoch done... acc 0.531\n",
      "iteration 12800 / 13860: loss 0.973303\n",
      "iteration 12900 / 13860: loss 1.123189\n",
      "iteration 13000 / 13860: loss 0.857300\n",
      "iteration 13100 / 13860: loss 1.055035\n",
      "iteration 13200 / 13860: loss 1.104578\n",
      "epoch done... acc 0.532\n",
      "iteration 13300 / 13860: loss 1.062824\n",
      "iteration 13400 / 13860: loss 1.126806\n",
      "iteration 13500 / 13860: loss 0.881124\n",
      "iteration 13600 / 13860: loss 1.085167\n",
      "iteration 13700 / 13860: loss 1.081085\n",
      "epoch done... acc 0.518\n",
      "iteration 13800 / 13860: loss 1.047956\n",
      "Final training loss:  1.1682447211421747\n",
      "Final validation loss:  1.3559049195470212\n",
      "Final validation accuracy:  0.518\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "44 1 1 385 12600 100 0.001 0.98 0.518\n",
      "iteration 0 / 11340: loss 2.302657\n",
      "epoch done... acc 0.152\n",
      "iteration 100 / 11340: loss 1.951321\n",
      "iteration 200 / 11340: loss 1.907055\n",
      "iteration 300 / 11340: loss 1.829448\n",
      "iteration 400 / 11340: loss 1.702757\n",
      "epoch done... acc 0.378\n",
      "iteration 500 / 11340: loss 1.773276\n",
      "iteration 600 / 11340: loss 1.792902\n",
      "iteration 700 / 11340: loss 1.833036\n",
      "iteration 800 / 11340: loss 1.635039\n",
      "iteration 900 / 11340: loss 1.538908\n",
      "epoch done... acc 0.419\n",
      "iteration 1000 / 11340: loss 1.733851\n",
      "iteration 1100 / 11340: loss 1.627874\n",
      "iteration 1200 / 11340: loss 1.726274\n",
      "iteration 1300 / 11340: loss 1.508640\n",
      "iteration 1400 / 11340: loss 1.397896\n",
      "epoch done... acc 0.444\n",
      "iteration 1500 / 11340: loss 1.460987\n",
      "iteration 1600 / 11340: loss 1.653410\n",
      "iteration 1700 / 11340: loss 1.628745\n",
      "iteration 1800 / 11340: loss 1.715443\n",
      "iteration 1900 / 11340: loss 1.323261\n",
      "epoch done... acc 0.443\n",
      "iteration 2000 / 11340: loss 1.465136\n",
      "iteration 2100 / 11340: loss 1.523210\n",
      "iteration 2200 / 11340: loss 1.533508\n",
      "iteration 2300 / 11340: loss 1.571003\n",
      "iteration 2400 / 11340: loss 1.625971\n",
      "epoch done... acc 0.455\n",
      "iteration 2500 / 11340: loss 1.398681\n",
      "iteration 2600 / 11340: loss 1.367555\n",
      "iteration 2700 / 11340: loss 1.453434\n",
      "iteration 2800 / 11340: loss 1.495788\n",
      "iteration 2900 / 11340: loss 1.365981\n",
      "epoch done... acc 0.479\n",
      "iteration 3000 / 11340: loss 1.228434\n",
      "iteration 3100 / 11340: loss 1.554062\n",
      "iteration 3200 / 11340: loss 1.328982\n",
      "iteration 3300 / 11340: loss 1.431787\n",
      "iteration 3400 / 11340: loss 1.393240\n",
      "epoch done... acc 0.474\n",
      "iteration 3500 / 11340: loss 1.420007\n",
      "iteration 3600 / 11340: loss 1.571939\n",
      "iteration 3700 / 11340: loss 1.349119\n",
      "iteration 3800 / 11340: loss 1.306964\n",
      "iteration 3900 / 11340: loss 1.304203\n",
      "epoch done... acc 0.485\n",
      "iteration 4000 / 11340: loss 1.338983\n",
      "iteration 4100 / 11340: loss 1.501341\n",
      "iteration 4200 / 11340: loss 1.327468\n",
      "iteration 4300 / 11340: loss 1.350504\n",
      "iteration 4400 / 11340: loss 1.230709\n",
      "epoch done... acc 0.491\n",
      "iteration 4500 / 11340: loss 1.221232\n",
      "iteration 4600 / 11340: loss 1.345461\n",
      "iteration 4700 / 11340: loss 1.331772\n",
      "iteration 4800 / 11340: loss 1.317432\n",
      "iteration 4900 / 11340: loss 1.283956\n",
      "epoch done... acc 0.504\n",
      "iteration 5000 / 11340: loss 1.203386\n",
      "iteration 5100 / 11340: loss 1.130333\n",
      "iteration 5200 / 11340: loss 1.174809\n",
      "iteration 5300 / 11340: loss 1.290259\n",
      "epoch done... acc 0.514\n",
      "iteration 5400 / 11340: loss 1.242449\n",
      "iteration 5500 / 11340: loss 1.370258\n",
      "iteration 5600 / 11340: loss 1.235501\n",
      "iteration 5700 / 11340: loss 1.284653\n",
      "iteration 5800 / 11340: loss 1.133092\n",
      "epoch done... acc 0.51\n",
      "iteration 5900 / 11340: loss 1.268580\n",
      "iteration 6000 / 11340: loss 1.168263\n",
      "iteration 6100 / 11340: loss 1.167608\n",
      "iteration 6200 / 11340: loss 1.206975\n",
      "iteration 6300 / 11340: loss 1.274322\n",
      "epoch done... acc 0.495\n",
      "iteration 6400 / 11340: loss 1.376368\n",
      "iteration 6500 / 11340: loss 1.135620\n",
      "iteration 6600 / 11340: loss 1.139125\n",
      "iteration 6700 / 11340: loss 1.328169\n",
      "iteration 6800 / 11340: loss 1.282609\n",
      "epoch done... acc 0.511\n",
      "iteration 6900 / 11340: loss 1.278518\n",
      "iteration 7000 / 11340: loss 1.368045\n",
      "iteration 7100 / 11340: loss 1.162521\n",
      "iteration 7200 / 11340: loss 1.103690\n",
      "iteration 7300 / 11340: loss 1.277903\n",
      "epoch done... acc 0.516\n",
      "iteration 7400 / 11340: loss 1.317224\n",
      "iteration 7500 / 11340: loss 1.375044\n",
      "iteration 7600 / 11340: loss 1.275111\n",
      "iteration 7700 / 11340: loss 1.084041\n",
      "iteration 7800 / 11340: loss 1.284610\n",
      "epoch done... acc 0.51\n",
      "iteration 7900 / 11340: loss 1.109862\n",
      "iteration 8000 / 11340: loss 1.233357\n",
      "iteration 8100 / 11340: loss 1.185020\n",
      "iteration 8200 / 11340: loss 1.217172\n",
      "iteration 8300 / 11340: loss 1.077145\n",
      "epoch done... acc 0.518\n",
      "iteration 8400 / 11340: loss 1.254168\n",
      "iteration 8500 / 11340: loss 1.026777\n",
      "iteration 8600 / 11340: loss 1.128656\n",
      "iteration 8700 / 11340: loss 1.202880\n",
      "iteration 8800 / 11340: loss 1.333481\n",
      "epoch done... acc 0.514\n",
      "iteration 8900 / 11340: loss 1.140017\n",
      "iteration 9000 / 11340: loss 1.248930\n",
      "iteration 9100 / 11340: loss 1.112027\n",
      "iteration 9200 / 11340: loss 1.213293\n",
      "iteration 9300 / 11340: loss 1.251634\n",
      "epoch done... acc 0.522\n",
      "iteration 9400 / 11340: loss 0.959728\n",
      "iteration 9500 / 11340: loss 1.287915\n",
      "iteration 9600 / 11340: loss 1.021535\n",
      "iteration 9700 / 11340: loss 1.021624\n",
      "iteration 9800 / 11340: loss 1.038227\n",
      "epoch done... acc 0.547\n",
      "iteration 9900 / 11340: loss 1.100859\n",
      "iteration 10000 / 11340: loss 1.136961\n",
      "iteration 10100 / 11340: loss 1.201386\n",
      "iteration 10200 / 11340: loss 1.263034\n",
      "epoch done... acc 0.531\n",
      "iteration 10300 / 11340: loss 0.862341\n",
      "iteration 10400 / 11340: loss 0.947953\n",
      "iteration 10500 / 11340: loss 1.022628\n",
      "iteration 10600 / 11340: loss 1.108387\n",
      "iteration 10700 / 11340: loss 0.966805\n",
      "epoch done... acc 0.534\n",
      "iteration 10800 / 11340: loss 1.090564\n",
      "iteration 10900 / 11340: loss 1.049474\n",
      "iteration 11000 / 11340: loss 0.932171\n",
      "iteration 11100 / 11340: loss 0.844212\n",
      "iteration 11200 / 11340: loss 1.071255\n",
      "epoch done... acc 0.524\n",
      "iteration 11300 / 11340: loss 1.278763\n",
      "Final training loss:  1.211633656661348\n",
      "Final validation loss:  1.355472152050354\n",
      "Final validation accuracy:  0.524\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "45 2 1 385 12600 100 0.001 0.98 0.524\n",
      "iteration 0 / 12600: loss 2.302488\n",
      "epoch done... acc 0.111\n",
      "iteration 100 / 12600: loss 2.039899\n",
      "iteration 200 / 12600: loss 1.977721\n",
      "iteration 300 / 12600: loss 1.786310\n",
      "iteration 400 / 12600: loss 1.777480\n",
      "epoch done... acc 0.4\n",
      "iteration 500 / 12600: loss 1.847345\n",
      "iteration 600 / 12600: loss 1.816631\n",
      "iteration 700 / 12600: loss 1.702816\n",
      "iteration 800 / 12600: loss 1.669052\n",
      "iteration 900 / 12600: loss 1.499693\n",
      "epoch done... acc 0.437\n",
      "iteration 1000 / 12600: loss 1.596303\n",
      "iteration 1100 / 12600: loss 1.545352\n",
      "iteration 1200 / 12600: loss 1.719492\n",
      "iteration 1300 / 12600: loss 1.514904\n",
      "iteration 1400 / 12600: loss 1.494208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch done... acc 0.456\n",
      "iteration 1500 / 12600: loss 1.474942\n",
      "iteration 1600 / 12600: loss 1.627648\n",
      "iteration 1700 / 12600: loss 1.514385\n",
      "iteration 1800 / 12600: loss 1.366571\n",
      "iteration 1900 / 12600: loss 1.476959\n",
      "epoch done... acc 0.456\n",
      "iteration 2000 / 12600: loss 1.564580\n",
      "iteration 2100 / 12600: loss 1.615289\n",
      "iteration 2200 / 12600: loss 1.458918\n",
      "iteration 2300 / 12600: loss 1.383999\n",
      "iteration 2400 / 12600: loss 1.580569\n",
      "epoch done... acc 0.459\n",
      "iteration 2500 / 12600: loss 1.530847\n",
      "iteration 2600 / 12600: loss 1.410902\n",
      "iteration 2700 / 12600: loss 1.525676\n",
      "iteration 2800 / 12600: loss 1.408926\n",
      "iteration 2900 / 12600: loss 1.526708\n",
      "epoch done... acc 0.484\n",
      "iteration 3000 / 12600: loss 1.418015\n",
      "iteration 3100 / 12600: loss 1.374301\n",
      "iteration 3200 / 12600: loss 1.426185\n",
      "iteration 3300 / 12600: loss 1.426183\n",
      "iteration 3400 / 12600: loss 1.341383\n",
      "epoch done... acc 0.479\n",
      "iteration 3500 / 12600: loss 1.421783\n",
      "iteration 3600 / 12600: loss 1.608569\n",
      "iteration 3700 / 12600: loss 1.430268\n",
      "iteration 3800 / 12600: loss 1.453345\n",
      "iteration 3900 / 12600: loss 1.462307\n",
      "epoch done... acc 0.507\n",
      "iteration 4000 / 12600: loss 1.322993\n",
      "iteration 4100 / 12600: loss 1.270119\n",
      "iteration 4200 / 12600: loss 1.310052\n",
      "iteration 4300 / 12600: loss 1.400163\n",
      "iteration 4400 / 12600: loss 1.290551\n",
      "epoch done... acc 0.502\n",
      "iteration 4500 / 12600: loss 1.359548\n",
      "iteration 4600 / 12600: loss 1.274598\n",
      "iteration 4700 / 12600: loss 1.179569\n",
      "iteration 4800 / 12600: loss 1.226339\n",
      "iteration 4900 / 12600: loss 1.254379\n",
      "epoch done... acc 0.492\n",
      "iteration 5000 / 12600: loss 1.208624\n",
      "iteration 5100 / 12600: loss 1.385864\n",
      "iteration 5200 / 12600: loss 1.450736\n",
      "iteration 5300 / 12600: loss 1.231019\n",
      "epoch done... acc 0.506\n",
      "iteration 5400 / 12600: loss 1.240937\n",
      "iteration 5500 / 12600: loss 1.303353\n",
      "iteration 5600 / 12600: loss 1.269168\n",
      "iteration 5700 / 12600: loss 1.371659\n",
      "iteration 5800 / 12600: loss 1.167010\n",
      "epoch done... acc 0.501\n",
      "iteration 5900 / 12600: loss 1.241056\n",
      "iteration 6000 / 12600: loss 1.444271\n",
      "iteration 6100 / 12600: loss 1.247337\n",
      "iteration 6200 / 12600: loss 1.331279\n",
      "iteration 6300 / 12600: loss 1.350209\n",
      "epoch done... acc 0.504\n",
      "iteration 6400 / 12600: loss 1.049626\n",
      "iteration 6500 / 12600: loss 1.144217\n",
      "iteration 6600 / 12600: loss 1.174768\n",
      "iteration 6700 / 12600: loss 1.310126\n",
      "iteration 6800 / 12600: loss 1.129788\n",
      "epoch done... acc 0.519\n",
      "iteration 6900 / 12600: loss 1.092770\n",
      "iteration 7000 / 12600: loss 1.112134\n",
      "iteration 7100 / 12600: loss 1.385263\n",
      "iteration 7200 / 12600: loss 1.168183\n",
      "iteration 7300 / 12600: loss 1.245726\n",
      "epoch done... acc 0.509\n",
      "iteration 7400 / 12600: loss 1.214006\n",
      "iteration 7500 / 12600: loss 1.226557\n",
      "iteration 7600 / 12600: loss 1.292906\n",
      "iteration 7700 / 12600: loss 1.263999\n",
      "iteration 7800 / 12600: loss 1.200141\n",
      "epoch done... acc 0.516\n",
      "iteration 7900 / 12600: loss 1.166912\n",
      "iteration 8000 / 12600: loss 1.093618\n",
      "iteration 8100 / 12600: loss 1.294601\n",
      "iteration 8200 / 12600: loss 1.211138\n",
      "iteration 8300 / 12600: loss 1.212630\n",
      "epoch done... acc 0.497\n",
      "iteration 8400 / 12600: loss 1.300022\n",
      "iteration 8500 / 12600: loss 1.117951\n",
      "iteration 8600 / 12600: loss 1.154707\n",
      "iteration 8700 / 12600: loss 0.919980\n",
      "iteration 8800 / 12600: loss 1.287011\n",
      "epoch done... acc 0.52\n",
      "iteration 8900 / 12600: loss 1.340628\n",
      "iteration 9000 / 12600: loss 1.339061\n",
      "iteration 9100 / 12600: loss 1.296916\n",
      "iteration 9200 / 12600: loss 1.138485\n",
      "iteration 9300 / 12600: loss 1.234115\n",
      "epoch done... acc 0.507\n",
      "iteration 9400 / 12600: loss 1.157097\n",
      "iteration 9500 / 12600: loss 1.108683\n",
      "iteration 9600 / 12600: loss 1.138347\n",
      "iteration 9700 / 12600: loss 1.141687\n",
      "iteration 9800 / 12600: loss 1.181236\n",
      "epoch done... acc 0.527\n",
      "iteration 9900 / 12600: loss 1.127590\n",
      "iteration 10000 / 12600: loss 1.175140\n",
      "iteration 10100 / 12600: loss 1.153299\n",
      "iteration 10200 / 12600: loss 1.097106\n",
      "epoch done... acc 0.51\n",
      "iteration 10300 / 12600: loss 1.141334\n",
      "iteration 10400 / 12600: loss 1.159559\n",
      "iteration 10500 / 12600: loss 1.106845\n",
      "iteration 10600 / 12600: loss 1.047223\n",
      "iteration 10700 / 12600: loss 1.132482\n",
      "epoch done... acc 0.544\n",
      "iteration 10800 / 12600: loss 0.952497\n",
      "iteration 10900 / 12600: loss 1.027841\n",
      "iteration 11000 / 12600: loss 0.997384\n",
      "iteration 11100 / 12600: loss 1.050627\n",
      "iteration 11200 / 12600: loss 1.229405\n",
      "epoch done... acc 0.525\n",
      "iteration 11300 / 12600: loss 1.112616\n",
      "iteration 11400 / 12600: loss 1.072962\n",
      "iteration 11500 / 12600: loss 0.899358\n",
      "iteration 11600 / 12600: loss 1.097357\n",
      "iteration 11700 / 12600: loss 1.335846\n",
      "epoch done... acc 0.534\n",
      "iteration 11800 / 12600: loss 1.100256\n",
      "iteration 11900 / 12600: loss 1.011907\n",
      "iteration 12000 / 12600: loss 0.949072\n",
      "iteration 12100 / 12600: loss 1.191591\n",
      "iteration 12200 / 12600: loss 1.110030\n",
      "epoch done... acc 0.519\n",
      "iteration 12300 / 12600: loss 1.135591\n",
      "iteration 12400 / 12600: loss 0.993718\n",
      "iteration 12500 / 12600: loss 1.010535\n",
      "Final training loss:  1.0943190486418295\n",
      "Final validation loss:  1.3632213894399452\n",
      "Final validation accuracy:  0.519\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "46 1 0 385 12600 100 0.001 0.98 0.519\n",
      "iteration 0 / 12600: loss 2.302609\n",
      "epoch done... acc 0.129\n",
      "iteration 100 / 12600: loss 2.117499\n",
      "iteration 200 / 12600: loss 1.963451\n",
      "iteration 300 / 12600: loss 1.918749\n",
      "iteration 400 / 12600: loss 1.782052\n",
      "epoch done... acc 0.365\n",
      "iteration 500 / 12600: loss 1.793341\n",
      "iteration 600 / 12600: loss 1.566159\n",
      "iteration 700 / 12600: loss 1.718881\n",
      "iteration 800 / 12600: loss 1.619390\n",
      "iteration 900 / 12600: loss 1.656280\n",
      "epoch done... acc 0.429\n",
      "iteration 1000 / 12600: loss 1.509122\n",
      "iteration 1100 / 12600: loss 1.638478\n",
      "iteration 1200 / 12600: loss 1.713753\n",
      "iteration 1300 / 12600: loss 1.379976\n",
      "iteration 1400 / 12600: loss 1.468949\n",
      "epoch done... acc 0.445\n",
      "iteration 1500 / 12600: loss 1.595240\n",
      "iteration 1600 / 12600: loss 1.530053\n",
      "iteration 1700 / 12600: loss 1.373059\n",
      "iteration 1800 / 12600: loss 1.707842\n",
      "iteration 1900 / 12600: loss 1.404969\n",
      "epoch done... acc 0.467\n",
      "iteration 2000 / 12600: loss 1.539345\n",
      "iteration 2100 / 12600: loss 1.444318\n",
      "iteration 2200 / 12600: loss 1.491150\n",
      "iteration 2300 / 12600: loss 1.432347\n",
      "iteration 2400 / 12600: loss 1.399101\n",
      "epoch done... acc 0.469\n",
      "iteration 2500 / 12600: loss 1.430217\n",
      "iteration 2600 / 12600: loss 1.402613\n",
      "iteration 2700 / 12600: loss 1.435814\n",
      "iteration 2800 / 12600: loss 1.304146\n",
      "iteration 2900 / 12600: loss 1.469913\n",
      "epoch done... acc 0.47\n",
      "iteration 3000 / 12600: loss 1.611483\n",
      "iteration 3100 / 12600: loss 1.445079\n",
      "iteration 3200 / 12600: loss 1.565344\n",
      "iteration 3300 / 12600: loss 1.553075\n",
      "iteration 3400 / 12600: loss 1.258943\n",
      "epoch done... acc 0.477\n",
      "iteration 3500 / 12600: loss 1.503604\n",
      "iteration 3600 / 12600: loss 1.532450\n",
      "iteration 3700 / 12600: loss 1.333205\n",
      "iteration 3800 / 12600: loss 1.210220\n",
      "iteration 3900 / 12600: loss 1.226620\n",
      "epoch done... acc 0.486\n",
      "iteration 4000 / 12600: loss 1.446167\n",
      "iteration 4100 / 12600: loss 1.459208\n",
      "iteration 4200 / 12600: loss 1.366618\n",
      "iteration 4300 / 12600: loss 1.267026\n",
      "iteration 4400 / 12600: loss 1.233133\n",
      "epoch done... acc 0.493\n",
      "iteration 4500 / 12600: loss 1.441653\n",
      "iteration 4600 / 12600: loss 1.554498\n",
      "iteration 4700 / 12600: loss 1.345551\n",
      "iteration 4800 / 12600: loss 1.500869\n",
      "iteration 4900 / 12600: loss 1.263784\n",
      "epoch done... acc 0.497\n",
      "iteration 5000 / 12600: loss 1.259165\n",
      "iteration 5100 / 12600: loss 1.398418\n",
      "iteration 5200 / 12600: loss 1.111453\n",
      "iteration 5300 / 12600: loss 1.283702\n",
      "epoch done... acc 0.507\n",
      "iteration 5400 / 12600: loss 1.420188\n",
      "iteration 5500 / 12600: loss 1.290638\n",
      "iteration 5600 / 12600: loss 1.232155\n",
      "iteration 5700 / 12600: loss 1.263618\n",
      "iteration 5800 / 12600: loss 1.450672\n",
      "epoch done... acc 0.499\n",
      "iteration 5900 / 12600: loss 1.318217\n",
      "iteration 6000 / 12600: loss 1.139856\n",
      "iteration 6100 / 12600: loss 1.424125\n",
      "iteration 6200 / 12600: loss 1.256673\n",
      "iteration 6300 / 12600: loss 1.224566\n",
      "epoch done... acc 0.517\n",
      "iteration 6400 / 12600: loss 1.399414\n",
      "iteration 6500 / 12600: loss 1.063165\n",
      "iteration 6600 / 12600: loss 1.253157\n",
      "iteration 6700 / 12600: loss 1.306456\n",
      "iteration 6800 / 12600: loss 1.184783\n",
      "epoch done... acc 0.52\n",
      "iteration 6900 / 12600: loss 1.191827\n",
      "iteration 7000 / 12600: loss 1.362534\n",
      "iteration 7100 / 12600: loss 1.525412\n",
      "iteration 7200 / 12600: loss 1.378147\n",
      "iteration 7300 / 12600: loss 1.230460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch done... acc 0.517\n",
      "iteration 7400 / 12600: loss 1.133549\n",
      "iteration 7500 / 12600: loss 1.118321\n",
      "iteration 7600 / 12600: loss 1.295855\n",
      "iteration 7700 / 12600: loss 1.085018\n",
      "iteration 7800 / 12600: loss 1.110970\n",
      "epoch done... acc 0.514\n",
      "iteration 7900 / 12600: loss 1.243992\n",
      "iteration 8000 / 12600: loss 1.252534\n",
      "iteration 8100 / 12600: loss 1.253041\n",
      "iteration 8200 / 12600: loss 1.243696\n",
      "iteration 8300 / 12600: loss 1.229858\n",
      "epoch done... acc 0.523\n",
      "iteration 8400 / 12600: loss 1.225580\n",
      "iteration 8500 / 12600: loss 1.031054\n",
      "iteration 8600 / 12600: loss 1.187021\n",
      "iteration 8700 / 12600: loss 1.211608\n",
      "iteration 8800 / 12600: loss 1.044897\n",
      "epoch done... acc 0.528\n",
      "iteration 8900 / 12600: loss 1.309176\n",
      "iteration 9000 / 12600: loss 1.235770\n",
      "iteration 9100 / 12600: loss 1.169012\n",
      "iteration 9200 / 12600: loss 1.147527\n",
      "iteration 9300 / 12600: loss 1.251975\n",
      "epoch done... acc 0.526\n",
      "iteration 9400 / 12600: loss 1.121176\n",
      "iteration 9500 / 12600: loss 1.223414\n",
      "iteration 9600 / 12600: loss 1.070357\n",
      "iteration 9700 / 12600: loss 1.328508\n",
      "iteration 9800 / 12600: loss 1.095119\n",
      "epoch done... acc 0.532\n",
      "iteration 9900 / 12600: loss 1.162454\n",
      "iteration 10000 / 12600: loss 1.108657\n",
      "iteration 10100 / 12600: loss 1.255036\n",
      "iteration 10200 / 12600: loss 1.198800\n",
      "epoch done... acc 0.53\n",
      "iteration 10300 / 12600: loss 1.095617\n",
      "iteration 10400 / 12600: loss 1.017439\n",
      "iteration 10500 / 12600: loss 1.070484\n",
      "iteration 10600 / 12600: loss 0.994509\n",
      "iteration 10700 / 12600: loss 1.112499\n",
      "epoch done... acc 0.527\n",
      "iteration 10800 / 12600: loss 1.174958\n",
      "iteration 10900 / 12600: loss 1.045479\n",
      "iteration 11000 / 12600: loss 1.200428\n",
      "iteration 11100 / 12600: loss 1.055626\n",
      "iteration 11200 / 12600: loss 1.084730\n",
      "epoch done... acc 0.53\n",
      "iteration 11300 / 12600: loss 1.133525\n",
      "iteration 11400 / 12600: loss 1.141132\n",
      "iteration 11500 / 12600: loss 1.138492\n",
      "iteration 11600 / 12600: loss 0.954819\n",
      "iteration 11700 / 12600: loss 1.121421\n",
      "epoch done... acc 0.519\n",
      "iteration 11800 / 12600: loss 1.180069\n",
      "iteration 11900 / 12600: loss 1.128992\n",
      "iteration 12000 / 12600: loss 1.144329\n",
      "iteration 12100 / 12600: loss 1.033774\n",
      "iteration 12200 / 12600: loss 1.058300\n",
      "epoch done... acc 0.534\n",
      "iteration 12300 / 12600: loss 1.197116\n",
      "iteration 12400 / 12600: loss 1.025996\n",
      "iteration 12500 / 12600: loss 1.280354\n",
      "Final training loss:  1.076341270977204\n",
      "Final validation loss:  1.3509530375236118\n",
      "Final validation accuracy:  0.534\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "47 2 0 385 12600 100 0.001 0.98 0.534\n",
      "iteration 0 / 13860: loss 2.302592\n",
      "epoch done... acc 0.112\n",
      "iteration 100 / 13860: loss 2.003349\n",
      "iteration 200 / 13860: loss 1.876780\n",
      "iteration 300 / 13860: loss 1.938847\n",
      "iteration 400 / 13860: loss 1.863290\n",
      "epoch done... acc 0.38\n",
      "iteration 500 / 13860: loss 1.698959\n",
      "iteration 600 / 13860: loss 1.696874\n",
      "iteration 700 / 13860: loss 1.838938\n",
      "iteration 800 / 13860: loss 1.493112\n",
      "iteration 900 / 13860: loss 1.456346\n",
      "epoch done... acc 0.421\n",
      "iteration 1000 / 13860: loss 1.708746\n",
      "iteration 1100 / 13860: loss 1.657824\n",
      "iteration 1200 / 13860: loss 1.534079\n",
      "iteration 1300 / 13860: loss 1.665793\n",
      "iteration 1400 / 13860: loss 1.546588\n",
      "epoch done... acc 0.463\n",
      "iteration 1500 / 13860: loss 1.472509\n",
      "iteration 1600 / 13860: loss 1.585489\n",
      "iteration 1700 / 13860: loss 1.543806\n",
      "iteration 1800 / 13860: loss 1.583507\n",
      "iteration 1900 / 13860: loss 1.509342\n",
      "epoch done... acc 0.458\n",
      "iteration 2000 / 13860: loss 1.319807\n",
      "iteration 2100 / 13860: loss 1.523939\n",
      "iteration 2200 / 13860: loss 1.424837\n",
      "iteration 2300 / 13860: loss 1.405135\n",
      "iteration 2400 / 13860: loss 1.401616\n",
      "epoch done... acc 0.456\n",
      "iteration 2500 / 13860: loss 1.548837\n",
      "iteration 2600 / 13860: loss 1.417895\n",
      "iteration 2700 / 13860: loss 1.456909\n",
      "iteration 2800 / 13860: loss 1.424677\n",
      "iteration 2900 / 13860: loss 1.213280\n",
      "epoch done... acc 0.485\n",
      "iteration 3000 / 13860: loss 1.447205\n",
      "iteration 3100 / 13860: loss 1.435599\n",
      "iteration 3200 / 13860: loss 1.548517\n",
      "iteration 3300 / 13860: loss 1.444422\n",
      "iteration 3400 / 13860: loss 1.388888\n",
      "epoch done... acc 0.477\n",
      "iteration 3500 / 13860: loss 1.289100\n",
      "iteration 3600 / 13860: loss 1.382467\n",
      "iteration 3700 / 13860: loss 1.505678\n",
      "iteration 3800 / 13860: loss 1.368135\n",
      "iteration 3900 / 13860: loss 1.310906\n",
      "epoch done... acc 0.485\n",
      "iteration 4000 / 13860: loss 1.363070\n",
      "iteration 4100 / 13860: loss 1.433572\n",
      "iteration 4200 / 13860: loss 1.440333\n",
      "iteration 4300 / 13860: loss 1.353377\n",
      "iteration 4400 / 13860: loss 1.365872\n",
      "epoch done... acc 0.501\n",
      "iteration 4500 / 13860: loss 1.390694\n",
      "iteration 4600 / 13860: loss 1.343003\n",
      "iteration 4700 / 13860: loss 1.282511\n",
      "iteration 4800 / 13860: loss 1.359550\n",
      "iteration 4900 / 13860: loss 1.270863\n",
      "epoch done... acc 0.5\n",
      "iteration 5000 / 13860: loss 1.092927\n",
      "iteration 5100 / 13860: loss 1.332073\n",
      "iteration 5200 / 13860: loss 1.244853\n",
      "iteration 5300 / 13860: loss 1.259139\n",
      "epoch done... acc 0.492\n",
      "iteration 5400 / 13860: loss 1.359067\n",
      "iteration 5500 / 13860: loss 1.437599\n",
      "iteration 5600 / 13860: loss 1.259870\n",
      "iteration 5700 / 13860: loss 1.376061\n",
      "iteration 5800 / 13860: loss 1.328705\n",
      "epoch done... acc 0.501\n",
      "iteration 5900 / 13860: loss 1.294255\n",
      "iteration 6000 / 13860: loss 1.340119\n",
      "iteration 6100 / 13860: loss 1.149180\n",
      "iteration 6200 / 13860: loss 1.326740\n",
      "iteration 6300 / 13860: loss 1.336485\n",
      "epoch done... acc 0.507\n",
      "iteration 6400 / 13860: loss 1.382591\n",
      "iteration 6500 / 13860: loss 1.246789\n",
      "iteration 6600 / 13860: loss 1.102909\n",
      "iteration 6700 / 13860: loss 1.180540\n",
      "iteration 6800 / 13860: loss 1.245807\n",
      "epoch done... acc 0.492\n",
      "iteration 6900 / 13860: loss 1.136152\n",
      "iteration 7000 / 13860: loss 1.112268\n",
      "iteration 7100 / 13860: loss 1.151819\n",
      "iteration 7200 / 13860: loss 1.403434\n",
      "iteration 7300 / 13860: loss 1.199100\n",
      "epoch done... acc 0.51\n",
      "iteration 7400 / 13860: loss 1.106565\n",
      "iteration 7500 / 13860: loss 1.010507\n",
      "iteration 7600 / 13860: loss 1.166231\n",
      "iteration 7700 / 13860: loss 1.171182\n",
      "iteration 7800 / 13860: loss 1.181085\n",
      "epoch done... acc 0.512\n",
      "iteration 7900 / 13860: loss 1.097507\n",
      "iteration 8000 / 13860: loss 1.283043\n",
      "iteration 8100 / 13860: loss 1.146003\n",
      "iteration 8200 / 13860: loss 1.338047\n",
      "iteration 8300 / 13860: loss 1.177939\n",
      "epoch done... acc 0.52\n",
      "iteration 8400 / 13860: loss 1.230989\n",
      "iteration 8500 / 13860: loss 1.069515\n",
      "iteration 8600 / 13860: loss 1.212150\n",
      "iteration 8700 / 13860: loss 1.297138\n",
      "iteration 8800 / 13860: loss 1.069975\n",
      "epoch done... acc 0.513\n",
      "iteration 8900 / 13860: loss 1.175169\n",
      "iteration 9000 / 13860: loss 1.212052\n",
      "iteration 9100 / 13860: loss 1.384910\n",
      "iteration 9200 / 13860: loss 1.153638\n",
      "iteration 9300 / 13860: loss 1.225429\n",
      "epoch done... acc 0.508\n",
      "iteration 9400 / 13860: loss 1.249580\n",
      "iteration 9500 / 13860: loss 0.957616\n",
      "iteration 9600 / 13860: loss 1.260128\n",
      "iteration 9700 / 13860: loss 1.162794\n",
      "iteration 9800 / 13860: loss 1.009347\n",
      "epoch done... acc 0.524\n",
      "iteration 9900 / 13860: loss 1.123565\n",
      "iteration 10000 / 13860: loss 1.236134\n",
      "iteration 10100 / 13860: loss 1.233540\n",
      "iteration 10200 / 13860: loss 1.003808\n",
      "epoch done... acc 0.515\n",
      "iteration 10300 / 13860: loss 1.122999\n",
      "iteration 10400 / 13860: loss 1.092400\n",
      "iteration 10500 / 13860: loss 1.024508\n",
      "iteration 10600 / 13860: loss 1.281991\n",
      "iteration 10700 / 13860: loss 1.210849\n",
      "epoch done... acc 0.536\n",
      "iteration 10800 / 13860: loss 1.049830\n",
      "iteration 10900 / 13860: loss 1.153754\n",
      "iteration 11000 / 13860: loss 1.136998\n",
      "iteration 11100 / 13860: loss 1.077225\n",
      "iteration 11200 / 13860: loss 1.367043\n",
      "epoch done... acc 0.527\n",
      "iteration 11300 / 13860: loss 1.029580\n",
      "iteration 11400 / 13860: loss 1.126012\n",
      "iteration 11500 / 13860: loss 1.131264\n",
      "iteration 11600 / 13860: loss 1.060058\n",
      "iteration 11700 / 13860: loss 0.928582\n",
      "epoch done... acc 0.525\n",
      "iteration 11800 / 13860: loss 1.011475\n",
      "iteration 11900 / 13860: loss 0.982611\n",
      "iteration 12000 / 13860: loss 1.038512\n",
      "iteration 12100 / 13860: loss 1.082264\n",
      "iteration 12200 / 13860: loss 1.098576\n",
      "epoch done... acc 0.519\n",
      "iteration 12300 / 13860: loss 1.066166\n",
      "iteration 12400 / 13860: loss 1.005307\n",
      "iteration 12500 / 13860: loss 1.097363\n",
      "iteration 12600 / 13860: loss 1.141148\n",
      "iteration 12700 / 13860: loss 0.849893\n",
      "epoch done... acc 0.524\n",
      "iteration 12800 / 13860: loss 1.144395\n",
      "iteration 12900 / 13860: loss 1.198534\n",
      "iteration 13000 / 13860: loss 1.073517\n",
      "iteration 13100 / 13860: loss 1.051130\n",
      "iteration 13200 / 13860: loss 0.973370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch done... acc 0.527\n",
      "iteration 13300 / 13860: loss 1.053028\n",
      "iteration 13400 / 13860: loss 0.960269\n",
      "iteration 13500 / 13860: loss 1.100073\n",
      "iteration 13600 / 13860: loss 1.027531\n",
      "iteration 13700 / 13860: loss 1.020545\n",
      "epoch done... acc 0.534\n",
      "iteration 13800 / 13860: loss 1.226341\n",
      "Final training loss:  1.0042633381137318\n",
      "Final validation loss:  1.357213074818186\n",
      "Final validation accuracy:  0.534\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "48 1 1 385 12600 100 0.001 0.98 0.534\n",
      "iteration 0 / 11340: loss 2.302530\n",
      "epoch done... acc 0.146\n",
      "iteration 100 / 11340: loss 1.948271\n",
      "iteration 200 / 11340: loss 1.866916\n",
      "iteration 300 / 11340: loss 1.862949\n",
      "iteration 400 / 11340: loss 1.697811\n",
      "epoch done... acc 0.377\n",
      "iteration 500 / 11340: loss 1.583078\n",
      "iteration 600 / 11340: loss 1.740953\n",
      "iteration 700 / 11340: loss 1.663205\n",
      "iteration 800 / 11340: loss 1.617808\n",
      "iteration 900 / 11340: loss 1.591561\n",
      "epoch done... acc 0.416\n",
      "iteration 1000 / 11340: loss 1.779716\n",
      "iteration 1100 / 11340: loss 1.630175\n",
      "iteration 1200 / 11340: loss 1.596216\n",
      "iteration 1300 / 11340: loss 1.663376\n",
      "iteration 1400 / 11340: loss 1.508504\n",
      "epoch done... acc 0.45\n",
      "iteration 1500 / 11340: loss 1.560771\n",
      "iteration 1600 / 11340: loss 1.590407\n",
      "iteration 1700 / 11340: loss 1.573910\n",
      "iteration 1800 / 11340: loss 1.705631\n",
      "iteration 1900 / 11340: loss 1.492336\n",
      "epoch done... acc 0.468\n",
      "iteration 2000 / 11340: loss 1.565043\n",
      "iteration 2100 / 11340: loss 1.493589\n",
      "iteration 2200 / 11340: loss 1.541802\n",
      "iteration 2300 / 11340: loss 1.389010\n",
      "iteration 2400 / 11340: loss 1.444027\n",
      "epoch done... acc 0.462\n",
      "iteration 2500 / 11340: loss 1.506746\n",
      "iteration 2600 / 11340: loss 1.315575\n",
      "iteration 2700 / 11340: loss 1.393468\n",
      "iteration 2800 / 11340: loss 1.584937\n",
      "iteration 2900 / 11340: loss 1.578135\n",
      "epoch done... acc 0.474\n",
      "iteration 3000 / 11340: loss 1.373160\n",
      "iteration 3100 / 11340: loss 1.495743\n",
      "iteration 3200 / 11340: loss 1.438422\n",
      "iteration 3300 / 11340: loss 1.432289\n",
      "iteration 3400 / 11340: loss 1.342695\n",
      "epoch done... acc 0.489\n",
      "iteration 3500 / 11340: loss 1.425321\n",
      "iteration 3600 / 11340: loss 1.205954\n",
      "iteration 3700 / 11340: loss 1.509023\n",
      "iteration 3800 / 11340: loss 1.506359\n",
      "iteration 3900 / 11340: loss 1.298607\n",
      "epoch done... acc 0.482\n",
      "iteration 4000 / 11340: loss 1.156312\n",
      "iteration 4100 / 11340: loss 1.434885\n",
      "iteration 4200 / 11340: loss 1.354864\n",
      "iteration 4300 / 11340: loss 1.496650\n",
      "iteration 4400 / 11340: loss 1.243890\n",
      "epoch done... acc 0.511\n",
      "iteration 4500 / 11340: loss 1.414934\n",
      "iteration 4600 / 11340: loss 1.233447\n",
      "iteration 4700 / 11340: loss 1.329318\n",
      "iteration 4800 / 11340: loss 1.240019\n",
      "iteration 4900 / 11340: loss 1.394970\n",
      "epoch done... acc 0.491\n",
      "iteration 5000 / 11340: loss 1.246967\n",
      "iteration 5100 / 11340: loss 1.414968\n",
      "iteration 5200 / 11340: loss 1.432282\n",
      "iteration 5300 / 11340: loss 1.263665\n",
      "epoch done... acc 0.499\n",
      "iteration 5400 / 11340: loss 1.265248\n",
      "iteration 5500 / 11340: loss 1.178339\n",
      "iteration 5600 / 11340: loss 1.175643\n",
      "iteration 5700 / 11340: loss 1.313044\n",
      "iteration 5800 / 11340: loss 1.026091\n",
      "epoch done... acc 0.501\n",
      "iteration 5900 / 11340: loss 1.443475\n",
      "iteration 6000 / 11340: loss 1.226340\n",
      "iteration 6100 / 11340: loss 1.308358\n",
      "iteration 6200 / 11340: loss 1.147958\n",
      "iteration 6300 / 11340: loss 1.115567\n",
      "epoch done... acc 0.518\n",
      "iteration 6400 / 11340: loss 1.053656\n",
      "iteration 6500 / 11340: loss 1.257876\n",
      "iteration 6600 / 11340: loss 1.311993\n",
      "iteration 6700 / 11340: loss 1.187878\n",
      "iteration 6800 / 11340: loss 1.139103\n",
      "epoch done... acc 0.508\n",
      "iteration 6900 / 11340: loss 1.040467\n",
      "iteration 7000 / 11340: loss 1.072442\n",
      "iteration 7100 / 11340: loss 1.203364\n",
      "iteration 7200 / 11340: loss 1.111016\n",
      "iteration 7300 / 11340: loss 1.305619\n",
      "epoch done... acc 0.513\n",
      "iteration 7400 / 11340: loss 1.231034\n",
      "iteration 7500 / 11340: loss 1.236762\n",
      "iteration 7600 / 11340: loss 1.208055\n",
      "iteration 7700 / 11340: loss 1.201649\n",
      "iteration 7800 / 11340: loss 1.193061\n",
      "epoch done... acc 0.496\n",
      "iteration 7900 / 11340: loss 1.342106\n",
      "iteration 8000 / 11340: loss 1.146550\n",
      "iteration 8100 / 11340: loss 1.164383\n",
      "iteration 8200 / 11340: loss 1.177635\n",
      "iteration 8300 / 11340: loss 1.028501\n",
      "epoch done... acc 0.509\n",
      "iteration 8400 / 11340: loss 1.257054\n",
      "iteration 8500 / 11340: loss 1.270315\n",
      "iteration 8600 / 11340: loss 1.206397\n",
      "iteration 8700 / 11340: loss 0.954326\n",
      "iteration 8800 / 11340: loss 0.966786\n",
      "epoch done... acc 0.52\n",
      "iteration 8900 / 11340: loss 1.324802\n",
      "iteration 9000 / 11340: loss 1.259815\n",
      "iteration 9100 / 11340: loss 1.188449\n",
      "iteration 9200 / 11340: loss 1.044139\n",
      "iteration 9300 / 11340: loss 1.139097\n",
      "epoch done... acc 0.519\n",
      "iteration 9400 / 11340: loss 1.269081\n",
      "iteration 9500 / 11340: loss 1.149323\n",
      "iteration 9600 / 11340: loss 1.137253\n",
      "iteration 9700 / 11340: loss 1.211285\n",
      "iteration 9800 / 11340: loss 1.063702\n",
      "epoch done... acc 0.512\n",
      "iteration 9900 / 11340: loss 1.020602\n",
      "iteration 10000 / 11340: loss 1.140854\n",
      "iteration 10100 / 11340: loss 1.188555\n",
      "iteration 10200 / 11340: loss 1.176155\n",
      "epoch done... acc 0.537\n",
      "iteration 10300 / 11340: loss 1.268744\n",
      "iteration 10400 / 11340: loss 1.110964\n",
      "iteration 10500 / 11340: loss 1.094025\n",
      "iteration 10600 / 11340: loss 1.177916\n",
      "iteration 10700 / 11340: loss 0.943515\n",
      "epoch done... acc 0.51\n",
      "iteration 10800 / 11340: loss 1.093219\n",
      "iteration 10900 / 11340: loss 1.156952\n",
      "iteration 11000 / 11340: loss 1.008476\n",
      "iteration 11100 / 11340: loss 1.162633\n",
      "iteration 11200 / 11340: loss 1.033759\n",
      "epoch done... acc 0.518\n",
      "iteration 11300 / 11340: loss 1.061787\n",
      "Final training loss:  1.1251436088027518\n",
      "Final validation loss:  1.3697842532551563\n",
      "Final validation accuracy:  0.518\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "49 2 1 385 12600 100 0.001 0.98 0.518\n",
      "iteration 0 / 12600: loss 2.302474\n",
      "epoch done... acc 0.139\n",
      "iteration 100 / 12600: loss 1.956444\n",
      "iteration 200 / 12600: loss 1.916742\n",
      "iteration 300 / 12600: loss 1.856130\n",
      "iteration 400 / 12600: loss 1.822380\n",
      "epoch done... acc 0.395\n",
      "iteration 500 / 12600: loss 1.795108\n",
      "iteration 600 / 12600: loss 1.582621\n",
      "iteration 700 / 12600: loss 1.653281\n",
      "iteration 800 / 12600: loss 1.749339\n",
      "iteration 900 / 12600: loss 1.743908\n",
      "epoch done... acc 0.424\n",
      "iteration 1000 / 12600: loss 1.485383\n",
      "iteration 1100 / 12600: loss 1.602816\n",
      "iteration 1200 / 12600: loss 1.606153\n",
      "iteration 1300 / 12600: loss 1.546554\n",
      "iteration 1400 / 12600: loss 1.556404\n",
      "epoch done... acc 0.443\n",
      "iteration 1500 / 12600: loss 1.497851\n",
      "iteration 1600 / 12600: loss 1.767899\n",
      "iteration 1700 / 12600: loss 1.463827\n",
      "iteration 1800 / 12600: loss 1.633815\n",
      "iteration 1900 / 12600: loss 1.453111\n",
      "epoch done... acc 0.459\n",
      "iteration 2000 / 12600: loss 1.489763\n",
      "iteration 2100 / 12600: loss 1.439769\n",
      "iteration 2200 / 12600: loss 1.385453\n",
      "iteration 2300 / 12600: loss 1.446300\n",
      "iteration 2400 / 12600: loss 1.422449\n",
      "epoch done... acc 0.474\n",
      "iteration 2500 / 12600: loss 1.571944\n",
      "iteration 2600 / 12600: loss 1.375638\n",
      "iteration 2700 / 12600: loss 1.358173\n",
      "iteration 2800 / 12600: loss 1.511155\n",
      "iteration 2900 / 12600: loss 1.293272\n",
      "epoch done... acc 0.475\n",
      "iteration 3000 / 12600: loss 1.553035\n",
      "iteration 3100 / 12600: loss 1.181981\n",
      "iteration 3200 / 12600: loss 1.402526\n",
      "iteration 3300 / 12600: loss 1.693901\n",
      "iteration 3400 / 12600: loss 1.411533\n",
      "epoch done... acc 0.471\n",
      "iteration 3500 / 12600: loss 1.460641\n",
      "iteration 3600 / 12600: loss 1.484352\n",
      "iteration 3700 / 12600: loss 1.266727\n",
      "iteration 3800 / 12600: loss 1.429427\n",
      "iteration 3900 / 12600: loss 1.434225\n",
      "epoch done... acc 0.473\n",
      "iteration 4000 / 12600: loss 1.292176\n",
      "iteration 4100 / 12600: loss 1.607846\n",
      "iteration 4200 / 12600: loss 1.251133\n",
      "iteration 4300 / 12600: loss 1.529015\n",
      "iteration 4400 / 12600: loss 1.198751\n",
      "epoch done... acc 0.492\n",
      "iteration 4500 / 12600: loss 1.306241\n",
      "iteration 4600 / 12600: loss 1.367008\n",
      "iteration 4700 / 12600: loss 1.283969\n",
      "iteration 4800 / 12600: loss 1.257908\n",
      "iteration 4900 / 12600: loss 1.140347\n",
      "epoch done... acc 0.509\n",
      "iteration 5000 / 12600: loss 1.447071\n",
      "iteration 5100 / 12600: loss 1.432696\n",
      "iteration 5200 / 12600: loss 1.272059\n",
      "iteration 5300 / 12600: loss 1.294402\n",
      "epoch done... acc 0.51\n",
      "iteration 5400 / 12600: loss 1.192007\n",
      "iteration 5500 / 12600: loss 1.311936\n",
      "iteration 5600 / 12600: loss 1.426388\n",
      "iteration 5700 / 12600: loss 1.359018\n",
      "iteration 5800 / 12600: loss 1.134813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch done... acc 0.512\n",
      "iteration 5900 / 12600: loss 1.265882\n",
      "iteration 6000 / 12600: loss 1.168758\n",
      "iteration 6100 / 12600: loss 1.281523\n",
      "iteration 6200 / 12600: loss 1.212754\n",
      "iteration 6300 / 12600: loss 1.285018\n",
      "epoch done... acc 0.501\n",
      "iteration 6400 / 12600: loss 1.055537\n",
      "iteration 6500 / 12600: loss 1.184055\n",
      "iteration 6600 / 12600: loss 1.052736\n",
      "iteration 6700 / 12600: loss 1.153286\n",
      "iteration 6800 / 12600: loss 1.110750\n",
      "epoch done... acc 0.499\n",
      "iteration 6900 / 12600: loss 1.233372\n",
      "iteration 7000 / 12600: loss 1.268288\n",
      "iteration 7100 / 12600: loss 1.349617\n",
      "iteration 7200 / 12600: loss 1.230195\n",
      "iteration 7300 / 12600: loss 1.361047\n",
      "epoch done... acc 0.514\n",
      "iteration 7400 / 12600: loss 1.117400\n",
      "iteration 7500 / 12600: loss 1.042235\n",
      "iteration 7600 / 12600: loss 1.136023\n",
      "iteration 7700 / 12600: loss 1.301866\n",
      "iteration 7800 / 12600: loss 1.287823\n",
      "epoch done... acc 0.52\n",
      "iteration 7900 / 12600: loss 1.254248\n",
      "iteration 8000 / 12600: loss 1.215083\n",
      "iteration 8100 / 12600: loss 1.141958\n",
      "iteration 8200 / 12600: loss 1.161174\n",
      "iteration 8300 / 12600: loss 1.120031\n",
      "epoch done... acc 0.518\n",
      "iteration 8400 / 12600: loss 1.304133\n",
      "iteration 8500 / 12600: loss 1.070143\n",
      "iteration 8600 / 12600: loss 1.283934\n",
      "iteration 8700 / 12600: loss 1.194666\n",
      "iteration 8800 / 12600: loss 1.212964\n",
      "epoch done... acc 0.507\n",
      "iteration 8900 / 12600: loss 1.008001\n",
      "iteration 9000 / 12600: loss 1.118554\n",
      "iteration 9100 / 12600: loss 1.078776\n",
      "iteration 9200 / 12600: loss 1.218280\n",
      "iteration 9300 / 12600: loss 1.386944\n",
      "epoch done... acc 0.517\n",
      "iteration 9400 / 12600: loss 1.034440\n",
      "iteration 9500 / 12600: loss 1.343213\n",
      "iteration 9600 / 12600: loss 1.023409\n",
      "iteration 9700 / 12600: loss 1.081856\n",
      "iteration 9800 / 12600: loss 1.089754\n",
      "epoch done... acc 0.522\n",
      "iteration 9900 / 12600: loss 1.058807\n",
      "iteration 10000 / 12600: loss 1.091249\n",
      "iteration 10100 / 12600: loss 1.136904\n",
      "iteration 10200 / 12600: loss 1.301704\n",
      "epoch done... acc 0.513\n",
      "iteration 10300 / 12600: loss 1.055954\n",
      "iteration 10400 / 12600: loss 1.053071\n",
      "iteration 10500 / 12600: loss 0.967327\n",
      "iteration 10600 / 12600: loss 1.160996\n",
      "iteration 10700 / 12600: loss 1.156458\n",
      "epoch done... acc 0.519\n",
      "iteration 10800 / 12600: loss 1.088044\n",
      "iteration 10900 / 12600: loss 1.162225\n",
      "iteration 11000 / 12600: loss 1.052822\n",
      "iteration 11100 / 12600: loss 1.125327\n",
      "iteration 11200 / 12600: loss 0.959792\n",
      "epoch done... acc 0.529\n",
      "iteration 11300 / 12600: loss 1.265315\n",
      "iteration 11400 / 12600: loss 1.011571\n",
      "iteration 11500 / 12600: loss 1.000567\n",
      "iteration 11600 / 12600: loss 1.012536\n",
      "iteration 11700 / 12600: loss 0.922670\n",
      "epoch done... acc 0.52\n",
      "iteration 11800 / 12600: loss 1.090307\n",
      "iteration 11900 / 12600: loss 1.056414\n",
      "iteration 12000 / 12600: loss 1.112646\n",
      "iteration 12100 / 12600: loss 0.945449\n",
      "iteration 12200 / 12600: loss 1.119335\n",
      "epoch done... acc 0.511\n",
      "iteration 12300 / 12600: loss 1.254112\n",
      "iteration 12400 / 12600: loss 0.997957\n",
      "iteration 12500 / 12600: loss 0.986013\n",
      "Final training loss:  1.0467359610944231\n",
      "Final validation loss:  1.3516782224613408\n",
      "Final validation accuracy:  0.511\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "50 1 0 385 12600 100 0.001 0.98 0.511\n",
      "iteration 0 / 12600: loss 2.302588\n",
      "epoch done... acc 0.148\n",
      "iteration 100 / 12600: loss 1.890339\n",
      "iteration 200 / 12600: loss 1.810906\n",
      "iteration 300 / 12600: loss 1.778188\n",
      "iteration 400 / 12600: loss 1.910107\n",
      "epoch done... acc 0.39\n",
      "iteration 500 / 12600: loss 1.701600\n",
      "iteration 600 / 12600: loss 1.712332\n",
      "iteration 700 / 12600: loss 1.634837\n",
      "iteration 800 / 12600: loss 1.732676\n",
      "iteration 900 / 12600: loss 1.595513\n",
      "epoch done... acc 0.418\n",
      "iteration 1000 / 12600: loss 1.603250\n",
      "iteration 1100 / 12600: loss 1.657675\n",
      "iteration 1200 / 12600: loss 1.675586\n",
      "iteration 1300 / 12600: loss 1.570501\n",
      "iteration 1400 / 12600: loss 1.266597\n",
      "epoch done... acc 0.437\n",
      "iteration 1500 / 12600: loss 1.531757\n",
      "iteration 1600 / 12600: loss 1.668430\n",
      "iteration 1700 / 12600: loss 1.405535\n",
      "iteration 1800 / 12600: loss 1.484958\n",
      "iteration 1900 / 12600: loss 1.639547\n",
      "epoch done... acc 0.457\n",
      "iteration 2000 / 12600: loss 1.651398\n",
      "iteration 2100 / 12600: loss 1.397873\n",
      "iteration 2200 / 12600: loss 1.492363\n",
      "iteration 2300 / 12600: loss 1.458862\n",
      "iteration 2400 / 12600: loss 1.486186\n",
      "epoch done... acc 0.457\n",
      "iteration 2500 / 12600: loss 1.327625\n",
      "iteration 2600 / 12600: loss 1.370702\n",
      "iteration 2700 / 12600: loss 1.405666\n",
      "iteration 2800 / 12600: loss 1.572233\n",
      "iteration 2900 / 12600: loss 1.523608\n",
      "epoch done... acc 0.461\n",
      "iteration 3000 / 12600: loss 1.503288\n",
      "iteration 3100 / 12600: loss 1.441924\n",
      "iteration 3200 / 12600: loss 1.364226\n",
      "iteration 3300 / 12600: loss 1.441004\n",
      "iteration 3400 / 12600: loss 1.313777\n",
      "epoch done... acc 0.476\n",
      "iteration 3500 / 12600: loss 1.147126\n",
      "iteration 3600 / 12600: loss 1.269932\n",
      "iteration 3700 / 12600: loss 1.409586\n",
      "iteration 3800 / 12600: loss 1.325388\n",
      "iteration 3900 / 12600: loss 1.301416\n",
      "epoch done... acc 0.484\n",
      "iteration 4000 / 12600: loss 1.532424\n",
      "iteration 4100 / 12600: loss 1.286575\n",
      "iteration 4200 / 12600: loss 1.478027\n",
      "iteration 4300 / 12600: loss 1.338285\n",
      "iteration 4400 / 12600: loss 1.250698\n",
      "epoch done... acc 0.484\n",
      "iteration 4500 / 12600: loss 1.222263\n",
      "iteration 4600 / 12600: loss 1.358916\n",
      "iteration 4700 / 12600: loss 1.304985\n",
      "iteration 4800 / 12600: loss 1.382455\n",
      "iteration 4900 / 12600: loss 1.403815\n",
      "epoch done... acc 0.49\n",
      "iteration 5000 / 12600: loss 1.394963\n",
      "iteration 5100 / 12600: loss 1.333718\n",
      "iteration 5200 / 12600: loss 1.156848\n",
      "iteration 5300 / 12600: loss 1.249225\n",
      "epoch done... acc 0.498\n",
      "iteration 5400 / 12600: loss 1.212504\n",
      "iteration 5500 / 12600: loss 1.345099\n",
      "iteration 5600 / 12600: loss 1.310638\n",
      "iteration 5700 / 12600: loss 1.349742\n",
      "iteration 5800 / 12600: loss 1.222260\n",
      "epoch done... acc 0.495\n",
      "iteration 5900 / 12600: loss 1.492401\n",
      "iteration 6000 / 12600: loss 1.184657\n",
      "iteration 6100 / 12600: loss 1.240578\n",
      "iteration 6200 / 12600: loss 1.448198\n",
      "iteration 6300 / 12600: loss 1.346523\n",
      "epoch done... acc 0.512\n",
      "iteration 6400 / 12600: loss 1.323159\n",
      "iteration 6500 / 12600: loss 1.043911\n",
      "iteration 6600 / 12600: loss 1.289902\n",
      "iteration 6700 / 12600: loss 1.167306\n",
      "iteration 6800 / 12600: loss 1.274034\n",
      "epoch done... acc 0.505\n",
      "iteration 6900 / 12600: loss 1.260536\n",
      "iteration 7000 / 12600: loss 1.326152\n",
      "iteration 7100 / 12600: loss 1.164551\n",
      "iteration 7200 / 12600: loss 1.344213\n",
      "iteration 7300 / 12600: loss 1.177173\n",
      "epoch done... acc 0.513\n",
      "iteration 7400 / 12600: loss 1.045166\n",
      "iteration 7500 / 12600: loss 1.204437\n",
      "iteration 7600 / 12600: loss 1.138508\n",
      "iteration 7700 / 12600: loss 1.164169\n",
      "iteration 7800 / 12600: loss 1.179668\n",
      "epoch done... acc 0.495\n",
      "iteration 7900 / 12600: loss 1.187514\n",
      "iteration 8000 / 12600: loss 1.343008\n",
      "iteration 8100 / 12600: loss 1.089060\n",
      "iteration 8200 / 12600: loss 1.262625\n",
      "iteration 8300 / 12600: loss 1.283694\n",
      "epoch done... acc 0.51\n",
      "iteration 8400 / 12600: loss 1.169664\n",
      "iteration 8500 / 12600: loss 1.254809\n",
      "iteration 8600 / 12600: loss 1.260879\n",
      "iteration 8700 / 12600: loss 1.213535\n",
      "iteration 8800 / 12600: loss 1.231954\n",
      "epoch done... acc 0.509\n",
      "iteration 8900 / 12600: loss 1.104480\n",
      "iteration 9000 / 12600: loss 1.225912\n",
      "iteration 9100 / 12600: loss 1.130950\n",
      "iteration 9200 / 12600: loss 1.352202\n",
      "iteration 9300 / 12600: loss 1.127066\n",
      "epoch done... acc 0.522\n",
      "iteration 9400 / 12600: loss 1.177216\n",
      "iteration 9500 / 12600: loss 1.355075\n",
      "iteration 9600 / 12600: loss 1.197341\n",
      "iteration 9700 / 12600: loss 1.258506\n",
      "iteration 9800 / 12600: loss 1.144308\n",
      "epoch done... acc 0.522\n",
      "iteration 9900 / 12600: loss 1.180111\n",
      "iteration 10000 / 12600: loss 1.034367\n",
      "iteration 10100 / 12600: loss 1.073989\n",
      "iteration 10200 / 12600: loss 1.031627\n",
      "epoch done... acc 0.539\n",
      "iteration 10300 / 12600: loss 1.123509\n",
      "iteration 10400 / 12600: loss 1.055901\n",
      "iteration 10500 / 12600: loss 1.028906\n",
      "iteration 10600 / 12600: loss 1.260938\n",
      "iteration 10700 / 12600: loss 1.000923\n",
      "epoch done... acc 0.525\n",
      "iteration 10800 / 12600: loss 1.073391\n",
      "iteration 10900 / 12600: loss 1.150849\n",
      "iteration 11000 / 12600: loss 1.209903\n",
      "iteration 11100 / 12600: loss 0.901642\n",
      "iteration 11200 / 12600: loss 1.024324\n",
      "epoch done... acc 0.526\n",
      "iteration 11300 / 12600: loss 1.084864\n",
      "iteration 11400 / 12600: loss 1.115033\n",
      "iteration 11500 / 12600: loss 1.191154\n",
      "iteration 11600 / 12600: loss 1.051348\n",
      "iteration 11700 / 12600: loss 1.118944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch done... acc 0.528\n",
      "iteration 11800 / 12600: loss 1.036045\n",
      "iteration 11900 / 12600: loss 1.322105\n",
      "iteration 12000 / 12600: loss 1.050015\n",
      "iteration 12100 / 12600: loss 1.125839\n",
      "iteration 12200 / 12600: loss 1.229860\n",
      "epoch done... acc 0.533\n",
      "iteration 12300 / 12600: loss 1.293334\n",
      "iteration 12400 / 12600: loss 1.113507\n",
      "iteration 12500 / 12600: loss 1.041270\n",
      "Final training loss:  0.9896572322954664\n",
      "Final validation loss:  1.3643299222894798\n",
      "Final validation accuracy:  0.533\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "51 2 0 385 12600 100 0.001 0.98 0.533\n",
      "iteration 0 / 13860: loss 2.302561\n",
      "epoch done... acc 0.121\n",
      "iteration 100 / 13860: loss 1.980355\n",
      "iteration 200 / 13860: loss 1.908808\n",
      "iteration 300 / 13860: loss 1.790562\n",
      "iteration 400 / 13860: loss 1.922582\n",
      "epoch done... acc 0.38\n",
      "iteration 500 / 13860: loss 1.734671\n",
      "iteration 600 / 13860: loss 1.611841\n",
      "iteration 700 / 13860: loss 1.684742\n",
      "iteration 800 / 13860: loss 1.610255\n",
      "iteration 900 / 13860: loss 1.749736\n",
      "epoch done... acc 0.419\n",
      "iteration 1000 / 13860: loss 1.805630\n",
      "iteration 1100 / 13860: loss 1.547463\n",
      "iteration 1200 / 13860: loss 1.671440\n",
      "iteration 1300 / 13860: loss 1.584856\n",
      "iteration 1400 / 13860: loss 1.778255\n",
      "epoch done... acc 0.44\n",
      "iteration 1500 / 13860: loss 1.407590\n",
      "iteration 1600 / 13860: loss 1.272432\n",
      "iteration 1700 / 13860: loss 1.672319\n",
      "iteration 1800 / 13860: loss 1.602146\n",
      "iteration 1900 / 13860: loss 1.476787\n",
      "epoch done... acc 0.457\n",
      "iteration 2000 / 13860: loss 1.761106\n",
      "iteration 2100 / 13860: loss 1.333884\n",
      "iteration 2200 / 13860: loss 1.475531\n",
      "iteration 2300 / 13860: loss 1.506824\n",
      "iteration 2400 / 13860: loss 1.728530\n",
      "epoch done... acc 0.454\n",
      "iteration 2500 / 13860: loss 1.306768\n",
      "iteration 2600 / 13860: loss 1.437378\n",
      "iteration 2700 / 13860: loss 1.380893\n",
      "iteration 2800 / 13860: loss 1.456985\n",
      "iteration 2900 / 13860: loss 1.460653\n",
      "epoch done... acc 0.449\n",
      "iteration 3000 / 13860: loss 1.579916\n",
      "iteration 3100 / 13860: loss 1.501560\n",
      "iteration 3200 / 13860: loss 1.355882\n",
      "iteration 3300 / 13860: loss 1.382516\n",
      "iteration 3400 / 13860: loss 1.291797\n",
      "epoch done... acc 0.472\n",
      "iteration 3500 / 13860: loss 1.358702\n",
      "iteration 3600 / 13860: loss 1.281214\n",
      "iteration 3700 / 13860: loss 1.529699\n",
      "iteration 3800 / 13860: loss 1.224219\n",
      "iteration 3900 / 13860: loss 1.475183\n",
      "epoch done... acc 0.497\n",
      "iteration 4000 / 13860: loss 1.442825\n",
      "iteration 4100 / 13860: loss 1.202440\n",
      "iteration 4200 / 13860: loss 1.433495\n",
      "iteration 4300 / 13860: loss 1.565974\n",
      "iteration 4400 / 13860: loss 1.306342\n",
      "epoch done... acc 0.487\n",
      "iteration 4500 / 13860: loss 1.369994\n",
      "iteration 4600 / 13860: loss 1.387646\n",
      "iteration 4700 / 13860: loss 1.177970\n",
      "iteration 4800 / 13860: loss 1.258129\n",
      "iteration 4900 / 13860: loss 1.294895\n",
      "epoch done... acc 0.488\n",
      "iteration 5000 / 13860: loss 1.370781\n",
      "iteration 5100 / 13860: loss 1.138689\n",
      "iteration 5200 / 13860: loss 1.250423\n",
      "iteration 5300 / 13860: loss 1.342204\n",
      "epoch done... acc 0.495\n",
      "iteration 5400 / 13860: loss 1.300372\n",
      "iteration 5500 / 13860: loss 1.163373\n",
      "iteration 5600 / 13860: loss 1.328397\n",
      "iteration 5700 / 13860: loss 1.340389\n",
      "iteration 5800 / 13860: loss 1.340970\n",
      "epoch done... acc 0.526\n",
      "iteration 5900 / 13860: loss 1.157801\n",
      "iteration 6000 / 13860: loss 1.331173\n",
      "iteration 6100 / 13860: loss 1.208023\n",
      "iteration 6200 / 13860: loss 1.272244\n",
      "iteration 6300 / 13860: loss 1.179330\n",
      "epoch done... acc 0.496\n",
      "iteration 6400 / 13860: loss 1.297397\n",
      "iteration 6500 / 13860: loss 1.209006\n",
      "iteration 6600 / 13860: loss 1.138659\n",
      "iteration 6700 / 13860: loss 1.242315\n",
      "iteration 6800 / 13860: loss 1.422721\n",
      "epoch done... acc 0.506\n",
      "iteration 6900 / 13860: loss 1.222939\n",
      "iteration 7000 / 13860: loss 1.326636\n",
      "iteration 7100 / 13860: loss 1.138776\n",
      "iteration 7200 / 13860: loss 1.233460\n",
      "iteration 7300 / 13860: loss 1.167050\n",
      "epoch done... acc 0.505\n",
      "iteration 7400 / 13860: loss 1.397465\n",
      "iteration 7500 / 13860: loss 1.377952\n",
      "iteration 7600 / 13860: loss 1.201594\n",
      "iteration 7700 / 13860: loss 1.410704\n",
      "iteration 7800 / 13860: loss 1.124519\n",
      "epoch done... acc 0.515\n",
      "iteration 7900 / 13860: loss 1.116279\n",
      "iteration 8000 / 13860: loss 1.278746\n",
      "iteration 8100 / 13860: loss 1.143569\n",
      "iteration 8200 / 13860: loss 1.342555\n",
      "iteration 8300 / 13860: loss 1.235186\n",
      "epoch done... acc 0.512\n",
      "iteration 8400 / 13860: loss 1.385409\n",
      "iteration 8500 / 13860: loss 1.296216\n",
      "iteration 8600 / 13860: loss 1.203638\n",
      "iteration 8700 / 13860: loss 1.089432\n",
      "iteration 8800 / 13860: loss 0.922554\n",
      "epoch done... acc 0.512\n",
      "iteration 8900 / 13860: loss 1.246543\n",
      "iteration 9000 / 13860: loss 1.235375\n",
      "iteration 9100 / 13860: loss 1.145302\n",
      "iteration 9200 / 13860: loss 1.156839\n",
      "iteration 9300 / 13860: loss 1.186641\n",
      "epoch done... acc 0.508\n",
      "iteration 9400 / 13860: loss 1.014713\n",
      "iteration 9500 / 13860: loss 1.141747\n",
      "iteration 9600 / 13860: loss 1.122643\n",
      "iteration 9700 / 13860: loss 1.369627\n",
      "iteration 9800 / 13860: loss 1.233332\n",
      "epoch done... acc 0.515\n",
      "iteration 9900 / 13860: loss 1.087947\n",
      "iteration 10000 / 13860: loss 1.301094\n",
      "iteration 10100 / 13860: loss 1.198349\n",
      "iteration 10200 / 13860: loss 1.046876\n",
      "epoch done... acc 0.515\n",
      "iteration 10300 / 13860: loss 1.088428\n",
      "iteration 10400 / 13860: loss 1.141912\n",
      "iteration 10500 / 13860: loss 1.179879\n",
      "iteration 10600 / 13860: loss 1.203179\n",
      "iteration 10700 / 13860: loss 1.061675\n",
      "epoch done... acc 0.51\n",
      "iteration 10800 / 13860: loss 1.101242\n",
      "iteration 10900 / 13860: loss 1.072741\n",
      "iteration 11000 / 13860: loss 1.182169\n",
      "iteration 11100 / 13860: loss 1.005928\n",
      "iteration 11200 / 13860: loss 1.082106\n",
      "epoch done... acc 0.521\n",
      "iteration 11300 / 13860: loss 1.036696\n",
      "iteration 11400 / 13860: loss 1.026577\n",
      "iteration 11500 / 13860: loss 1.086958\n",
      "iteration 11600 / 13860: loss 1.068742\n",
      "iteration 11700 / 13860: loss 1.086164\n",
      "epoch done... acc 0.515\n",
      "iteration 11800 / 13860: loss 1.023999\n",
      "iteration 11900 / 13860: loss 1.186526\n",
      "iteration 12000 / 13860: loss 1.180698\n",
      "iteration 12100 / 13860: loss 0.973421\n",
      "iteration 12200 / 13860: loss 0.989324\n",
      "epoch done... acc 0.508\n",
      "iteration 12300 / 13860: loss 1.283455\n",
      "iteration 12400 / 13860: loss 1.096409\n",
      "iteration 12500 / 13860: loss 0.943321\n",
      "iteration 12600 / 13860: loss 0.895758\n",
      "iteration 12700 / 13860: loss 1.002189\n",
      "epoch done... acc 0.53\n",
      "iteration 12800 / 13860: loss 0.979192\n",
      "iteration 12900 / 13860: loss 0.986077\n",
      "iteration 13000 / 13860: loss 1.023514\n",
      "iteration 13100 / 13860: loss 1.083816\n",
      "iteration 13200 / 13860: loss 1.117254\n",
      "epoch done... acc 0.53\n",
      "iteration 13300 / 13860: loss 1.000671\n",
      "iteration 13400 / 13860: loss 1.112053\n",
      "iteration 13500 / 13860: loss 0.902993\n",
      "iteration 13600 / 13860: loss 0.977326\n",
      "iteration 13700 / 13860: loss 1.227438\n",
      "epoch done... acc 0.532\n",
      "iteration 13800 / 13860: loss 0.961832\n",
      "Final training loss:  1.0973915511277648\n",
      "Final validation loss:  1.3330429259204528\n",
      "Final validation accuracy:  0.532\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "52 1 1 385 12600 100 0.001 0.98 0.532\n",
      "iteration 0 / 11340: loss 2.302587\n",
      "epoch done... acc 0.139\n",
      "iteration 100 / 11340: loss 2.023300\n",
      "iteration 200 / 11340: loss 1.817472\n",
      "iteration 300 / 11340: loss 1.730724\n",
      "iteration 400 / 11340: loss 1.722665\n",
      "epoch done... acc 0.387\n",
      "iteration 500 / 11340: loss 1.859469\n",
      "iteration 600 / 11340: loss 1.861993\n",
      "iteration 700 / 11340: loss 1.650018\n",
      "iteration 800 / 11340: loss 1.637276\n",
      "iteration 900 / 11340: loss 1.674621\n",
      "epoch done... acc 0.438\n",
      "iteration 1000 / 11340: loss 1.707929\n",
      "iteration 1100 / 11340: loss 1.637253\n",
      "iteration 1200 / 11340: loss 1.647854\n",
      "iteration 1300 / 11340: loss 1.550557\n",
      "iteration 1400 / 11340: loss 1.665005\n",
      "epoch done... acc 0.447\n",
      "iteration 1500 / 11340: loss 1.646266\n",
      "iteration 1600 / 11340: loss 1.587356\n",
      "iteration 1700 / 11340: loss 1.483959\n",
      "iteration 1800 / 11340: loss 1.513631\n",
      "iteration 1900 / 11340: loss 1.450471\n",
      "epoch done... acc 0.47\n",
      "iteration 2000 / 11340: loss 1.314155\n",
      "iteration 2100 / 11340: loss 1.411971\n",
      "iteration 2200 / 11340: loss 1.470357\n",
      "iteration 2300 / 11340: loss 1.474004\n",
      "iteration 2400 / 11340: loss 1.468534\n",
      "epoch done... acc 0.46\n",
      "iteration 2500 / 11340: loss 1.477667\n",
      "iteration 2600 / 11340: loss 1.622790\n",
      "iteration 2700 / 11340: loss 1.369856\n",
      "iteration 2800 / 11340: loss 1.434024\n",
      "iteration 2900 / 11340: loss 1.416632\n",
      "epoch done... acc 0.474\n",
      "iteration 3000 / 11340: loss 1.460332\n",
      "iteration 3100 / 11340: loss 1.318112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3200 / 11340: loss 1.500837\n",
      "iteration 3300 / 11340: loss 1.380664\n",
      "iteration 3400 / 11340: loss 1.398656\n",
      "epoch done... acc 0.488\n",
      "iteration 3500 / 11340: loss 1.556715\n",
      "iteration 3600 / 11340: loss 1.444564\n",
      "iteration 3700 / 11340: loss 1.411511\n",
      "iteration 3800 / 11340: loss 1.468873\n",
      "iteration 3900 / 11340: loss 1.447265\n",
      "epoch done... acc 0.49\n",
      "iteration 4000 / 11340: loss 1.407412\n",
      "iteration 4100 / 11340: loss 1.497162\n",
      "iteration 4200 / 11340: loss 1.354791\n",
      "iteration 4300 / 11340: loss 1.275202\n",
      "iteration 4400 / 11340: loss 1.337338\n",
      "epoch done... acc 0.475\n",
      "iteration 4500 / 11340: loss 1.458754\n",
      "iteration 4600 / 11340: loss 1.028566\n",
      "iteration 4700 / 11340: loss 1.377307\n",
      "iteration 4800 / 11340: loss 1.352789\n",
      "iteration 4900 / 11340: loss 1.233918\n",
      "epoch done... acc 0.492\n",
      "iteration 5000 / 11340: loss 1.479297\n",
      "iteration 5100 / 11340: loss 1.455366\n",
      "iteration 5200 / 11340: loss 1.161313\n",
      "iteration 5300 / 11340: loss 1.356052\n",
      "epoch done... acc 0.522\n",
      "iteration 5400 / 11340: loss 1.366453\n",
      "iteration 5500 / 11340: loss 1.235585\n",
      "iteration 5600 / 11340: loss 1.237703\n",
      "iteration 5700 / 11340: loss 1.292220\n",
      "iteration 5800 / 11340: loss 1.359297\n",
      "epoch done... acc 0.51\n",
      "iteration 5900 / 11340: loss 1.404041\n",
      "iteration 6000 / 11340: loss 1.272327\n",
      "iteration 6100 / 11340: loss 1.293802\n",
      "iteration 6200 / 11340: loss 1.177187\n",
      "iteration 6300 / 11340: loss 1.021767\n",
      "epoch done... acc 0.524\n",
      "iteration 6400 / 11340: loss 1.123174\n",
      "iteration 6500 / 11340: loss 1.206096\n",
      "iteration 6600 / 11340: loss 1.253895\n",
      "iteration 6700 / 11340: loss 1.329374\n",
      "iteration 6800 / 11340: loss 1.318845\n",
      "epoch done... acc 0.506\n",
      "iteration 6900 / 11340: loss 1.148510\n",
      "iteration 7000 / 11340: loss 1.380284\n",
      "iteration 7100 / 11340: loss 1.323795\n",
      "iteration 7200 / 11340: loss 1.395378\n",
      "iteration 7300 / 11340: loss 1.247790\n",
      "epoch done... acc 0.505\n",
      "iteration 7400 / 11340: loss 1.189152\n",
      "iteration 7500 / 11340: loss 1.157820\n",
      "iteration 7600 / 11340: loss 1.419501\n",
      "iteration 7700 / 11340: loss 1.095177\n",
      "iteration 7800 / 11340: loss 1.423021\n",
      "epoch done... acc 0.517\n",
      "iteration 7900 / 11340: loss 1.191919\n",
      "iteration 8000 / 11340: loss 1.116784\n",
      "iteration 8100 / 11340: loss 1.046840\n",
      "iteration 8200 / 11340: loss 1.039261\n",
      "iteration 8300 / 11340: loss 1.202370\n",
      "epoch done... acc 0.521\n",
      "iteration 8400 / 11340: loss 1.229970\n",
      "iteration 8500 / 11340: loss 1.157321\n",
      "iteration 8600 / 11340: loss 1.137919\n",
      "iteration 8700 / 11340: loss 1.184929\n",
      "iteration 8800 / 11340: loss 1.150733\n",
      "epoch done... acc 0.531\n",
      "iteration 8900 / 11340: loss 1.264132\n",
      "iteration 9000 / 11340: loss 1.111435\n",
      "iteration 9100 / 11340: loss 1.006952\n",
      "iteration 9200 / 11340: loss 1.134614\n",
      "iteration 9300 / 11340: loss 1.079287\n",
      "epoch done... acc 0.506\n",
      "iteration 9400 / 11340: loss 1.055758\n",
      "iteration 9500 / 11340: loss 1.196582\n",
      "iteration 9600 / 11340: loss 1.004454\n",
      "iteration 9700 / 11340: loss 1.148206\n",
      "iteration 9800 / 11340: loss 1.084683\n",
      "epoch done... acc 0.516\n",
      "iteration 9900 / 11340: loss 1.232064\n",
      "iteration 10000 / 11340: loss 1.005581\n",
      "iteration 10100 / 11340: loss 1.195555\n",
      "iteration 10200 / 11340: loss 1.075872\n",
      "epoch done... acc 0.519\n",
      "iteration 10300 / 11340: loss 1.098047\n",
      "iteration 10400 / 11340: loss 1.387898\n",
      "iteration 10500 / 11340: loss 0.957442\n",
      "iteration 10600 / 11340: loss 1.087249\n",
      "iteration 10700 / 11340: loss 0.945327\n",
      "epoch done... acc 0.528\n",
      "iteration 10800 / 11340: loss 1.138848\n",
      "iteration 10900 / 11340: loss 1.118037\n",
      "iteration 11000 / 11340: loss 1.138034\n",
      "iteration 11100 / 11340: loss 1.259603\n",
      "iteration 11200 / 11340: loss 1.028037\n",
      "epoch done... acc 0.523\n",
      "iteration 11300 / 11340: loss 1.218703\n",
      "Final training loss:  1.1983395137876969\n",
      "Final validation loss:  1.3850126876524633\n",
      "Final validation accuracy:  0.523\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "53 2 1 385 12600 100 0.001 0.98 0.523\n",
      "iteration 0 / 12600: loss 2.302636\n",
      "epoch done... acc 0.16\n",
      "iteration 100 / 12600: loss 1.945760\n",
      "iteration 200 / 12600: loss 1.913236\n",
      "iteration 300 / 12600: loss 1.747787\n",
      "iteration 400 / 12600: loss 1.794024\n",
      "epoch done... acc 0.391\n",
      "iteration 500 / 12600: loss 1.889859\n",
      "iteration 600 / 12600: loss 1.829524\n",
      "iteration 700 / 12600: loss 1.637708\n",
      "iteration 800 / 12600: loss 1.760875\n",
      "iteration 900 / 12600: loss 1.817766\n",
      "epoch done... acc 0.432\n",
      "iteration 1000 / 12600: loss 1.538476\n",
      "iteration 1100 / 12600: loss 1.565512\n",
      "iteration 1200 / 12600: loss 1.729708\n",
      "iteration 1300 / 12600: loss 1.534930\n",
      "iteration 1400 / 12600: loss 1.535601\n",
      "epoch done... acc 0.451\n",
      "iteration 1500 / 12600: loss 1.530162\n",
      "iteration 1600 / 12600: loss 1.544873\n",
      "iteration 1700 / 12600: loss 1.584790\n",
      "iteration 1800 / 12600: loss 1.629705\n",
      "iteration 1900 / 12600: loss 1.589999\n",
      "epoch done... acc 0.454\n",
      "iteration 2000 / 12600: loss 1.539720\n",
      "iteration 2100 / 12600: loss 1.465943\n",
      "iteration 2200 / 12600: loss 1.468203\n",
      "iteration 2300 / 12600: loss 1.420125\n",
      "iteration 2400 / 12600: loss 1.470223\n",
      "epoch done... acc 0.463\n",
      "iteration 2500 / 12600: loss 1.343184\n",
      "iteration 2600 / 12600: loss 1.433279\n",
      "iteration 2700 / 12600: loss 1.537226\n",
      "iteration 2800 / 12600: loss 1.255265\n",
      "iteration 2900 / 12600: loss 1.337510\n",
      "epoch done... acc 0.461\n",
      "iteration 3000 / 12600: loss 1.381135\n",
      "iteration 3100 / 12600: loss 1.462311\n",
      "iteration 3200 / 12600: loss 1.440263\n",
      "iteration 3300 / 12600: loss 1.451244\n",
      "iteration 3400 / 12600: loss 1.448736\n",
      "epoch done... acc 0.483\n",
      "iteration 3500 / 12600: loss 1.526549\n",
      "iteration 3600 / 12600: loss 1.411052\n",
      "iteration 3700 / 12600: loss 1.159134\n",
      "iteration 3800 / 12600: loss 1.220628\n",
      "iteration 3900 / 12600: loss 1.352152\n",
      "epoch done... acc 0.475\n",
      "iteration 4000 / 12600: loss 1.331449\n",
      "iteration 4100 / 12600: loss 1.424515\n",
      "iteration 4200 / 12600: loss 1.227443\n",
      "iteration 4300 / 12600: loss 1.492676\n",
      "iteration 4400 / 12600: loss 1.365592\n",
      "epoch done... acc 0.488\n",
      "iteration 4500 / 12600: loss 1.285554\n",
      "iteration 4600 / 12600: loss 1.248749\n",
      "iteration 4700 / 12600: loss 1.115993\n",
      "iteration 4800 / 12600: loss 1.220367\n",
      "iteration 4900 / 12600: loss 1.377159\n",
      "epoch done... acc 0.508\n",
      "iteration 5000 / 12600: loss 1.153775\n",
      "iteration 5100 / 12600: loss 1.328913\n",
      "iteration 5200 / 12600: loss 1.269083\n",
      "iteration 5300 / 12600: loss 1.410787\n",
      "epoch done... acc 0.501\n",
      "iteration 5400 / 12600: loss 1.280512\n",
      "iteration 5500 / 12600: loss 1.331067\n",
      "iteration 5600 / 12600: loss 1.324799\n",
      "iteration 5700 / 12600: loss 1.019389\n",
      "iteration 5800 / 12600: loss 1.285695\n",
      "epoch done... acc 0.493\n",
      "iteration 5900 / 12600: loss 1.266454\n",
      "iteration 6000 / 12600: loss 1.325467\n",
      "iteration 6100 / 12600: loss 1.241696\n",
      "iteration 6200 / 12600: loss 1.171882\n",
      "iteration 6300 / 12600: loss 1.338585\n",
      "epoch done... acc 0.5\n",
      "iteration 6400 / 12600: loss 1.181253\n",
      "iteration 6500 / 12600: loss 1.291109\n",
      "iteration 6600 / 12600: loss 1.129053\n",
      "iteration 6700 / 12600: loss 1.085600\n",
      "iteration 6800 / 12600: loss 1.205108\n",
      "epoch done... acc 0.507\n",
      "iteration 6900 / 12600: loss 1.178555\n",
      "iteration 7000 / 12600: loss 1.119027\n",
      "iteration 7100 / 12600: loss 1.142790\n",
      "iteration 7200 / 12600: loss 1.149176\n",
      "iteration 7300 / 12600: loss 1.232372\n",
      "epoch done... acc 0.529\n",
      "iteration 7400 / 12600: loss 1.140021\n",
      "iteration 7500 / 12600: loss 1.180074\n",
      "iteration 7600 / 12600: loss 1.145368\n",
      "iteration 7700 / 12600: loss 1.098068\n",
      "iteration 7800 / 12600: loss 1.213939\n",
      "epoch done... acc 0.506\n",
      "iteration 7900 / 12600: loss 1.214274\n",
      "iteration 8000 / 12600: loss 1.174982\n",
      "iteration 8100 / 12600: loss 1.302811\n",
      "iteration 8200 / 12600: loss 1.340181\n",
      "iteration 8300 / 12600: loss 1.185080\n",
      "epoch done... acc 0.514\n",
      "iteration 8400 / 12600: loss 1.066963\n",
      "iteration 8500 / 12600: loss 1.124011\n",
      "iteration 8600 / 12600: loss 1.127205\n",
      "iteration 8700 / 12600: loss 1.179757\n",
      "iteration 8800 / 12600: loss 1.097822\n",
      "epoch done... acc 0.537\n",
      "iteration 8900 / 12600: loss 1.370802\n",
      "iteration 9000 / 12600: loss 1.165761\n",
      "iteration 9100 / 12600: loss 1.166602\n",
      "iteration 9200 / 12600: loss 1.194797\n",
      "iteration 9300 / 12600: loss 1.050020\n",
      "epoch done... acc 0.513\n",
      "iteration 9400 / 12600: loss 1.120405\n",
      "iteration 9500 / 12600: loss 1.122022\n",
      "iteration 9600 / 12600: loss 1.197426\n",
      "iteration 9700 / 12600: loss 1.037168\n",
      "iteration 9800 / 12600: loss 1.134204\n",
      "epoch done... acc 0.535\n",
      "iteration 9900 / 12600: loss 1.252606\n",
      "iteration 10000 / 12600: loss 1.130494\n",
      "iteration 10100 / 12600: loss 1.010962\n",
      "iteration 10200 / 12600: loss 0.985245\n",
      "epoch done... acc 0.538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10300 / 12600: loss 1.018107\n",
      "iteration 10400 / 12600: loss 1.057512\n",
      "iteration 10500 / 12600: loss 1.098847\n",
      "iteration 10600 / 12600: loss 1.341353\n",
      "iteration 10700 / 12600: loss 1.080432\n",
      "epoch done... acc 0.537\n",
      "iteration 10800 / 12600: loss 1.208865\n",
      "iteration 10900 / 12600: loss 1.213524\n",
      "iteration 11000 / 12600: loss 1.077472\n",
      "iteration 11100 / 12600: loss 1.040228\n",
      "iteration 11200 / 12600: loss 0.954911\n",
      "epoch done... acc 0.545\n",
      "iteration 11300 / 12600: loss 1.135411\n",
      "iteration 11400 / 12600: loss 1.035878\n",
      "iteration 11500 / 12600: loss 0.827911\n",
      "iteration 11600 / 12600: loss 1.055711\n",
      "iteration 11700 / 12600: loss 1.075023\n",
      "epoch done... acc 0.527\n",
      "iteration 11800 / 12600: loss 1.043408\n",
      "iteration 11900 / 12600: loss 1.138971\n",
      "iteration 12000 / 12600: loss 1.162260\n",
      "iteration 12100 / 12600: loss 1.022436\n",
      "iteration 12200 / 12600: loss 1.061163\n",
      "epoch done... acc 0.533\n",
      "iteration 12300 / 12600: loss 1.254811\n",
      "iteration 12400 / 12600: loss 0.929736\n",
      "iteration 12500 / 12600: loss 1.101983\n",
      "Final training loss:  0.8899556162539316\n",
      "Final validation loss:  1.3587820763662957\n",
      "Final validation accuracy:  0.533\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "54 1 0 385 12600 100 0.001 0.98 0.533\n",
      "iteration 0 / 12600: loss 2.302611\n",
      "epoch done... acc 0.154\n",
      "iteration 100 / 12600: loss 2.060749\n",
      "iteration 200 / 12600: loss 1.999299\n",
      "iteration 300 / 12600: loss 1.911323\n",
      "iteration 400 / 12600: loss 1.777849\n",
      "epoch done... acc 0.37\n",
      "iteration 500 / 12600: loss 1.813699\n",
      "iteration 600 / 12600: loss 1.839046\n",
      "iteration 700 / 12600: loss 1.594935\n",
      "iteration 800 / 12600: loss 1.684412\n",
      "iteration 900 / 12600: loss 1.515256\n",
      "epoch done... acc 0.426\n",
      "iteration 1000 / 12600: loss 1.697046\n",
      "iteration 1100 / 12600: loss 1.731271\n",
      "iteration 1200 / 12600: loss 1.571511\n",
      "iteration 1300 / 12600: loss 1.495347\n",
      "iteration 1400 / 12600: loss 1.631458\n",
      "epoch done... acc 0.443\n",
      "iteration 1500 / 12600: loss 1.536646\n",
      "iteration 1600 / 12600: loss 1.685859\n",
      "iteration 1700 / 12600: loss 1.621713\n",
      "iteration 1800 / 12600: loss 1.682648\n",
      "iteration 1900 / 12600: loss 1.501064\n",
      "epoch done... acc 0.451\n",
      "iteration 2000 / 12600: loss 1.549924\n",
      "iteration 2100 / 12600: loss 1.544126\n",
      "iteration 2200 / 12600: loss 1.502239\n",
      "iteration 2300 / 12600: loss 1.440288\n",
      "iteration 2400 / 12600: loss 1.443268\n",
      "epoch done... acc 0.457\n",
      "iteration 2500 / 12600: loss 1.453060\n",
      "iteration 2600 / 12600: loss 1.460936\n",
      "iteration 2700 / 12600: loss 1.523497\n",
      "iteration 2800 / 12600: loss 1.406263\n",
      "iteration 2900 / 12600: loss 1.405686\n",
      "epoch done... acc 0.469\n",
      "iteration 3000 / 12600: loss 1.442670\n",
      "iteration 3100 / 12600: loss 1.525349\n",
      "iteration 3200 / 12600: loss 1.385948\n",
      "iteration 3300 / 12600: loss 1.358443\n",
      "iteration 3400 / 12600: loss 1.374861\n",
      "epoch done... acc 0.483\n",
      "iteration 3500 / 12600: loss 1.252576\n",
      "iteration 3600 / 12600: loss 1.260733\n",
      "iteration 3700 / 12600: loss 1.460946\n",
      "iteration 3800 / 12600: loss 1.165542\n",
      "iteration 3900 / 12600: loss 1.194281\n",
      "epoch done... acc 0.496\n",
      "iteration 4000 / 12600: loss 1.361170\n",
      "iteration 4100 / 12600: loss 1.186551\n",
      "iteration 4200 / 12600: loss 1.424684\n",
      "iteration 4300 / 12600: loss 1.445706\n",
      "iteration 4400 / 12600: loss 1.423264\n",
      "epoch done... acc 0.49\n",
      "iteration 4500 / 12600: loss 1.268137\n",
      "iteration 4600 / 12600: loss 1.273361\n",
      "iteration 4700 / 12600: loss 1.154892\n",
      "iteration 4800 / 12600: loss 1.216785\n",
      "iteration 4900 / 12600: loss 1.426329\n",
      "epoch done... acc 0.493\n",
      "iteration 5000 / 12600: loss 1.110436\n",
      "iteration 5100 / 12600: loss 1.250656\n",
      "iteration 5200 / 12600: loss 1.322746\n",
      "iteration 5300 / 12600: loss 1.315661\n",
      "epoch done... acc 0.493\n",
      "iteration 5400 / 12600: loss 1.167222\n",
      "iteration 5500 / 12600: loss 1.288295\n",
      "iteration 5600 / 12600: loss 1.211643\n",
      "iteration 5700 / 12600: loss 1.529661\n",
      "iteration 5800 / 12600: loss 1.235845\n",
      "epoch done... acc 0.505\n",
      "iteration 5900 / 12600: loss 1.252483\n",
      "iteration 6000 / 12600: loss 1.575197\n",
      "iteration 6100 / 12600: loss 1.230218\n",
      "iteration 6200 / 12600: loss 1.117687\n",
      "iteration 6300 / 12600: loss 1.144038\n",
      "epoch done... acc 0.503\n",
      "iteration 6400 / 12600: loss 1.100030\n",
      "iteration 6500 / 12600: loss 1.352367\n",
      "iteration 6600 / 12600: loss 1.085802\n",
      "iteration 6700 / 12600: loss 1.376717\n",
      "iteration 6800 / 12600: loss 1.088040\n",
      "epoch done... acc 0.495\n",
      "iteration 6900 / 12600: loss 1.335313\n",
      "iteration 7000 / 12600: loss 1.087324\n",
      "iteration 7100 / 12600: loss 1.186182\n",
      "iteration 7200 / 12600: loss 1.284736\n",
      "iteration 7300 / 12600: loss 1.202611\n",
      "epoch done... acc 0.521\n",
      "iteration 7400 / 12600: loss 1.336623\n",
      "iteration 7500 / 12600: loss 1.044102\n",
      "iteration 7600 / 12600: loss 1.278193\n",
      "iteration 7700 / 12600: loss 1.178940\n",
      "iteration 7800 / 12600: loss 1.196231\n",
      "epoch done... acc 0.515\n",
      "iteration 7900 / 12600: loss 1.232024\n",
      "iteration 8000 / 12600: loss 1.443318\n",
      "iteration 8100 / 12600: loss 1.198149\n",
      "iteration 8200 / 12600: loss 1.340728\n",
      "iteration 8300 / 12600: loss 1.086814\n",
      "epoch done... acc 0.516\n",
      "iteration 8400 / 12600: loss 1.066542\n",
      "iteration 8500 / 12600: loss 1.079882\n",
      "iteration 8600 / 12600: loss 1.203850\n",
      "iteration 8700 / 12600: loss 0.988577\n",
      "iteration 8800 / 12600: loss 1.345684\n",
      "epoch done... acc 0.516\n",
      "iteration 8900 / 12600: loss 1.139518\n",
      "iteration 9000 / 12600: loss 1.127103\n",
      "iteration 9100 / 12600: loss 1.026131\n",
      "iteration 9200 / 12600: loss 0.967298\n",
      "iteration 9300 / 12600: loss 1.119339\n",
      "epoch done... acc 0.522\n",
      "iteration 9400 / 12600: loss 1.216976\n",
      "iteration 9500 / 12600: loss 1.234706\n",
      "iteration 9600 / 12600: loss 1.152611\n",
      "iteration 9700 / 12600: loss 1.184783\n",
      "iteration 9800 / 12600: loss 1.090132\n",
      "epoch done... acc 0.523\n",
      "iteration 9900 / 12600: loss 1.135353\n",
      "iteration 10000 / 12600: loss 1.258956\n",
      "iteration 10100 / 12600: loss 1.148701\n",
      "iteration 10200 / 12600: loss 0.932929\n",
      "epoch done... acc 0.525\n",
      "iteration 10300 / 12600: loss 1.059623\n",
      "iteration 10400 / 12600: loss 0.925459\n",
      "iteration 10500 / 12600: loss 1.006475\n",
      "iteration 10600 / 12600: loss 1.203024\n",
      "iteration 10700 / 12600: loss 1.206788\n",
      "epoch done... acc 0.506\n",
      "iteration 10800 / 12600: loss 1.155407\n",
      "iteration 10900 / 12600: loss 1.295069\n",
      "iteration 11000 / 12600: loss 1.177433\n",
      "iteration 11100 / 12600: loss 0.961450\n",
      "iteration 11200 / 12600: loss 1.207876\n",
      "epoch done... acc 0.526\n",
      "iteration 11300 / 12600: loss 1.084828\n",
      "iteration 11400 / 12600: loss 1.009134\n",
      "iteration 11500 / 12600: loss 0.987944\n",
      "iteration 11600 / 12600: loss 1.061749\n",
      "iteration 11700 / 12600: loss 0.877907\n",
      "epoch done... acc 0.516\n",
      "iteration 11800 / 12600: loss 1.097870\n",
      "iteration 11900 / 12600: loss 0.985010\n",
      "iteration 12000 / 12600: loss 0.940946\n",
      "iteration 12100 / 12600: loss 1.122321\n",
      "iteration 12200 / 12600: loss 0.990613\n",
      "epoch done... acc 0.515\n",
      "iteration 12300 / 12600: loss 0.858215\n",
      "iteration 12400 / 12600: loss 1.034606\n",
      "iteration 12500 / 12600: loss 1.162534\n",
      "Final training loss:  1.1158885600874573\n",
      "Final validation loss:  1.3786656986779355\n",
      "Final validation accuracy:  0.515\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "55 2 0 385 12600 100 0.001 0.98 0.515\n",
      "iteration 0 / 13860: loss 2.302511\n",
      "epoch done... acc 0.175\n",
      "iteration 100 / 13860: loss 2.038021\n",
      "iteration 200 / 13860: loss 2.042414\n",
      "iteration 300 / 13860: loss 1.725236\n",
      "iteration 400 / 13860: loss 1.932762\n",
      "epoch done... acc 0.376\n",
      "iteration 500 / 13860: loss 1.755680\n",
      "iteration 600 / 13860: loss 1.578650\n",
      "iteration 700 / 13860: loss 1.679734\n",
      "iteration 800 / 13860: loss 1.613705\n",
      "iteration 900 / 13860: loss 1.585923\n",
      "epoch done... acc 0.418\n",
      "iteration 1000 / 13860: loss 1.716686\n",
      "iteration 1100 / 13860: loss 1.779719\n",
      "iteration 1200 / 13860: loss 1.667859\n",
      "iteration 1300 / 13860: loss 1.554968\n",
      "iteration 1400 / 13860: loss 1.586409\n",
      "epoch done... acc 0.45\n",
      "iteration 1500 / 13860: loss 1.653653\n",
      "iteration 1600 / 13860: loss 1.600659\n",
      "iteration 1700 / 13860: loss 1.599505\n",
      "iteration 1800 / 13860: loss 1.442674\n",
      "iteration 1900 / 13860: loss 1.492077\n",
      "epoch done... acc 0.455\n",
      "iteration 2000 / 13860: loss 1.398557\n",
      "iteration 2100 / 13860: loss 1.301374\n",
      "iteration 2200 / 13860: loss 1.447346\n",
      "iteration 2300 / 13860: loss 1.550529\n",
      "iteration 2400 / 13860: loss 1.502294\n",
      "epoch done... acc 0.464\n",
      "iteration 2500 / 13860: loss 1.553311\n",
      "iteration 2600 / 13860: loss 1.566554\n",
      "iteration 2700 / 13860: loss 1.501274\n",
      "iteration 2800 / 13860: loss 1.226070\n",
      "iteration 2900 / 13860: loss 1.472597\n",
      "epoch done... acc 0.479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 3000 / 13860: loss 1.505778\n",
      "iteration 3100 / 13860: loss 1.376457\n",
      "iteration 3200 / 13860: loss 1.408550\n",
      "iteration 3300 / 13860: loss 1.442393\n",
      "iteration 3400 / 13860: loss 1.397488\n",
      "epoch done... acc 0.476\n",
      "iteration 3500 / 13860: loss 1.522753\n",
      "iteration 3600 / 13860: loss 1.322911\n",
      "iteration 3700 / 13860: loss 1.365184\n",
      "iteration 3800 / 13860: loss 1.456498\n",
      "iteration 3900 / 13860: loss 1.367223\n",
      "epoch done... acc 0.474\n",
      "iteration 4000 / 13860: loss 1.306859\n",
      "iteration 4100 / 13860: loss 1.342863\n",
      "iteration 4200 / 13860: loss 1.524379\n",
      "iteration 4300 / 13860: loss 1.279150\n",
      "iteration 4400 / 13860: loss 1.380796\n",
      "epoch done... acc 0.475\n",
      "iteration 4500 / 13860: loss 1.216355\n",
      "iteration 4600 / 13860: loss 1.269087\n",
      "iteration 4700 / 13860: loss 1.230822\n",
      "iteration 4800 / 13860: loss 1.180678\n",
      "iteration 4900 / 13860: loss 1.396324\n",
      "epoch done... acc 0.499\n",
      "iteration 5000 / 13860: loss 1.407241\n",
      "iteration 5100 / 13860: loss 1.402208\n",
      "iteration 5200 / 13860: loss 1.274227\n",
      "iteration 5300 / 13860: loss 1.267119\n",
      "epoch done... acc 0.491\n",
      "iteration 5400 / 13860: loss 1.289680\n",
      "iteration 5500 / 13860: loss 1.272189\n",
      "iteration 5600 / 13860: loss 1.045217\n",
      "iteration 5700 / 13860: loss 1.303877\n",
      "iteration 5800 / 13860: loss 1.150142\n",
      "epoch done... acc 0.496\n",
      "iteration 5900 / 13860: loss 1.227850\n",
      "iteration 6000 / 13860: loss 1.203943\n",
      "iteration 6100 / 13860: loss 1.155495\n",
      "iteration 6200 / 13860: loss 1.105532\n",
      "iteration 6300 / 13860: loss 1.347985\n",
      "epoch done... acc 0.486\n",
      "iteration 6400 / 13860: loss 1.330216\n",
      "iteration 6500 / 13860: loss 1.111805\n",
      "iteration 6600 / 13860: loss 1.225336\n",
      "iteration 6700 / 13860: loss 1.309113\n",
      "iteration 6800 / 13860: loss 1.230952\n",
      "epoch done... acc 0.505\n",
      "iteration 6900 / 13860: loss 1.052261\n",
      "iteration 7000 / 13860: loss 1.142227\n",
      "iteration 7100 / 13860: loss 1.169399\n",
      "iteration 7200 / 13860: loss 1.377939\n",
      "iteration 7300 / 13860: loss 1.242980\n",
      "epoch done... acc 0.518\n",
      "iteration 7400 / 13860: loss 1.104415\n",
      "iteration 7500 / 13860: loss 1.098911\n",
      "iteration 7600 / 13860: loss 1.114466\n",
      "iteration 7700 / 13860: loss 1.040152\n",
      "iteration 7800 / 13860: loss 1.317996\n",
      "epoch done... acc 0.529\n",
      "iteration 7900 / 13860: loss 1.081887\n",
      "iteration 8000 / 13860: loss 1.411990\n",
      "iteration 8100 / 13860: loss 1.202749\n",
      "iteration 8200 / 13860: loss 1.231085\n",
      "iteration 8300 / 13860: loss 1.239600\n",
      "epoch done... acc 0.518\n",
      "iteration 8400 / 13860: loss 1.280137\n",
      "iteration 8500 / 13860: loss 1.104162\n",
      "iteration 8600 / 13860: loss 1.275251\n",
      "iteration 8700 / 13860: loss 1.205112\n",
      "iteration 8800 / 13860: loss 1.069206\n",
      "epoch done... acc 0.508\n",
      "iteration 8900 / 13860: loss 1.257314\n",
      "iteration 9000 / 13860: loss 1.149472\n",
      "iteration 9100 / 13860: loss 1.083949\n",
      "iteration 9200 / 13860: loss 1.167981\n",
      "iteration 9300 / 13860: loss 1.233704\n",
      "epoch done... acc 0.52\n",
      "iteration 9400 / 13860: loss 1.137237\n",
      "iteration 9500 / 13860: loss 1.096440\n",
      "iteration 9600 / 13860: loss 1.193720\n",
      "iteration 9700 / 13860: loss 1.112893\n",
      "iteration 9800 / 13860: loss 1.270250\n",
      "epoch done... acc 0.53\n",
      "iteration 9900 / 13860: loss 1.013741\n",
      "iteration 10000 / 13860: loss 1.113182\n",
      "iteration 10100 / 13860: loss 0.937314\n",
      "iteration 10200 / 13860: loss 1.010633\n",
      "epoch done... acc 0.512\n",
      "iteration 10300 / 13860: loss 1.135501\n",
      "iteration 10400 / 13860: loss 1.051366\n",
      "iteration 10500 / 13860: loss 1.134554\n",
      "iteration 10600 / 13860: loss 1.111234\n",
      "iteration 10700 / 13860: loss 1.216524\n",
      "epoch done... acc 0.537\n",
      "iteration 10800 / 13860: loss 1.061746\n",
      "iteration 10900 / 13860: loss 1.194893\n",
      "iteration 11000 / 13860: loss 1.186760\n",
      "iteration 11100 / 13860: loss 1.045356\n",
      "iteration 11200 / 13860: loss 1.158175\n",
      "epoch done... acc 0.532\n",
      "iteration 11300 / 13860: loss 1.074946\n",
      "iteration 11400 / 13860: loss 0.862817\n",
      "iteration 11500 / 13860: loss 1.123733\n",
      "iteration 11600 / 13860: loss 1.197697\n",
      "iteration 11700 / 13860: loss 0.923372\n",
      "epoch done... acc 0.525\n",
      "iteration 11800 / 13860: loss 1.051071\n",
      "iteration 11900 / 13860: loss 1.154063\n",
      "iteration 12000 / 13860: loss 1.058407\n",
      "iteration 12100 / 13860: loss 0.987364\n",
      "iteration 12200 / 13860: loss 0.907414\n",
      "epoch done... acc 0.53\n",
      "iteration 12300 / 13860: loss 1.001090\n",
      "iteration 12400 / 13860: loss 1.047552\n",
      "iteration 12500 / 13860: loss 1.000459\n",
      "iteration 12600 / 13860: loss 1.046632\n",
      "iteration 12700 / 13860: loss 1.152966\n",
      "epoch done... acc 0.52\n",
      "iteration 12800 / 13860: loss 1.207757\n",
      "iteration 12900 / 13860: loss 1.099583\n",
      "iteration 13000 / 13860: loss 1.098966\n",
      "iteration 13100 / 13860: loss 0.963487\n",
      "iteration 13200 / 13860: loss 1.074960\n",
      "epoch done... acc 0.524\n",
      "iteration 13300 / 13860: loss 0.954573\n",
      "iteration 13400 / 13860: loss 1.097176\n",
      "iteration 13500 / 13860: loss 0.917688\n",
      "iteration 13600 / 13860: loss 0.803913\n",
      "iteration 13700 / 13860: loss 1.176144\n",
      "epoch done... acc 0.533\n",
      "iteration 13800 / 13860: loss 0.987155\n",
      "Final training loss:  1.020593787889393\n",
      "Final validation loss:  1.3772676625935403\n",
      "Final validation accuracy:  0.533\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "56 1 1 385 12600 100 0.001 0.98 0.533\n",
      "iteration 0 / 11340: loss 2.302554\n",
      "epoch done... acc 0.185\n",
      "iteration 100 / 11340: loss 1.929668\n",
      "iteration 200 / 11340: loss 1.926004\n",
      "iteration 300 / 11340: loss 1.898027\n",
      "iteration 400 / 11340: loss 1.980739\n",
      "epoch done... acc 0.376\n",
      "iteration 500 / 11340: loss 1.688902\n",
      "iteration 600 / 11340: loss 1.666913\n",
      "iteration 700 / 11340: loss 1.465270\n",
      "iteration 800 / 11340: loss 1.704328\n",
      "iteration 900 / 11340: loss 1.622299\n",
      "epoch done... acc 0.428\n",
      "iteration 1000 / 11340: loss 1.605747\n",
      "iteration 1100 / 11340: loss 1.608667\n",
      "iteration 1200 / 11340: loss 1.560699\n",
      "iteration 1300 / 11340: loss 1.601968\n",
      "iteration 1400 / 11340: loss 1.524752\n",
      "epoch done... acc 0.445\n",
      "iteration 1500 / 11340: loss 1.611956\n",
      "iteration 1600 / 11340: loss 1.594040\n",
      "iteration 1700 / 11340: loss 1.491170\n",
      "iteration 1800 / 11340: loss 1.444617\n",
      "iteration 1900 / 11340: loss 1.533142\n",
      "epoch done... acc 0.463\n",
      "iteration 2000 / 11340: loss 1.482648\n",
      "iteration 2100 / 11340: loss 1.281365\n",
      "iteration 2200 / 11340: loss 1.454294\n",
      "iteration 2300 / 11340: loss 1.491112\n",
      "iteration 2400 / 11340: loss 1.507851\n",
      "epoch done... acc 0.471\n",
      "iteration 2500 / 11340: loss 1.526738\n",
      "iteration 2600 / 11340: loss 1.341775\n",
      "iteration 2700 / 11340: loss 1.347902\n",
      "iteration 2800 / 11340: loss 1.368805\n",
      "iteration 2900 / 11340: loss 1.473475\n",
      "epoch done... acc 0.477\n",
      "iteration 3000 / 11340: loss 1.437916\n",
      "iteration 3100 / 11340: loss 1.473603\n",
      "iteration 3200 / 11340: loss 1.492952\n",
      "iteration 3300 / 11340: loss 1.417920\n",
      "iteration 3400 / 11340: loss 1.373461\n",
      "epoch done... acc 0.496\n",
      "iteration 3500 / 11340: loss 1.360831\n",
      "iteration 3600 / 11340: loss 1.321862\n",
      "iteration 3700 / 11340: loss 1.264748\n",
      "iteration 3800 / 11340: loss 1.447840\n",
      "iteration 3900 / 11340: loss 1.495832\n",
      "epoch done... acc 0.482\n",
      "iteration 4000 / 11340: loss 1.528864\n",
      "iteration 4100 / 11340: loss 1.291595\n",
      "iteration 4200 / 11340: loss 1.344072\n",
      "iteration 4300 / 11340: loss 1.375477\n",
      "iteration 4400 / 11340: loss 1.366506\n",
      "epoch done... acc 0.486\n",
      "iteration 4500 / 11340: loss 1.210997\n",
      "iteration 4600 / 11340: loss 1.283276\n",
      "iteration 4700 / 11340: loss 1.321530\n",
      "iteration 4800 / 11340: loss 1.225721\n",
      "iteration 4900 / 11340: loss 1.342278\n",
      "epoch done... acc 0.492\n",
      "iteration 5000 / 11340: loss 1.428334\n",
      "iteration 5100 / 11340: loss 1.227523\n",
      "iteration 5200 / 11340: loss 1.176471\n",
      "iteration 5300 / 11340: loss 1.289214\n",
      "epoch done... acc 0.512\n",
      "iteration 5400 / 11340: loss 1.260836\n",
      "iteration 5500 / 11340: loss 1.264878\n",
      "iteration 5600 / 11340: loss 1.279805\n",
      "iteration 5700 / 11340: loss 1.207868\n",
      "iteration 5800 / 11340: loss 1.173493\n",
      "epoch done... acc 0.524\n",
      "iteration 5900 / 11340: loss 1.236645\n",
      "iteration 6000 / 11340: loss 1.225221\n",
      "iteration 6100 / 11340: loss 1.198565\n",
      "iteration 6200 / 11340: loss 0.947218\n",
      "iteration 6300 / 11340: loss 1.114928\n",
      "epoch done... acc 0.501\n",
      "iteration 6400 / 11340: loss 1.176715\n",
      "iteration 6500 / 11340: loss 1.363241\n",
      "iteration 6600 / 11340: loss 1.201229\n",
      "iteration 6700 / 11340: loss 1.215377\n",
      "iteration 6800 / 11340: loss 1.368420\n",
      "epoch done... acc 0.522\n",
      "iteration 6900 / 11340: loss 1.346980\n",
      "iteration 7000 / 11340: loss 1.228835\n",
      "iteration 7100 / 11340: loss 1.196825\n",
      "iteration 7200 / 11340: loss 1.246873\n",
      "iteration 7300 / 11340: loss 1.316197\n",
      "epoch done... acc 0.512\n",
      "iteration 7400 / 11340: loss 1.432894\n",
      "iteration 7500 / 11340: loss 1.277500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7600 / 11340: loss 1.331146\n",
      "iteration 7700 / 11340: loss 1.091908\n",
      "iteration 7800 / 11340: loss 1.125822\n",
      "epoch done... acc 0.509\n",
      "iteration 7900 / 11340: loss 1.281708\n",
      "iteration 8000 / 11340: loss 1.336215\n",
      "iteration 8100 / 11340: loss 1.314401\n",
      "iteration 8200 / 11340: loss 1.178196\n",
      "iteration 8300 / 11340: loss 1.160798\n",
      "epoch done... acc 0.522\n",
      "iteration 8400 / 11340: loss 1.173456\n",
      "iteration 8500 / 11340: loss 1.032226\n",
      "iteration 8600 / 11340: loss 1.166667\n",
      "iteration 8700 / 11340: loss 1.125290\n",
      "iteration 8800 / 11340: loss 1.149498\n",
      "epoch done... acc 0.518\n",
      "iteration 8900 / 11340: loss 1.048124\n",
      "iteration 9000 / 11340: loss 1.254530\n",
      "iteration 9100 / 11340: loss 0.976386\n",
      "iteration 9200 / 11340: loss 1.130624\n",
      "iteration 9300 / 11340: loss 1.255076\n",
      "epoch done... acc 0.519\n",
      "iteration 9400 / 11340: loss 1.096400\n",
      "iteration 9500 / 11340: loss 1.302289\n",
      "iteration 9600 / 11340: loss 1.058672\n",
      "iteration 9700 / 11340: loss 1.179496\n",
      "iteration 9800 / 11340: loss 1.054780\n",
      "epoch done... acc 0.514\n",
      "iteration 9900 / 11340: loss 0.933853\n",
      "iteration 10000 / 11340: loss 1.003405\n",
      "iteration 10100 / 11340: loss 1.080199\n",
      "iteration 10200 / 11340: loss 1.153274\n",
      "epoch done... acc 0.519\n",
      "iteration 10300 / 11340: loss 1.068329\n",
      "iteration 10400 / 11340: loss 1.068854\n",
      "iteration 10500 / 11340: loss 1.243809\n",
      "iteration 10600 / 11340: loss 1.179821\n",
      "iteration 10700 / 11340: loss 1.036218\n",
      "epoch done... acc 0.53\n",
      "iteration 10800 / 11340: loss 1.123918\n",
      "iteration 10900 / 11340: loss 1.068416\n",
      "iteration 11000 / 11340: loss 1.240420\n",
      "iteration 11100 / 11340: loss 1.211680\n",
      "iteration 11200 / 11340: loss 0.978648\n",
      "epoch done... acc 0.532\n",
      "iteration 11300 / 11340: loss 1.337367\n",
      "Final training loss:  1.07786744425493\n",
      "Final validation loss:  1.3674810819164214\n",
      "Final validation accuracy:  0.532\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "57 2 1 385 12600 100 0.001 0.98 0.532\n",
      "iteration 0 / 12600: loss 2.302649\n",
      "epoch done... acc 0.138\n",
      "iteration 100 / 12600: loss 2.100796\n",
      "iteration 200 / 12600: loss 1.927007\n",
      "iteration 300 / 12600: loss 1.875513\n",
      "iteration 400 / 12600: loss 1.719003\n",
      "epoch done... acc 0.374\n",
      "iteration 500 / 12600: loss 1.866421\n",
      "iteration 600 / 12600: loss 1.657356\n",
      "iteration 700 / 12600: loss 1.549024\n",
      "iteration 800 / 12600: loss 1.774472\n",
      "iteration 900 / 12600: loss 1.749808\n",
      "epoch done... acc 0.432\n",
      "iteration 1000 / 12600: loss 1.722074\n",
      "iteration 1100 / 12600: loss 1.446908\n",
      "iteration 1200 / 12600: loss 1.648555\n",
      "iteration 1300 / 12600: loss 1.528966\n",
      "iteration 1400 / 12600: loss 1.560687\n",
      "epoch done... acc 0.453\n",
      "iteration 1500 / 12600: loss 1.625624\n",
      "iteration 1600 / 12600: loss 1.633662\n",
      "iteration 1700 / 12600: loss 1.479752\n",
      "iteration 1800 / 12600: loss 1.543471\n",
      "iteration 1900 / 12600: loss 1.368651\n",
      "epoch done... acc 0.453\n",
      "iteration 2000 / 12600: loss 1.341474\n",
      "iteration 2100 / 12600: loss 1.384040\n",
      "iteration 2200 / 12600: loss 1.548765\n",
      "iteration 2300 / 12600: loss 1.443056\n",
      "iteration 2400 / 12600: loss 1.394108\n",
      "epoch done... acc 0.46\n",
      "iteration 2500 / 12600: loss 1.509578\n",
      "iteration 2600 / 12600: loss 1.406195\n",
      "iteration 2700 / 12600: loss 1.380640\n",
      "iteration 2800 / 12600: loss 1.316166\n",
      "iteration 2900 / 12600: loss 1.479428\n",
      "epoch done... acc 0.491\n",
      "iteration 3000 / 12600: loss 1.438212\n",
      "iteration 3100 / 12600: loss 1.529036\n",
      "iteration 3200 / 12600: loss 1.493054\n",
      "iteration 3300 / 12600: loss 1.301387\n",
      "iteration 3400 / 12600: loss 1.376533\n",
      "epoch done... acc 0.479\n",
      "iteration 3500 / 12600: loss 1.390210\n",
      "iteration 3600 / 12600: loss 1.469308\n",
      "iteration 3700 / 12600: loss 1.425924\n",
      "iteration 3800 / 12600: loss 1.331618\n",
      "iteration 3900 / 12600: loss 1.335447\n",
      "epoch done... acc 0.49\n",
      "iteration 4000 / 12600: loss 1.505653\n",
      "iteration 4100 / 12600: loss 1.468419\n",
      "iteration 4200 / 12600: loss 1.420202\n",
      "iteration 4300 / 12600: loss 1.357611\n",
      "iteration 4400 / 12600: loss 1.397638\n",
      "epoch done... acc 0.494\n",
      "iteration 4500 / 12600: loss 1.282288\n",
      "iteration 4600 / 12600: loss 1.292951\n",
      "iteration 4700 / 12600: loss 1.240219\n",
      "iteration 4800 / 12600: loss 1.260320\n",
      "iteration 4900 / 12600: loss 1.226947\n",
      "epoch done... acc 0.505\n",
      "iteration 5000 / 12600: loss 1.214118\n",
      "iteration 5100 / 12600: loss 1.339012\n",
      "iteration 5200 / 12600: loss 1.274434\n",
      "iteration 5300 / 12600: loss 1.089402\n",
      "epoch done... acc 0.504\n",
      "iteration 5400 / 12600: loss 1.455948\n",
      "iteration 5500 / 12600: loss 1.279552\n",
      "iteration 5600 / 12600: loss 1.324900\n",
      "iteration 5700 / 12600: loss 1.104171\n",
      "iteration 5800 / 12600: loss 1.321746\n",
      "epoch done... acc 0.499\n",
      "iteration 5900 / 12600: loss 1.263588\n",
      "iteration 6000 / 12600: loss 1.140923\n",
      "iteration 6100 / 12600: loss 1.331674\n",
      "iteration 6200 / 12600: loss 1.493430\n",
      "iteration 6300 / 12600: loss 1.393875\n",
      "epoch done... acc 0.495\n",
      "iteration 6400 / 12600: loss 1.450270\n",
      "iteration 6500 / 12600: loss 1.489193\n",
      "iteration 6600 / 12600: loss 1.307147\n",
      "iteration 6700 / 12600: loss 1.110042\n",
      "iteration 6800 / 12600: loss 1.417284\n",
      "epoch done... acc 0.498\n",
      "iteration 6900 / 12600: loss 1.188781\n",
      "iteration 7000 / 12600: loss 1.227384\n",
      "iteration 7100 / 12600: loss 1.046106\n",
      "iteration 7200 / 12600: loss 1.231265\n",
      "iteration 7300 / 12600: loss 1.335744\n",
      "epoch done... acc 0.517\n",
      "iteration 7400 / 12600: loss 1.218398\n",
      "iteration 7500 / 12600: loss 1.086083\n",
      "iteration 7600 / 12600: loss 1.049342\n",
      "iteration 7700 / 12600: loss 1.173285\n",
      "iteration 7800 / 12600: loss 1.281417\n",
      "epoch done... acc 0.513\n",
      "iteration 7900 / 12600: loss 1.261984\n",
      "iteration 8000 / 12600: loss 1.224252\n",
      "iteration 8100 / 12600: loss 1.148140\n",
      "iteration 8200 / 12600: loss 1.130940\n",
      "iteration 8300 / 12600: loss 1.216334\n",
      "epoch done... acc 0.511\n",
      "iteration 8400 / 12600: loss 1.295889\n",
      "iteration 8500 / 12600: loss 1.108272\n",
      "iteration 8600 / 12600: loss 1.260065\n",
      "iteration 8700 / 12600: loss 1.071670\n",
      "iteration 8800 / 12600: loss 1.103363\n",
      "epoch done... acc 0.524\n",
      "iteration 8900 / 12600: loss 0.916015\n",
      "iteration 9000 / 12600: loss 1.188046\n",
      "iteration 9100 / 12600: loss 1.323108\n",
      "iteration 9200 / 12600: loss 0.932044\n",
      "iteration 9300 / 12600: loss 1.199881\n",
      "epoch done... acc 0.513\n",
      "iteration 9400 / 12600: loss 1.116506\n",
      "iteration 9500 / 12600: loss 0.995376\n",
      "iteration 9600 / 12600: loss 1.212528\n",
      "iteration 9700 / 12600: loss 1.148700\n",
      "iteration 9800 / 12600: loss 1.189472\n",
      "epoch done... acc 0.527\n",
      "iteration 9900 / 12600: loss 1.341486\n",
      "iteration 10000 / 12600: loss 0.969981\n",
      "iteration 10100 / 12600: loss 0.980395\n",
      "iteration 10200 / 12600: loss 1.081914\n",
      "epoch done... acc 0.514\n",
      "iteration 10300 / 12600: loss 1.097022\n",
      "iteration 10400 / 12600: loss 1.165108\n",
      "iteration 10500 / 12600: loss 1.088389\n",
      "iteration 10600 / 12600: loss 1.143858\n",
      "iteration 10700 / 12600: loss 1.033449\n",
      "epoch done... acc 0.519\n",
      "iteration 10800 / 12600: loss 1.117069\n",
      "iteration 10900 / 12600: loss 1.179960\n",
      "iteration 11000 / 12600: loss 0.941036\n",
      "iteration 11100 / 12600: loss 1.190046\n",
      "iteration 11200 / 12600: loss 1.121403\n",
      "epoch done... acc 0.521\n",
      "iteration 11300 / 12600: loss 1.092989\n",
      "iteration 11400 / 12600: loss 1.124767\n",
      "iteration 11500 / 12600: loss 1.043483\n",
      "iteration 11600 / 12600: loss 1.310043\n",
      "iteration 11700 / 12600: loss 1.067963\n",
      "epoch done... acc 0.522\n",
      "iteration 11800 / 12600: loss 1.184978\n",
      "iteration 11900 / 12600: loss 0.958602\n",
      "iteration 12000 / 12600: loss 1.013504\n",
      "iteration 12100 / 12600: loss 0.979598\n",
      "iteration 12200 / 12600: loss 1.084101\n",
      "epoch done... acc 0.528\n",
      "iteration 12300 / 12600: loss 1.114998\n",
      "iteration 12400 / 12600: loss 1.150043\n",
      "iteration 12500 / 12600: loss 1.083599\n",
      "Final training loss:  0.9954474442961702\n",
      "Final validation loss:  1.371211186302844\n",
      "Final validation accuracy:  0.528\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "58 1 0 385 12600 100 0.001 0.98 0.528\n",
      "iteration 0 / 12600: loss 2.302639\n",
      "epoch done... acc 0.189\n",
      "iteration 100 / 12600: loss 1.980009\n",
      "iteration 200 / 12600: loss 1.996928\n",
      "iteration 300 / 12600: loss 1.709651\n",
      "iteration 400 / 12600: loss 1.757330\n",
      "epoch done... acc 0.378\n",
      "iteration 500 / 12600: loss 1.710524\n",
      "iteration 600 / 12600: loss 1.628722\n",
      "iteration 700 / 12600: loss 1.682807\n",
      "iteration 800 / 12600: loss 1.739035\n",
      "iteration 900 / 12600: loss 1.784594\n",
      "epoch done... acc 0.425\n",
      "iteration 1000 / 12600: loss 1.757686\n",
      "iteration 1100 / 12600: loss 1.730132\n",
      "iteration 1200 / 12600: loss 1.557656\n",
      "iteration 1300 / 12600: loss 1.697422\n",
      "iteration 1400 / 12600: loss 1.578745\n",
      "epoch done... acc 0.441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1500 / 12600: loss 1.557160\n",
      "iteration 1600 / 12600: loss 1.511329\n",
      "iteration 1700 / 12600: loss 1.727873\n",
      "iteration 1800 / 12600: loss 1.724615\n",
      "iteration 1900 / 12600: loss 1.535542\n",
      "epoch done... acc 0.453\n",
      "iteration 2000 / 12600: loss 1.468608\n",
      "iteration 2100 / 12600: loss 1.473705\n",
      "iteration 2200 / 12600: loss 1.569346\n",
      "iteration 2300 / 12600: loss 1.379630\n",
      "iteration 2400 / 12600: loss 1.691426\n",
      "epoch done... acc 0.439\n",
      "iteration 2500 / 12600: loss 1.517595\n",
      "iteration 2600 / 12600: loss 1.482231\n",
      "iteration 2700 / 12600: loss 1.446992\n",
      "iteration 2800 / 12600: loss 1.389964\n",
      "iteration 2900 / 12600: loss 1.465003\n",
      "epoch done... acc 0.463\n",
      "iteration 3000 / 12600: loss 1.393854\n",
      "iteration 3100 / 12600: loss 1.449405\n",
      "iteration 3200 / 12600: loss 1.612617\n",
      "iteration 3300 / 12600: loss 1.353668\n",
      "iteration 3400 / 12600: loss 1.419789\n",
      "epoch done... acc 0.469\n",
      "iteration 3500 / 12600: loss 1.391261\n",
      "iteration 3600 / 12600: loss 1.288071\n",
      "iteration 3700 / 12600: loss 1.465093\n",
      "iteration 3800 / 12600: loss 1.377966\n",
      "iteration 3900 / 12600: loss 1.550538\n",
      "epoch done... acc 0.485\n",
      "iteration 4000 / 12600: loss 1.413255\n",
      "iteration 4100 / 12600: loss 1.304915\n",
      "iteration 4200 / 12600: loss 1.401247\n",
      "iteration 4300 / 12600: loss 1.282016\n",
      "iteration 4400 / 12600: loss 1.508579\n",
      "epoch done... acc 0.491\n",
      "iteration 4500 / 12600: loss 1.426559\n",
      "iteration 4600 / 12600: loss 1.145797\n",
      "iteration 4700 / 12600: loss 1.370769\n",
      "iteration 4800 / 12600: loss 1.243423\n",
      "iteration 4900 / 12600: loss 1.213141\n",
      "epoch done... acc 0.489\n",
      "iteration 5000 / 12600: loss 1.338343\n",
      "iteration 5100 / 12600: loss 1.473756\n",
      "iteration 5200 / 12600: loss 1.192030\n",
      "iteration 5300 / 12600: loss 1.283288\n",
      "epoch done... acc 0.502\n",
      "iteration 5400 / 12600: loss 1.396924\n",
      "iteration 5500 / 12600: loss 1.337853\n",
      "iteration 5600 / 12600: loss 1.424833\n",
      "iteration 5700 / 12600: loss 1.362191\n",
      "iteration 5800 / 12600: loss 1.296119\n",
      "epoch done... acc 0.516\n",
      "iteration 5900 / 12600: loss 1.358741\n",
      "iteration 6000 / 12600: loss 1.197114\n",
      "iteration 6100 / 12600: loss 1.094395\n",
      "iteration 6200 / 12600: loss 1.181872\n",
      "iteration 6300 / 12600: loss 1.515543\n",
      "epoch done... acc 0.507\n",
      "iteration 6400 / 12600: loss 1.338020\n",
      "iteration 6500 / 12600: loss 1.345111\n",
      "iteration 6600 / 12600: loss 1.076721\n",
      "iteration 6700 / 12600: loss 1.044811\n",
      "iteration 6800 / 12600: loss 1.187890\n",
      "epoch done... acc 0.51\n",
      "iteration 6900 / 12600: loss 1.268377\n",
      "iteration 7000 / 12600: loss 1.206283\n",
      "iteration 7100 / 12600: loss 1.298811\n",
      "iteration 7200 / 12600: loss 1.339349\n",
      "iteration 7300 / 12600: loss 1.289792\n",
      "epoch done... acc 0.505\n",
      "iteration 7400 / 12600: loss 1.020656\n",
      "iteration 7500 / 12600: loss 1.182948\n",
      "iteration 7600 / 12600: loss 1.127351\n",
      "iteration 7700 / 12600: loss 1.219627\n",
      "iteration 7800 / 12600: loss 1.297604\n",
      "epoch done... acc 0.502\n",
      "iteration 7900 / 12600: loss 1.237424\n",
      "iteration 8000 / 12600: loss 1.122811\n",
      "iteration 8100 / 12600: loss 1.266246\n",
      "iteration 8200 / 12600: loss 1.214118\n",
      "iteration 8300 / 12600: loss 1.251413\n",
      "epoch done... acc 0.515\n",
      "iteration 8400 / 12600: loss 1.268328\n",
      "iteration 8500 / 12600: loss 1.391973\n",
      "iteration 8600 / 12600: loss 1.248394\n",
      "iteration 8700 / 12600: loss 1.288353\n",
      "iteration 8800 / 12600: loss 1.163814\n",
      "epoch done... acc 0.522\n",
      "iteration 8900 / 12600: loss 1.308764\n",
      "iteration 9000 / 12600: loss 1.164510\n",
      "iteration 9100 / 12600: loss 1.241440\n",
      "iteration 9200 / 12600: loss 1.062794\n",
      "iteration 9300 / 12600: loss 1.170082\n",
      "epoch done... acc 0.516\n",
      "iteration 9400 / 12600: loss 1.383085\n",
      "iteration 9500 / 12600: loss 1.332301\n",
      "iteration 9600 / 12600: loss 1.429330\n",
      "iteration 9700 / 12600: loss 1.089665\n",
      "iteration 9800 / 12600: loss 1.243857\n",
      "epoch done... acc 0.505\n",
      "iteration 9900 / 12600: loss 1.299701\n",
      "iteration 10000 / 12600: loss 1.099240\n",
      "iteration 10100 / 12600: loss 1.125615\n",
      "iteration 10200 / 12600: loss 1.075285\n",
      "epoch done... acc 0.508\n",
      "iteration 10300 / 12600: loss 1.060038\n",
      "iteration 10400 / 12600: loss 1.150525\n",
      "iteration 10500 / 12600: loss 1.231784\n",
      "iteration 10600 / 12600: loss 1.006901\n",
      "iteration 10700 / 12600: loss 1.316949\n",
      "epoch done... acc 0.507\n",
      "iteration 10800 / 12600: loss 1.069242\n",
      "iteration 10900 / 12600: loss 0.954542\n",
      "iteration 11000 / 12600: loss 1.167795\n",
      "iteration 11100 / 12600: loss 1.073373\n",
      "iteration 11200 / 12600: loss 1.092509\n",
      "epoch done... acc 0.498\n",
      "iteration 11300 / 12600: loss 1.013637\n",
      "iteration 11400 / 12600: loss 1.006722\n",
      "iteration 11500 / 12600: loss 0.965024\n",
      "iteration 11600 / 12600: loss 1.030385\n",
      "iteration 11700 / 12600: loss 1.197126\n",
      "epoch done... acc 0.501\n",
      "iteration 11800 / 12600: loss 1.273921\n",
      "iteration 11900 / 12600: loss 0.980504\n",
      "iteration 12000 / 12600: loss 1.157937\n",
      "iteration 12100 / 12600: loss 0.829244\n",
      "iteration 12200 / 12600: loss 1.143526\n",
      "epoch done... acc 0.513\n",
      "iteration 12300 / 12600: loss 0.970227\n",
      "iteration 12400 / 12600: loss 1.166128\n",
      "iteration 12500 / 12600: loss 1.164551\n",
      "Final training loss:  1.0780501922406502\n",
      "Final validation loss:  1.3619020774237618\n",
      "Final validation accuracy:  0.513\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "59 2 0 385 12600 100 0.001 0.98 0.513\n",
      "iteration 0 / 13860: loss 2.302548\n",
      "epoch done... acc 0.134\n",
      "iteration 100 / 13860: loss 1.970216\n",
      "iteration 200 / 13860: loss 1.805715\n",
      "iteration 300 / 13860: loss 1.758185\n",
      "iteration 400 / 13860: loss 1.681130\n",
      "epoch done... acc 0.37\n",
      "iteration 500 / 13860: loss 1.736312\n",
      "iteration 600 / 13860: loss 1.714438\n",
      "iteration 700 / 13860: loss 1.764492\n",
      "iteration 800 / 13860: loss 1.529776\n",
      "iteration 900 / 13860: loss 1.495300\n",
      "epoch done... acc 0.427\n",
      "iteration 1000 / 13860: loss 1.585648\n",
      "iteration 1100 / 13860: loss 1.806731\n",
      "iteration 1200 / 13860: loss 1.552371\n",
      "iteration 1300 / 13860: loss 1.560254\n",
      "iteration 1400 / 13860: loss 1.707568\n",
      "epoch done... acc 0.433\n",
      "iteration 1500 / 13860: loss 1.554733\n",
      "iteration 1600 / 13860: loss 1.632743\n",
      "iteration 1700 / 13860: loss 1.525809\n",
      "iteration 1800 / 13860: loss 1.530525\n",
      "iteration 1900 / 13860: loss 1.446572\n",
      "epoch done... acc 0.452\n",
      "iteration 2000 / 13860: loss 1.597139\n",
      "iteration 2100 / 13860: loss 1.671893\n",
      "iteration 2200 / 13860: loss 1.419327\n",
      "iteration 2300 / 13860: loss 1.286333\n",
      "iteration 2400 / 13860: loss 1.519666\n",
      "epoch done... acc 0.473\n",
      "iteration 2500 / 13860: loss 1.407189\n",
      "iteration 2600 / 13860: loss 1.353737\n",
      "iteration 2700 / 13860: loss 1.362707\n",
      "iteration 2800 / 13860: loss 1.311707\n",
      "iteration 2900 / 13860: loss 1.589069\n",
      "epoch done... acc 0.471\n",
      "iteration 3000 / 13860: loss 1.340906\n",
      "iteration 3100 / 13860: loss 1.366781\n",
      "iteration 3200 / 13860: loss 1.610897\n",
      "iteration 3300 / 13860: loss 1.395235\n",
      "iteration 3400 / 13860: loss 1.345160\n",
      "epoch done... acc 0.487\n",
      "iteration 3500 / 13860: loss 1.498189\n",
      "iteration 3600 / 13860: loss 1.552588\n",
      "iteration 3700 / 13860: loss 1.374919\n",
      "iteration 3800 / 13860: loss 1.379713\n",
      "iteration 3900 / 13860: loss 1.230764\n",
      "epoch done... acc 0.489\n",
      "iteration 4000 / 13860: loss 1.373036\n",
      "iteration 4100 / 13860: loss 1.252674\n",
      "iteration 4200 / 13860: loss 1.290699\n",
      "iteration 4300 / 13860: loss 1.370534\n",
      "iteration 4400 / 13860: loss 1.224047\n",
      "epoch done... acc 0.503\n",
      "iteration 4500 / 13860: loss 1.265244\n",
      "iteration 4600 / 13860: loss 1.349494\n",
      "iteration 4700 / 13860: loss 1.348313\n",
      "iteration 4800 / 13860: loss 1.314597\n",
      "iteration 4900 / 13860: loss 1.171646\n",
      "epoch done... acc 0.492\n",
      "iteration 5000 / 13860: loss 1.348495\n",
      "iteration 5100 / 13860: loss 1.389359\n",
      "iteration 5200 / 13860: loss 1.336680\n",
      "iteration 5300 / 13860: loss 1.144980\n",
      "epoch done... acc 0.5\n",
      "iteration 5400 / 13860: loss 1.246355\n",
      "iteration 5500 / 13860: loss 1.281304\n",
      "iteration 5600 / 13860: loss 1.407573\n",
      "iteration 5700 / 13860: loss 1.325916\n",
      "iteration 5800 / 13860: loss 1.240584\n",
      "epoch done... acc 0.497\n",
      "iteration 5900 / 13860: loss 1.359580\n",
      "iteration 6000 / 13860: loss 1.405774\n",
      "iteration 6100 / 13860: loss 1.336109\n",
      "iteration 6200 / 13860: loss 1.175347\n",
      "iteration 6300 / 13860: loss 1.269533\n",
      "epoch done... acc 0.511\n",
      "iteration 6400 / 13860: loss 1.350310\n",
      "iteration 6500 / 13860: loss 1.224370\n",
      "iteration 6600 / 13860: loss 1.403229\n",
      "iteration 6700 / 13860: loss 1.310311\n",
      "iteration 6800 / 13860: loss 1.339206\n",
      "epoch done... acc 0.507\n",
      "iteration 6900 / 13860: loss 1.177584\n",
      "iteration 7000 / 13860: loss 1.452652\n",
      "iteration 7100 / 13860: loss 1.245723\n",
      "iteration 7200 / 13860: loss 1.240046\n",
      "iteration 7300 / 13860: loss 1.174515\n",
      "epoch done... acc 0.513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7400 / 13860: loss 1.133334\n",
      "iteration 7500 / 13860: loss 1.107395\n",
      "iteration 7600 / 13860: loss 1.238585\n",
      "iteration 7700 / 13860: loss 1.236451\n",
      "iteration 7800 / 13860: loss 0.994655\n",
      "epoch done... acc 0.51\n",
      "iteration 7900 / 13860: loss 1.213837\n",
      "iteration 8000 / 13860: loss 1.246809\n",
      "iteration 8100 / 13860: loss 1.224206\n",
      "iteration 8200 / 13860: loss 1.094518\n",
      "iteration 8300 / 13860: loss 1.182375\n",
      "epoch done... acc 0.534\n",
      "iteration 8400 / 13860: loss 1.091309\n",
      "iteration 8500 / 13860: loss 1.161597\n",
      "iteration 8600 / 13860: loss 1.040315\n",
      "iteration 8700 / 13860: loss 1.162222\n",
      "iteration 8800 / 13860: loss 1.215003\n",
      "epoch done... acc 0.521\n",
      "iteration 8900 / 13860: loss 1.329156\n",
      "iteration 9000 / 13860: loss 1.132388\n",
      "iteration 9100 / 13860: loss 1.292756\n",
      "iteration 9200 / 13860: loss 1.219399\n",
      "iteration 9300 / 13860: loss 1.139412\n",
      "epoch done... acc 0.515\n",
      "iteration 9400 / 13860: loss 1.243561\n",
      "iteration 9500 / 13860: loss 1.250933\n",
      "iteration 9600 / 13860: loss 0.920108\n",
      "iteration 9700 / 13860: loss 1.236858\n",
      "iteration 9800 / 13860: loss 1.167747\n",
      "epoch done... acc 0.523\n",
      "iteration 9900 / 13860: loss 1.026017\n",
      "iteration 10000 / 13860: loss 1.129994\n",
      "iteration 10100 / 13860: loss 1.146272\n",
      "iteration 10200 / 13860: loss 0.881654\n",
      "epoch done... acc 0.511\n",
      "iteration 10300 / 13860: loss 1.483523\n",
      "iteration 10400 / 13860: loss 1.101463\n",
      "iteration 10500 / 13860: loss 1.071148\n",
      "iteration 10600 / 13860: loss 1.153880\n",
      "iteration 10700 / 13860: loss 1.112828\n",
      "epoch done... acc 0.523\n",
      "iteration 10800 / 13860: loss 1.184217\n",
      "iteration 10900 / 13860: loss 1.018590\n",
      "iteration 11000 / 13860: loss 1.001320\n",
      "iteration 11100 / 13860: loss 1.025647\n",
      "iteration 11200 / 13860: loss 1.309490\n",
      "epoch done... acc 0.529\n",
      "iteration 11300 / 13860: loss 1.106098\n",
      "iteration 11400 / 13860: loss 0.872056\n",
      "iteration 11500 / 13860: loss 1.057638\n",
      "iteration 11600 / 13860: loss 1.016893\n",
      "iteration 11700 / 13860: loss 1.022207\n",
      "epoch done... acc 0.527\n",
      "iteration 11800 / 13860: loss 0.956438\n",
      "iteration 11900 / 13860: loss 1.146507\n",
      "iteration 12000 / 13860: loss 0.997931\n",
      "iteration 12100 / 13860: loss 1.158308\n",
      "iteration 12200 / 13860: loss 1.171803\n",
      "epoch done... acc 0.539\n",
      "iteration 12300 / 13860: loss 0.958207\n",
      "iteration 12400 / 13860: loss 1.126816\n",
      "iteration 12500 / 13860: loss 1.156524\n",
      "iteration 12600 / 13860: loss 0.933207\n",
      "iteration 12700 / 13860: loss 1.183304\n",
      "epoch done... acc 0.518\n",
      "iteration 12800 / 13860: loss 1.066916\n",
      "iteration 12900 / 13860: loss 0.961415\n",
      "iteration 13000 / 13860: loss 0.792569\n",
      "iteration 13100 / 13860: loss 1.049784\n",
      "iteration 13200 / 13860: loss 1.093754\n",
      "epoch done... acc 0.523\n",
      "iteration 13300 / 13860: loss 1.035266\n",
      "iteration 13400 / 13860: loss 1.090309\n",
      "iteration 13500 / 13860: loss 0.998730\n",
      "iteration 13600 / 13860: loss 0.846176\n",
      "iteration 13700 / 13860: loss 1.003437\n",
      "epoch done... acc 0.531\n",
      "iteration 13800 / 13860: loss 1.144355\n",
      "Final training loss:  1.0365062981944968\n",
      "Final validation loss:  1.3577488955060968\n",
      "Final validation accuracy:  0.531\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "60 1 1 385 12600 100 0.001 0.98 0.531\n",
      "iteration 0 / 11340: loss 2.302467\n",
      "epoch done... acc 0.189\n",
      "iteration 100 / 11340: loss 1.896020\n",
      "iteration 200 / 11340: loss 1.902379\n",
      "iteration 300 / 11340: loss 1.846967\n",
      "iteration 400 / 11340: loss 1.849051\n",
      "epoch done... acc 0.376\n",
      "iteration 500 / 11340: loss 1.753449\n",
      "iteration 600 / 11340: loss 1.594571\n",
      "iteration 700 / 11340: loss 1.781832\n",
      "iteration 800 / 11340: loss 1.688044\n",
      "iteration 900 / 11340: loss 1.703967\n",
      "epoch done... acc 0.427\n",
      "iteration 1000 / 11340: loss 1.568690\n",
      "iteration 1100 / 11340: loss 1.543914\n",
      "iteration 1200 / 11340: loss 1.660257\n",
      "iteration 1300 / 11340: loss 1.596525\n",
      "iteration 1400 / 11340: loss 1.497765\n",
      "epoch done... acc 0.445\n",
      "iteration 1500 / 11340: loss 1.609181\n",
      "iteration 1600 / 11340: loss 1.497116\n",
      "iteration 1700 / 11340: loss 1.508202\n",
      "iteration 1800 / 11340: loss 1.410819\n",
      "iteration 1900 / 11340: loss 1.626450\n",
      "epoch done... acc 0.45\n",
      "iteration 2000 / 11340: loss 1.382536\n",
      "iteration 2100 / 11340: loss 1.377737\n",
      "iteration 2200 / 11340: loss 1.442519\n",
      "iteration 2300 / 11340: loss 1.478401\n",
      "iteration 2400 / 11340: loss 1.419148\n",
      "epoch done... acc 0.452\n",
      "iteration 2500 / 11340: loss 1.546331\n",
      "iteration 2600 / 11340: loss 1.354751\n",
      "iteration 2700 / 11340: loss 1.363057\n",
      "iteration 2800 / 11340: loss 1.310567\n",
      "iteration 2900 / 11340: loss 1.438198\n",
      "epoch done... acc 0.473\n",
      "iteration 3000 / 11340: loss 1.132597\n",
      "iteration 3100 / 11340: loss 1.419343\n",
      "iteration 3200 / 11340: loss 1.264711\n",
      "iteration 3300 / 11340: loss 1.248606\n",
      "iteration 3400 / 11340: loss 1.415011\n",
      "epoch done... acc 0.487\n",
      "iteration 3500 / 11340: loss 1.390999\n",
      "iteration 3600 / 11340: loss 1.284772\n",
      "iteration 3700 / 11340: loss 1.525988\n",
      "iteration 3800 / 11340: loss 1.405337\n",
      "iteration 3900 / 11340: loss 1.232032\n",
      "epoch done... acc 0.483\n",
      "iteration 4000 / 11340: loss 1.276957\n",
      "iteration 4100 / 11340: loss 1.421852\n",
      "iteration 4200 / 11340: loss 1.338867\n",
      "iteration 4300 / 11340: loss 1.355048\n",
      "iteration 4400 / 11340: loss 1.417338\n",
      "epoch done... acc 0.486\n",
      "iteration 4500 / 11340: loss 1.350861\n",
      "iteration 4600 / 11340: loss 1.125985\n",
      "iteration 4700 / 11340: loss 1.315234\n",
      "iteration 4800 / 11340: loss 1.229362\n",
      "iteration 4900 / 11340: loss 1.244939\n",
      "epoch done... acc 0.496\n",
      "iteration 5000 / 11340: loss 1.345924\n",
      "iteration 5100 / 11340: loss 1.131444\n",
      "iteration 5200 / 11340: loss 1.381668\n",
      "iteration 5300 / 11340: loss 1.199365\n",
      "epoch done... acc 0.494\n",
      "iteration 5400 / 11340: loss 1.234594\n",
      "iteration 5500 / 11340: loss 1.156956\n",
      "iteration 5600 / 11340: loss 1.427426\n",
      "iteration 5700 / 11340: loss 1.325723\n",
      "iteration 5800 / 11340: loss 1.125507\n",
      "epoch done... acc 0.483\n",
      "iteration 5900 / 11340: loss 1.328534\n",
      "iteration 6000 / 11340: loss 1.226301\n",
      "iteration 6100 / 11340: loss 1.141101\n",
      "iteration 6200 / 11340: loss 1.379448\n",
      "iteration 6300 / 11340: loss 1.100978\n",
      "epoch done... acc 0.501\n",
      "iteration 6400 / 11340: loss 1.353273\n",
      "iteration 6500 / 11340: loss 1.358007\n",
      "iteration 6600 / 11340: loss 1.149020\n",
      "iteration 6700 / 11340: loss 1.090360\n",
      "iteration 6800 / 11340: loss 1.261128\n",
      "epoch done... acc 0.484\n",
      "iteration 6900 / 11340: loss 1.141837\n",
      "iteration 7000 / 11340: loss 1.338724\n",
      "iteration 7100 / 11340: loss 1.214465\n",
      "iteration 7200 / 11340: loss 1.212950\n",
      "iteration 7300 / 11340: loss 1.142062\n",
      "epoch done... acc 0.492\n",
      "iteration 7400 / 11340: loss 1.310115\n",
      "iteration 7500 / 11340: loss 1.185635\n",
      "iteration 7600 / 11340: loss 1.092454\n",
      "iteration 7700 / 11340: loss 1.073490\n",
      "iteration 7800 / 11340: loss 1.138935\n",
      "epoch done... acc 0.519\n",
      "iteration 7900 / 11340: loss 1.171430\n",
      "iteration 8000 / 11340: loss 1.238327\n",
      "iteration 8100 / 11340: loss 1.247975\n",
      "iteration 8200 / 11340: loss 1.082156\n",
      "iteration 8300 / 11340: loss 1.162236\n",
      "epoch done... acc 0.493\n",
      "iteration 8400 / 11340: loss 1.208340\n",
      "iteration 8500 / 11340: loss 1.148827\n",
      "iteration 8600 / 11340: loss 1.013737\n",
      "iteration 8700 / 11340: loss 1.202276\n",
      "iteration 8800 / 11340: loss 1.216384\n",
      "epoch done... acc 0.514\n",
      "iteration 8900 / 11340: loss 1.179759\n",
      "iteration 9000 / 11340: loss 1.183579\n",
      "iteration 9100 / 11340: loss 1.171222\n",
      "iteration 9200 / 11340: loss 1.065998\n",
      "iteration 9300 / 11340: loss 1.022288\n",
      "epoch done... acc 0.513\n",
      "iteration 9400 / 11340: loss 1.284386\n",
      "iteration 9500 / 11340: loss 1.036294\n",
      "iteration 9600 / 11340: loss 1.209029\n",
      "iteration 9700 / 11340: loss 1.133413\n",
      "iteration 9800 / 11340: loss 1.043869\n",
      "epoch done... acc 0.515\n",
      "iteration 9900 / 11340: loss 1.263559\n",
      "iteration 10000 / 11340: loss 1.041272\n",
      "iteration 10100 / 11340: loss 1.108438\n",
      "iteration 10200 / 11340: loss 0.980744\n",
      "epoch done... acc 0.521\n",
      "iteration 10300 / 11340: loss 1.086380\n",
      "iteration 10400 / 11340: loss 1.164530\n",
      "iteration 10500 / 11340: loss 1.101409\n",
      "iteration 10600 / 11340: loss 1.012922\n",
      "iteration 10700 / 11340: loss 1.009799\n",
      "epoch done... acc 0.518\n",
      "iteration 10800 / 11340: loss 1.066186\n",
      "iteration 10900 / 11340: loss 0.947770\n",
      "iteration 11000 / 11340: loss 1.153054\n",
      "iteration 11100 / 11340: loss 1.069538\n",
      "iteration 11200 / 11340: loss 0.913350\n",
      "epoch done... acc 0.519\n",
      "iteration 11300 / 11340: loss 1.027071\n",
      "Final training loss:  1.1204363419455132\n",
      "Final validation loss:  1.3887627294188056\n",
      "Final validation accuracy:  0.519\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "61 2 1 385 12600 100 0.001 0.98 0.519\n",
      "iteration 0 / 12600: loss 2.302574\n",
      "epoch done... acc 0.147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 12600: loss 1.948398\n",
      "iteration 200 / 12600: loss 1.941438\n",
      "iteration 300 / 12600: loss 1.978750\n",
      "iteration 400 / 12600: loss 1.651292\n",
      "epoch done... acc 0.396\n",
      "iteration 500 / 12600: loss 1.593689\n",
      "iteration 600 / 12600: loss 1.635682\n",
      "iteration 700 / 12600: loss 1.764666\n",
      "iteration 800 / 12600: loss 1.510366\n",
      "iteration 900 / 12600: loss 1.604331\n",
      "epoch done... acc 0.414\n",
      "iteration 1000 / 12600: loss 1.566968\n",
      "iteration 1100 / 12600: loss 1.637829\n",
      "iteration 1200 / 12600: loss 1.816541\n",
      "iteration 1300 / 12600: loss 1.651938\n",
      "iteration 1400 / 12600: loss 1.483293\n",
      "epoch done... acc 0.447\n",
      "iteration 1500 / 12600: loss 1.536331\n",
      "iteration 1600 / 12600: loss 1.569650\n",
      "iteration 1700 / 12600: loss 1.364073\n",
      "iteration 1800 / 12600: loss 1.390577\n",
      "iteration 1900 / 12600: loss 1.381118\n",
      "epoch done... acc 0.47\n",
      "iteration 2000 / 12600: loss 1.454331\n",
      "iteration 2100 / 12600: loss 1.267592\n",
      "iteration 2200 / 12600: loss 1.394057\n",
      "iteration 2300 / 12600: loss 1.434115\n",
      "iteration 2400 / 12600: loss 1.480842\n",
      "epoch done... acc 0.457\n",
      "iteration 2500 / 12600: loss 1.524205\n",
      "iteration 2600 / 12600: loss 1.391590\n",
      "iteration 2700 / 12600: loss 1.336384\n",
      "iteration 2800 / 12600: loss 1.467238\n",
      "iteration 2900 / 12600: loss 1.436801\n",
      "epoch done... acc 0.485\n",
      "iteration 3000 / 12600: loss 1.631414\n",
      "iteration 3100 / 12600: loss 1.313388\n",
      "iteration 3200 / 12600: loss 1.256866\n",
      "iteration 3300 / 12600: loss 1.462816\n",
      "iteration 3400 / 12600: loss 1.423937\n",
      "epoch done... acc 0.468\n",
      "iteration 3500 / 12600: loss 1.152737\n",
      "iteration 3600 / 12600: loss 1.246431\n",
      "iteration 3700 / 12600: loss 1.221441\n",
      "iteration 3800 / 12600: loss 1.517516\n",
      "iteration 3900 / 12600: loss 1.620074\n",
      "epoch done... acc 0.489\n",
      "iteration 4000 / 12600: loss 1.538505\n",
      "iteration 4100 / 12600: loss 1.396651\n",
      "iteration 4200 / 12600: loss 1.239484\n",
      "iteration 4300 / 12600: loss 1.114686\n",
      "iteration 4400 / 12600: loss 1.166226\n",
      "epoch done... acc 0.501\n",
      "iteration 4500 / 12600: loss 1.309674\n",
      "iteration 4600 / 12600: loss 1.407691\n",
      "iteration 4700 / 12600: loss 1.110330\n",
      "iteration 4800 / 12600: loss 1.290025\n",
      "iteration 4900 / 12600: loss 1.072928\n",
      "epoch done... acc 0.491\n",
      "iteration 5000 / 12600: loss 1.224162\n",
      "iteration 5100 / 12600: loss 1.358873\n",
      "iteration 5200 / 12600: loss 1.181057\n",
      "iteration 5300 / 12600: loss 1.312390\n",
      "epoch done... acc 0.501\n",
      "iteration 5400 / 12600: loss 1.231145\n",
      "iteration 5500 / 12600: loss 1.268202\n",
      "iteration 5600 / 12600: loss 1.451198\n",
      "iteration 5700 / 12600: loss 1.206592\n",
      "iteration 5800 / 12600: loss 1.593747\n",
      "epoch done... acc 0.492\n",
      "iteration 5900 / 12600: loss 1.265868\n",
      "iteration 6000 / 12600: loss 1.342586\n",
      "iteration 6100 / 12600: loss 1.248390\n",
      "iteration 6200 / 12600: loss 1.263446\n",
      "iteration 6300 / 12600: loss 1.085216\n",
      "epoch done... acc 0.502\n",
      "iteration 6400 / 12600: loss 1.472902\n",
      "iteration 6500 / 12600: loss 1.161659\n",
      "iteration 6600 / 12600: loss 1.123701\n",
      "iteration 6700 / 12600: loss 1.163205\n",
      "iteration 6800 / 12600: loss 1.008134\n",
      "epoch done... acc 0.515\n",
      "iteration 6900 / 12600: loss 1.251712\n",
      "iteration 7000 / 12600: loss 1.331368\n",
      "iteration 7100 / 12600: loss 1.187941\n",
      "iteration 7200 / 12600: loss 1.158900\n",
      "iteration 7300 / 12600: loss 1.085044\n",
      "epoch done... acc 0.515\n",
      "iteration 7400 / 12600: loss 1.223435\n",
      "iteration 7500 / 12600: loss 1.214328\n",
      "iteration 7600 / 12600: loss 1.178476\n",
      "iteration 7700 / 12600: loss 1.162900\n",
      "iteration 7800 / 12600: loss 1.144929\n",
      "epoch done... acc 0.528\n",
      "iteration 7900 / 12600: loss 1.094354\n",
      "iteration 8000 / 12600: loss 1.148739\n",
      "iteration 8100 / 12600: loss 1.095624\n",
      "iteration 8200 / 12600: loss 1.212153\n",
      "iteration 8300 / 12600: loss 1.299899\n",
      "epoch done... acc 0.501\n",
      "iteration 8400 / 12600: loss 1.156634\n",
      "iteration 8500 / 12600: loss 1.404097\n",
      "iteration 8600 / 12600: loss 1.220907\n",
      "iteration 8700 / 12600: loss 1.266401\n",
      "iteration 8800 / 12600: loss 1.196944\n",
      "epoch done... acc 0.51\n",
      "iteration 8900 / 12600: loss 1.179387\n",
      "iteration 9000 / 12600: loss 1.071500\n",
      "iteration 9100 / 12600: loss 1.086092\n",
      "iteration 9200 / 12600: loss 1.200772\n",
      "iteration 9300 / 12600: loss 1.146772\n",
      "epoch done... acc 0.523\n",
      "iteration 9400 / 12600: loss 0.969615\n",
      "iteration 9500 / 12600: loss 1.167456\n",
      "iteration 9600 / 12600: loss 1.079329\n",
      "iteration 9700 / 12600: loss 1.190909\n",
      "iteration 9800 / 12600: loss 1.092471\n",
      "epoch done... acc 0.506\n",
      "iteration 9900 / 12600: loss 1.233757\n",
      "iteration 10000 / 12600: loss 0.904461\n",
      "iteration 10100 / 12600: loss 0.932306\n",
      "iteration 10200 / 12600: loss 1.063722\n",
      "epoch done... acc 0.506\n",
      "iteration 10300 / 12600: loss 1.084301\n",
      "iteration 10400 / 12600: loss 1.143713\n",
      "iteration 10500 / 12600: loss 1.114906\n",
      "iteration 10600 / 12600: loss 1.177119\n",
      "iteration 10700 / 12600: loss 1.116136\n",
      "epoch done... acc 0.512\n",
      "iteration 10800 / 12600: loss 1.084105\n",
      "iteration 10900 / 12600: loss 1.154181\n",
      "iteration 11000 / 12600: loss 1.064279\n",
      "iteration 11100 / 12600: loss 1.163960\n",
      "iteration 11200 / 12600: loss 1.010907\n",
      "epoch done... acc 0.52\n",
      "iteration 11300 / 12600: loss 0.879338\n",
      "iteration 11400 / 12600: loss 1.114078\n",
      "iteration 11500 / 12600: loss 1.096062\n",
      "iteration 11600 / 12600: loss 1.198373\n",
      "iteration 11700 / 12600: loss 1.224026\n",
      "epoch done... acc 0.529\n",
      "iteration 11800 / 12600: loss 1.067814\n",
      "iteration 11900 / 12600: loss 1.021229\n",
      "iteration 12000 / 12600: loss 1.062703\n",
      "iteration 12100 / 12600: loss 0.965404\n",
      "iteration 12200 / 12600: loss 1.147050\n",
      "epoch done... acc 0.522\n",
      "iteration 12300 / 12600: loss 1.158387\n",
      "iteration 12400 / 12600: loss 0.893407\n",
      "iteration 12500 / 12600: loss 0.982710\n",
      "Final training loss:  1.2275586160815826\n",
      "Final validation loss:  1.3714257106226182\n",
      "Final validation accuracy:  0.522\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "62 1 0 385 12600 100 0.001 0.98 0.522\n",
      "iteration 0 / 12600: loss 2.302616\n",
      "epoch done... acc 0.142\n",
      "iteration 100 / 12600: loss 2.087715\n",
      "iteration 200 / 12600: loss 1.855354\n",
      "iteration 300 / 12600: loss 1.890888\n",
      "iteration 400 / 12600: loss 1.900557\n",
      "epoch done... acc 0.368\n",
      "iteration 500 / 12600: loss 1.791793\n",
      "iteration 600 / 12600: loss 1.813777\n",
      "iteration 700 / 12600: loss 1.781369\n",
      "iteration 800 / 12600: loss 1.597362\n",
      "iteration 900 / 12600: loss 1.685322\n",
      "epoch done... acc 0.415\n",
      "iteration 1000 / 12600: loss 1.685162\n",
      "iteration 1100 / 12600: loss 1.606412\n",
      "iteration 1200 / 12600: loss 1.616273\n",
      "iteration 1300 / 12600: loss 1.494772\n",
      "iteration 1400 / 12600: loss 1.550169\n",
      "epoch done... acc 0.43\n",
      "iteration 1500 / 12600: loss 1.615637\n",
      "iteration 1600 / 12600: loss 1.847545\n",
      "iteration 1700 / 12600: loss 1.473913\n",
      "iteration 1800 / 12600: loss 1.429159\n",
      "iteration 1900 / 12600: loss 1.471325\n",
      "epoch done... acc 0.458\n",
      "iteration 2000 / 12600: loss 1.616503\n",
      "iteration 2100 / 12600: loss 1.652332\n",
      "iteration 2200 / 12600: loss 1.528478\n",
      "iteration 2300 / 12600: loss 1.514821\n",
      "iteration 2400 / 12600: loss 1.547158\n",
      "epoch done... acc 0.464\n",
      "iteration 2500 / 12600: loss 1.422292\n",
      "iteration 2600 / 12600: loss 1.554936\n",
      "iteration 2700 / 12600: loss 1.567248\n",
      "iteration 2800 / 12600: loss 1.439413\n",
      "iteration 2900 / 12600: loss 1.488631\n",
      "epoch done... acc 0.466\n",
      "iteration 3000 / 12600: loss 1.493196\n",
      "iteration 3100 / 12600: loss 1.435181\n",
      "iteration 3200 / 12600: loss 1.527817\n",
      "iteration 3300 / 12600: loss 1.291334\n",
      "iteration 3400 / 12600: loss 1.444741\n",
      "epoch done... acc 0.481\n",
      "iteration 3500 / 12600: loss 1.324296\n",
      "iteration 3600 / 12600: loss 1.249995\n",
      "iteration 3700 / 12600: loss 1.338794\n",
      "iteration 3800 / 12600: loss 1.334014\n",
      "iteration 3900 / 12600: loss 1.150345\n",
      "epoch done... acc 0.495\n",
      "iteration 4000 / 12600: loss 1.539845\n",
      "iteration 4100 / 12600: loss 1.437476\n",
      "iteration 4200 / 12600: loss 1.400525\n",
      "iteration 4300 / 12600: loss 1.320205\n",
      "iteration 4400 / 12600: loss 1.304863\n",
      "epoch done... acc 0.484\n",
      "iteration 4500 / 12600: loss 1.355754\n",
      "iteration 4600 / 12600: loss 1.465882\n",
      "iteration 4700 / 12600: loss 1.246544\n",
      "iteration 4800 / 12600: loss 1.396896\n",
      "iteration 4900 / 12600: loss 1.317302\n",
      "epoch done... acc 0.485\n",
      "iteration 5000 / 12600: loss 1.418724\n",
      "iteration 5100 / 12600: loss 1.097081\n",
      "iteration 5200 / 12600: loss 1.415727\n",
      "iteration 5300 / 12600: loss 1.231425\n",
      "epoch done... acc 0.503\n",
      "iteration 5400 / 12600: loss 1.353357\n",
      "iteration 5500 / 12600: loss 1.133988\n",
      "iteration 5600 / 12600: loss 1.317768\n",
      "iteration 5700 / 12600: loss 1.304609\n",
      "iteration 5800 / 12600: loss 1.297376\n",
      "epoch done... acc 0.504\n",
      "iteration 5900 / 12600: loss 1.510154\n",
      "iteration 6000 / 12600: loss 1.498553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6100 / 12600: loss 1.361031\n",
      "iteration 6200 / 12600: loss 1.376728\n",
      "iteration 6300 / 12600: loss 1.196292\n",
      "epoch done... acc 0.499\n",
      "iteration 6400 / 12600: loss 1.278871\n",
      "iteration 6500 / 12600: loss 1.214931\n",
      "iteration 6600 / 12600: loss 1.098068\n",
      "iteration 6700 / 12600: loss 1.215715\n",
      "iteration 6800 / 12600: loss 1.323437\n",
      "epoch done... acc 0.51\n",
      "iteration 6900 / 12600: loss 1.310779\n",
      "iteration 7000 / 12600: loss 1.281246\n",
      "iteration 7100 / 12600: loss 1.219231\n",
      "iteration 7200 / 12600: loss 1.207696\n",
      "iteration 7300 / 12600: loss 1.123930\n",
      "epoch done... acc 0.514\n",
      "iteration 7400 / 12600: loss 1.116896\n",
      "iteration 7500 / 12600: loss 1.378044\n",
      "iteration 7600 / 12600: loss 1.149497\n",
      "iteration 7700 / 12600: loss 1.177607\n",
      "iteration 7800 / 12600: loss 1.179523\n",
      "epoch done... acc 0.508\n",
      "iteration 7900 / 12600: loss 1.373865\n",
      "iteration 8000 / 12600: loss 1.107752\n",
      "iteration 8100 / 12600: loss 1.015214\n",
      "iteration 8200 / 12600: loss 1.090462\n",
      "iteration 8300 / 12600: loss 1.038015\n",
      "epoch done... acc 0.515\n",
      "iteration 8400 / 12600: loss 1.174307\n",
      "iteration 8500 / 12600: loss 1.124502\n",
      "iteration 8600 / 12600: loss 1.135423\n",
      "iteration 8700 / 12600: loss 1.033378\n",
      "iteration 8800 / 12600: loss 1.054129\n",
      "epoch done... acc 0.525\n",
      "iteration 8900 / 12600: loss 1.221066\n",
      "iteration 9000 / 12600: loss 1.105963\n",
      "iteration 9100 / 12600: loss 1.142703\n",
      "iteration 9200 / 12600: loss 1.310082\n",
      "iteration 9300 / 12600: loss 1.099890\n",
      "epoch done... acc 0.508\n",
      "iteration 9400 / 12600: loss 1.127894\n",
      "iteration 9500 / 12600: loss 1.074254\n",
      "iteration 9600 / 12600: loss 1.067641\n",
      "iteration 9700 / 12600: loss 1.252121\n",
      "iteration 9800 / 12600: loss 0.998437\n",
      "epoch done... acc 0.511\n",
      "iteration 9900 / 12600: loss 1.168965\n",
      "iteration 10000 / 12600: loss 1.076819\n",
      "iteration 10100 / 12600: loss 1.128034\n",
      "iteration 10200 / 12600: loss 1.220341\n",
      "epoch done... acc 0.528\n",
      "iteration 10300 / 12600: loss 1.205010\n",
      "iteration 10400 / 12600: loss 1.204859\n",
      "iteration 10500 / 12600: loss 1.023381\n",
      "iteration 10600 / 12600: loss 1.087869\n",
      "iteration 10700 / 12600: loss 1.126401\n",
      "epoch done... acc 0.521\n",
      "iteration 10800 / 12600: loss 1.078908\n",
      "iteration 10900 / 12600: loss 1.110721\n",
      "iteration 11000 / 12600: loss 1.134306\n",
      "iteration 11100 / 12600: loss 1.152549\n",
      "iteration 11200 / 12600: loss 1.046074\n",
      "epoch done... acc 0.519\n",
      "iteration 11300 / 12600: loss 0.951857\n",
      "iteration 11400 / 12600: loss 1.064480\n",
      "iteration 11500 / 12600: loss 1.090364\n",
      "iteration 11600 / 12600: loss 1.047200\n",
      "iteration 11700 / 12600: loss 1.172779\n",
      "epoch done... acc 0.519\n",
      "iteration 11800 / 12600: loss 1.154324\n",
      "iteration 11900 / 12600: loss 0.933879\n",
      "iteration 12000 / 12600: loss 0.958638\n",
      "iteration 12100 / 12600: loss 0.979259\n",
      "iteration 12200 / 12600: loss 1.296755\n",
      "epoch done... acc 0.522\n",
      "iteration 12300 / 12600: loss 1.008361\n",
      "iteration 12400 / 12600: loss 1.104515\n",
      "iteration 12500 / 12600: loss 0.899876\n",
      "Final training loss:  0.9307419099833665\n",
      "Final validation loss:  1.3821944695111663\n",
      "Final validation accuracy:  0.522\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "63 2 0 385 12600 100 0.001 0.98 0.522\n",
      "iteration 0 / 13860: loss 2.302600\n",
      "epoch done... acc 0.151\n",
      "iteration 100 / 13860: loss 1.918590\n",
      "iteration 200 / 13860: loss 2.064778\n",
      "iteration 300 / 13860: loss 1.847290\n",
      "iteration 400 / 13860: loss 1.748640\n",
      "epoch done... acc 0.381\n",
      "iteration 500 / 13860: loss 1.810691\n",
      "iteration 600 / 13860: loss 1.729409\n",
      "iteration 700 / 13860: loss 1.543942\n",
      "iteration 800 / 13860: loss 1.615811\n",
      "iteration 900 / 13860: loss 1.473839\n",
      "epoch done... acc 0.428\n",
      "iteration 1000 / 13860: loss 1.639249\n",
      "iteration 1100 / 13860: loss 1.615653\n",
      "iteration 1200 / 13860: loss 1.629226\n",
      "iteration 1300 / 13860: loss 1.579723\n",
      "iteration 1400 / 13860: loss 1.514963\n",
      "epoch done... acc 0.456\n",
      "iteration 1500 / 13860: loss 1.493939\n",
      "iteration 1600 / 13860: loss 1.884434\n",
      "iteration 1700 / 13860: loss 1.434987\n",
      "iteration 1800 / 13860: loss 1.566284\n",
      "iteration 1900 / 13860: loss 1.447581\n",
      "epoch done... acc 0.453\n",
      "iteration 2000 / 13860: loss 1.438219\n",
      "iteration 2100 / 13860: loss 1.451064\n",
      "iteration 2200 / 13860: loss 1.534777\n",
      "iteration 2300 / 13860: loss 1.486165\n",
      "iteration 2400 / 13860: loss 1.337908\n",
      "epoch done... acc 0.468\n",
      "iteration 2500 / 13860: loss 1.351264\n",
      "iteration 2600 / 13860: loss 1.278176\n",
      "iteration 2700 / 13860: loss 1.578331\n",
      "iteration 2800 / 13860: loss 1.442797\n",
      "iteration 2900 / 13860: loss 1.418274\n",
      "epoch done... acc 0.474\n",
      "iteration 3000 / 13860: loss 1.218723\n",
      "iteration 3100 / 13860: loss 1.349752\n",
      "iteration 3200 / 13860: loss 1.354338\n",
      "iteration 3300 / 13860: loss 1.501126\n",
      "iteration 3400 / 13860: loss 1.455943\n",
      "epoch done... acc 0.48\n",
      "iteration 3500 / 13860: loss 1.415716\n",
      "iteration 3600 / 13860: loss 1.337633\n",
      "iteration 3700 / 13860: loss 1.512180\n",
      "iteration 3800 / 13860: loss 1.342605\n",
      "iteration 3900 / 13860: loss 1.225005\n",
      "epoch done... acc 0.504\n",
      "iteration 4000 / 13860: loss 1.377808\n",
      "iteration 4100 / 13860: loss 1.449179\n",
      "iteration 4200 / 13860: loss 1.236976\n",
      "iteration 4300 / 13860: loss 1.307765\n",
      "iteration 4400 / 13860: loss 1.445395\n",
      "epoch done... acc 0.498\n",
      "iteration 4500 / 13860: loss 1.177343\n",
      "iteration 4600 / 13860: loss 1.197326\n",
      "iteration 4700 / 13860: loss 1.331700\n",
      "iteration 4800 / 13860: loss 1.128808\n",
      "iteration 4900 / 13860: loss 1.311192\n",
      "epoch done... acc 0.494\n",
      "iteration 5000 / 13860: loss 1.113694\n",
      "iteration 5100 / 13860: loss 1.258529\n",
      "iteration 5200 / 13860: loss 1.384119\n",
      "iteration 5300 / 13860: loss 1.257985\n",
      "epoch done... acc 0.499\n",
      "iteration 5400 / 13860: loss 1.362667\n",
      "iteration 5500 / 13860: loss 1.308866\n",
      "iteration 5600 / 13860: loss 1.181053\n",
      "iteration 5700 / 13860: loss 1.487755\n",
      "iteration 5800 / 13860: loss 1.171248\n",
      "epoch done... acc 0.502\n",
      "iteration 5900 / 13860: loss 1.264838\n",
      "iteration 6000 / 13860: loss 1.170147\n",
      "iteration 6100 / 13860: loss 1.289074\n",
      "iteration 6200 / 13860: loss 1.219709\n",
      "iteration 6300 / 13860: loss 1.286933\n",
      "epoch done... acc 0.492\n",
      "iteration 6400 / 13860: loss 1.378179\n",
      "iteration 6500 / 13860: loss 1.060213\n",
      "iteration 6600 / 13860: loss 1.238621\n",
      "iteration 6700 / 13860: loss 1.337366\n",
      "iteration 6800 / 13860: loss 0.895413\n",
      "epoch done... acc 0.518\n",
      "iteration 6900 / 13860: loss 1.185701\n",
      "iteration 7000 / 13860: loss 1.198263\n",
      "iteration 7100 / 13860: loss 1.166534\n",
      "iteration 7200 / 13860: loss 1.151109\n",
      "iteration 7300 / 13860: loss 1.271744\n",
      "epoch done... acc 0.528\n",
      "iteration 7400 / 13860: loss 1.377750\n",
      "iteration 7500 / 13860: loss 1.390926\n",
      "iteration 7600 / 13860: loss 1.319660\n",
      "iteration 7700 / 13860: loss 1.143280\n",
      "iteration 7800 / 13860: loss 1.303857\n",
      "epoch done... acc 0.526\n",
      "iteration 7900 / 13860: loss 1.121520\n",
      "iteration 8000 / 13860: loss 1.266560\n",
      "iteration 8100 / 13860: loss 1.170645\n",
      "iteration 8200 / 13860: loss 1.212378\n",
      "iteration 8300 / 13860: loss 1.086852\n",
      "epoch done... acc 0.513\n",
      "iteration 8400 / 13860: loss 1.218116\n",
      "iteration 8500 / 13860: loss 1.186255\n",
      "iteration 8600 / 13860: loss 1.265419\n",
      "iteration 8700 / 13860: loss 1.041091\n",
      "iteration 8800 / 13860: loss 1.071300\n",
      "epoch done... acc 0.525\n",
      "iteration 8900 / 13860: loss 0.818115\n",
      "iteration 9000 / 13860: loss 1.156110\n",
      "iteration 9100 / 13860: loss 1.105851\n",
      "iteration 9200 / 13860: loss 1.049640\n",
      "iteration 9300 / 13860: loss 1.028842\n",
      "epoch done... acc 0.523\n",
      "iteration 9400 / 13860: loss 1.189816\n",
      "iteration 9500 / 13860: loss 1.225251\n",
      "iteration 9600 / 13860: loss 1.092076\n",
      "iteration 9700 / 13860: loss 1.106078\n",
      "iteration 9800 / 13860: loss 1.091191\n",
      "epoch done... acc 0.527\n",
      "iteration 9900 / 13860: loss 1.135898\n",
      "iteration 10000 / 13860: loss 1.009881\n",
      "iteration 10100 / 13860: loss 1.176721\n",
      "iteration 10200 / 13860: loss 1.081729\n",
      "epoch done... acc 0.523\n",
      "iteration 10300 / 13860: loss 1.136333\n",
      "iteration 10400 / 13860: loss 1.021499\n",
      "iteration 10500 / 13860: loss 1.129382\n",
      "iteration 10600 / 13860: loss 1.069444\n",
      "iteration 10700 / 13860: loss 1.100071\n",
      "epoch done... acc 0.518\n",
      "iteration 10800 / 13860: loss 1.061156\n",
      "iteration 10900 / 13860: loss 1.207695\n",
      "iteration 11000 / 13860: loss 1.087021\n",
      "iteration 11100 / 13860: loss 1.176916\n",
      "iteration 11200 / 13860: loss 1.135797\n",
      "epoch done... acc 0.534\n",
      "iteration 11300 / 13860: loss 1.015444\n",
      "iteration 11400 / 13860: loss 1.121885\n",
      "iteration 11500 / 13860: loss 1.207113\n",
      "iteration 11600 / 13860: loss 1.064514\n",
      "iteration 11700 / 13860: loss 0.969775\n",
      "epoch done... acc 0.524\n",
      "iteration 11800 / 13860: loss 1.173423\n",
      "iteration 11900 / 13860: loss 1.072161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 12000 / 13860: loss 1.020948\n",
      "iteration 12100 / 13860: loss 1.180877\n",
      "iteration 12200 / 13860: loss 1.057181\n",
      "epoch done... acc 0.518\n",
      "iteration 12300 / 13860: loss 1.084154\n",
      "iteration 12400 / 13860: loss 0.939040\n",
      "iteration 12500 / 13860: loss 1.083803\n",
      "iteration 12600 / 13860: loss 1.111145\n",
      "iteration 12700 / 13860: loss 1.291539\n",
      "epoch done... acc 0.522\n",
      "iteration 12800 / 13860: loss 0.893513\n",
      "iteration 12900 / 13860: loss 0.997705\n",
      "iteration 13000 / 13860: loss 0.984643\n",
      "iteration 13100 / 13860: loss 1.067893\n",
      "iteration 13200 / 13860: loss 1.085103\n",
      "epoch done... acc 0.518\n",
      "iteration 13300 / 13860: loss 0.956501\n",
      "iteration 13400 / 13860: loss 0.878348\n",
      "iteration 13500 / 13860: loss 0.983324\n",
      "iteration 13600 / 13860: loss 1.106417\n",
      "iteration 13700 / 13860: loss 1.030268\n",
      "epoch done... acc 0.524\n",
      "iteration 13800 / 13860: loss 1.186725\n",
      "Final training loss:  1.0588418768372403\n",
      "Final validation loss:  1.3841822450672996\n",
      "Final validation accuracy:  0.524\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "64 1 1 385 12600 100 0.001 0.98 0.524\n",
      "iteration 0 / 11340: loss 2.302657\n",
      "epoch done... acc 0.139\n",
      "iteration 100 / 11340: loss 1.893068\n",
      "iteration 200 / 11340: loss 1.958288\n",
      "iteration 300 / 11340: loss 1.823689\n",
      "iteration 400 / 11340: loss 1.791295\n",
      "epoch done... acc 0.371\n",
      "iteration 500 / 11340: loss 1.779597\n",
      "iteration 600 / 11340: loss 1.761958\n",
      "iteration 700 / 11340: loss 1.638013\n",
      "iteration 800 / 11340: loss 1.773078\n",
      "iteration 900 / 11340: loss 1.590150\n",
      "epoch done... acc 0.427\n",
      "iteration 1000 / 11340: loss 1.476187\n",
      "iteration 1100 / 11340: loss 1.468088\n",
      "iteration 1200 / 11340: loss 1.701829\n",
      "iteration 1300 / 11340: loss 1.620799\n",
      "iteration 1400 / 11340: loss 1.646293\n",
      "epoch done... acc 0.445\n",
      "iteration 1500 / 11340: loss 1.593789\n",
      "iteration 1600 / 11340: loss 1.618246\n",
      "iteration 1700 / 11340: loss 1.365295\n",
      "iteration 1800 / 11340: loss 1.632453\n",
      "iteration 1900 / 11340: loss 1.575587\n",
      "epoch done... acc 0.467\n",
      "iteration 2000 / 11340: loss 1.696653\n",
      "iteration 2100 / 11340: loss 1.361408\n",
      "iteration 2200 / 11340: loss 1.472515\n",
      "iteration 2300 / 11340: loss 1.566068\n",
      "iteration 2400 / 11340: loss 1.398994\n",
      "epoch done... acc 0.448\n",
      "iteration 2500 / 11340: loss 1.560735\n",
      "iteration 2600 / 11340: loss 1.410903\n",
      "iteration 2700 / 11340: loss 1.400515\n",
      "iteration 2800 / 11340: loss 1.385275\n",
      "iteration 2900 / 11340: loss 1.424075\n",
      "epoch done... acc 0.468\n",
      "iteration 3000 / 11340: loss 1.567261\n",
      "iteration 3100 / 11340: loss 1.527868\n",
      "iteration 3200 / 11340: loss 1.530880\n",
      "iteration 3300 / 11340: loss 1.599513\n",
      "iteration 3400 / 11340: loss 1.310887\n",
      "epoch done... acc 0.462\n",
      "iteration 3500 / 11340: loss 1.367951\n",
      "iteration 3600 / 11340: loss 1.374632\n",
      "iteration 3700 / 11340: loss 1.511319\n",
      "iteration 3800 / 11340: loss 1.642484\n",
      "iteration 3900 / 11340: loss 1.307908\n",
      "epoch done... acc 0.491\n",
      "iteration 4000 / 11340: loss 1.355171\n",
      "iteration 4100 / 11340: loss 1.311577\n",
      "iteration 4200 / 11340: loss 1.334238\n",
      "iteration 4300 / 11340: loss 1.134196\n",
      "iteration 4400 / 11340: loss 1.229994\n",
      "epoch done... acc 0.484\n",
      "iteration 4500 / 11340: loss 1.164043\n",
      "iteration 4600 / 11340: loss 1.337339\n",
      "iteration 4700 / 11340: loss 1.287881\n",
      "iteration 4800 / 11340: loss 1.381739\n",
      "iteration 4900 / 11340: loss 1.259691\n",
      "epoch done... acc 0.493\n",
      "iteration 5000 / 11340: loss 1.430342\n",
      "iteration 5100 / 11340: loss 1.148720\n",
      "iteration 5200 / 11340: loss 1.266624\n",
      "iteration 5300 / 11340: loss 1.220287\n",
      "epoch done... acc 0.494\n",
      "iteration 5400 / 11340: loss 1.376341\n",
      "iteration 5500 / 11340: loss 1.177253\n",
      "iteration 5600 / 11340: loss 1.294348\n",
      "iteration 5700 / 11340: loss 1.311484\n",
      "iteration 5800 / 11340: loss 1.308625\n",
      "epoch done... acc 0.511\n",
      "iteration 5900 / 11340: loss 1.303824\n",
      "iteration 6000 / 11340: loss 1.295851\n",
      "iteration 6100 / 11340: loss 1.466857\n",
      "iteration 6200 / 11340: loss 1.347350\n",
      "iteration 6300 / 11340: loss 1.377435\n",
      "epoch done... acc 0.512\n",
      "iteration 6400 / 11340: loss 1.234657\n",
      "iteration 6500 / 11340: loss 1.369477\n",
      "iteration 6600 / 11340: loss 1.318023\n",
      "iteration 6700 / 11340: loss 1.208492\n",
      "iteration 6800 / 11340: loss 1.176289\n",
      "epoch done... acc 0.507\n",
      "iteration 6900 / 11340: loss 1.461056\n",
      "iteration 7000 / 11340: loss 1.403098\n",
      "iteration 7100 / 11340: loss 1.220367\n",
      "iteration 7200 / 11340: loss 1.214850\n",
      "iteration 7300 / 11340: loss 1.395407\n",
      "epoch done... acc 0.527\n",
      "iteration 7400 / 11340: loss 1.236281\n",
      "iteration 7500 / 11340: loss 1.025845\n",
      "iteration 7600 / 11340: loss 1.153345\n",
      "iteration 7700 / 11340: loss 1.177722\n",
      "iteration 7800 / 11340: loss 1.087173\n",
      "epoch done... acc 0.522\n",
      "iteration 7900 / 11340: loss 1.288144\n",
      "iteration 8000 / 11340: loss 1.153087\n",
      "iteration 8100 / 11340: loss 1.353988\n",
      "iteration 8200 / 11340: loss 1.137508\n",
      "iteration 8300 / 11340: loss 1.353332\n",
      "epoch done... acc 0.529\n",
      "iteration 8400 / 11340: loss 1.174710\n",
      "iteration 8500 / 11340: loss 1.255668\n",
      "iteration 8600 / 11340: loss 1.292084\n",
      "iteration 8700 / 11340: loss 1.043592\n",
      "iteration 8800 / 11340: loss 1.153840\n",
      "epoch done... acc 0.52\n",
      "iteration 8900 / 11340: loss 1.108892\n",
      "iteration 9000 / 11340: loss 1.158633\n",
      "iteration 9100 / 11340: loss 1.207705\n",
      "iteration 9200 / 11340: loss 1.085483\n",
      "iteration 9300 / 11340: loss 1.103122\n",
      "epoch done... acc 0.528\n",
      "iteration 9400 / 11340: loss 1.151899\n",
      "iteration 9500 / 11340: loss 1.194468\n",
      "iteration 9600 / 11340: loss 1.092679\n",
      "iteration 9700 / 11340: loss 1.124368\n",
      "iteration 9800 / 11340: loss 1.199142\n",
      "epoch done... acc 0.518\n",
      "iteration 9900 / 11340: loss 1.185396\n",
      "iteration 10000 / 11340: loss 1.216578\n",
      "iteration 10100 / 11340: loss 1.084856\n",
      "iteration 10200 / 11340: loss 0.825218\n",
      "epoch done... acc 0.514\n",
      "iteration 10300 / 11340: loss 1.079586\n",
      "iteration 10400 / 11340: loss 1.228174\n",
      "iteration 10500 / 11340: loss 1.154684\n",
      "iteration 10600 / 11340: loss 0.966526\n",
      "iteration 10700 / 11340: loss 1.118767\n",
      "epoch done... acc 0.533\n",
      "iteration 10800 / 11340: loss 0.785646\n",
      "iteration 10900 / 11340: loss 1.211616\n",
      "iteration 11000 / 11340: loss 1.090996\n",
      "iteration 11100 / 11340: loss 1.342200\n",
      "iteration 11200 / 11340: loss 1.120975\n",
      "epoch done... acc 0.528\n",
      "iteration 11300 / 11340: loss 1.002932\n",
      "Final training loss:  1.0551567035581408\n",
      "Final validation loss:  1.3512782250952207\n",
      "Final validation accuracy:  0.528\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "65 2 1 385 12600 100 0.001 0.98 0.528\n",
      "iteration 0 / 12600: loss 2.302573\n",
      "epoch done... acc 0.201\n",
      "iteration 100 / 12600: loss 1.947904\n",
      "iteration 200 / 12600: loss 1.963221\n",
      "iteration 300 / 12600: loss 1.844347\n",
      "iteration 400 / 12600: loss 1.625186\n",
      "epoch done... acc 0.389\n",
      "iteration 500 / 12600: loss 1.585725\n",
      "iteration 600 / 12600: loss 1.668794\n",
      "iteration 700 / 12600: loss 1.774993\n",
      "iteration 800 / 12600: loss 1.770262\n",
      "iteration 900 / 12600: loss 1.671774\n",
      "epoch done... acc 0.42\n",
      "iteration 1000 / 12600: loss 1.696493\n",
      "iteration 1100 / 12600: loss 1.676776\n",
      "iteration 1200 / 12600: loss 1.601294\n",
      "iteration 1300 / 12600: loss 1.509584\n",
      "iteration 1400 / 12600: loss 1.611536\n",
      "epoch done... acc 0.451\n",
      "iteration 1500 / 12600: loss 1.466585\n",
      "iteration 1600 / 12600: loss 1.448482\n",
      "iteration 1700 / 12600: loss 1.447916\n",
      "iteration 1800 / 12600: loss 1.486266\n",
      "iteration 1900 / 12600: loss 1.570017\n",
      "epoch done... acc 0.465\n",
      "iteration 2000 / 12600: loss 1.465702\n",
      "iteration 2100 / 12600: loss 1.413819\n",
      "iteration 2200 / 12600: loss 1.433307\n",
      "iteration 2300 / 12600: loss 1.392866\n",
      "iteration 2400 / 12600: loss 1.458656\n",
      "epoch done... acc 0.465\n",
      "iteration 2500 / 12600: loss 1.339359\n",
      "iteration 2600 / 12600: loss 1.325463\n",
      "iteration 2700 / 12600: loss 1.410212\n",
      "iteration 2800 / 12600: loss 1.521620\n",
      "iteration 2900 / 12600: loss 1.514097\n",
      "epoch done... acc 0.487\n",
      "iteration 3000 / 12600: loss 1.260793\n",
      "iteration 3100 / 12600: loss 1.286978\n",
      "iteration 3200 / 12600: loss 1.385337\n",
      "iteration 3300 / 12600: loss 1.484534\n",
      "iteration 3400 / 12600: loss 1.486534\n",
      "epoch done... acc 0.496\n",
      "iteration 3500 / 12600: loss 1.395441\n",
      "iteration 3600 / 12600: loss 1.389779\n",
      "iteration 3700 / 12600: loss 1.339460\n",
      "iteration 3800 / 12600: loss 1.335078\n",
      "iteration 3900 / 12600: loss 1.291757\n",
      "epoch done... acc 0.487\n",
      "iteration 4000 / 12600: loss 1.416406\n",
      "iteration 4100 / 12600: loss 1.329906\n",
      "iteration 4200 / 12600: loss 1.192037\n",
      "iteration 4300 / 12600: loss 1.270063\n",
      "iteration 4400 / 12600: loss 1.217866\n",
      "epoch done... acc 0.489\n",
      "iteration 4500 / 12600: loss 1.468662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4600 / 12600: loss 1.213222\n",
      "iteration 4700 / 12600: loss 1.279824\n",
      "iteration 4800 / 12600: loss 1.337751\n",
      "iteration 4900 / 12600: loss 1.231271\n",
      "epoch done... acc 0.501\n",
      "iteration 5000 / 12600: loss 1.249397\n",
      "iteration 5100 / 12600: loss 1.469850\n",
      "iteration 5200 / 12600: loss 1.258961\n",
      "iteration 5300 / 12600: loss 1.376898\n",
      "epoch done... acc 0.507\n",
      "iteration 5400 / 12600: loss 1.208018\n",
      "iteration 5500 / 12600: loss 1.292109\n",
      "iteration 5600 / 12600: loss 1.351633\n",
      "iteration 5700 / 12600: loss 1.232534\n",
      "iteration 5800 / 12600: loss 1.428141\n",
      "epoch done... acc 0.509\n",
      "iteration 5900 / 12600: loss 1.327765\n",
      "iteration 6000 / 12600: loss 1.087996\n",
      "iteration 6100 / 12600: loss 1.176982\n",
      "iteration 6200 / 12600: loss 1.180675\n",
      "iteration 6300 / 12600: loss 1.283718\n",
      "epoch done... acc 0.499\n",
      "iteration 6400 / 12600: loss 1.337153\n",
      "iteration 6500 / 12600: loss 1.352543\n",
      "iteration 6600 / 12600: loss 1.129757\n",
      "iteration 6700 / 12600: loss 1.023567\n",
      "iteration 6800 / 12600: loss 1.303668\n",
      "epoch done... acc 0.521\n",
      "iteration 6900 / 12600: loss 1.367482\n",
      "iteration 7000 / 12600: loss 1.273125\n",
      "iteration 7100 / 12600: loss 1.237470\n",
      "iteration 7200 / 12600: loss 1.169164\n",
      "iteration 7300 / 12600: loss 1.231845\n",
      "epoch done... acc 0.506\n",
      "iteration 7400 / 12600: loss 1.302681\n",
      "iteration 7500 / 12600: loss 1.233220\n",
      "iteration 7600 / 12600: loss 1.017197\n",
      "iteration 7700 / 12600: loss 1.208850\n",
      "iteration 7800 / 12600: loss 1.264240\n",
      "epoch done... acc 0.51\n",
      "iteration 7900 / 12600: loss 1.315841\n",
      "iteration 8000 / 12600: loss 1.237095\n",
      "iteration 8100 / 12600: loss 1.120389\n",
      "iteration 8200 / 12600: loss 1.031209\n",
      "iteration 8300 / 12600: loss 1.056107\n",
      "epoch done... acc 0.516\n",
      "iteration 8400 / 12600: loss 1.243425\n",
      "iteration 8500 / 12600: loss 1.076014\n",
      "iteration 8600 / 12600: loss 1.371618\n",
      "iteration 8700 / 12600: loss 0.943534\n",
      "iteration 8800 / 12600: loss 1.167026\n",
      "epoch done... acc 0.519\n",
      "iteration 8900 / 12600: loss 1.113119\n",
      "iteration 9000 / 12600: loss 1.070298\n",
      "iteration 9100 / 12600: loss 1.061699\n",
      "iteration 9200 / 12600: loss 1.091323\n",
      "iteration 9300 / 12600: loss 1.154581\n",
      "epoch done... acc 0.508\n",
      "iteration 9400 / 12600: loss 1.195679\n",
      "iteration 9500 / 12600: loss 1.292433\n",
      "iteration 9600 / 12600: loss 0.976172\n",
      "iteration 9700 / 12600: loss 1.140496\n",
      "iteration 9800 / 12600: loss 1.112241\n",
      "epoch done... acc 0.525\n",
      "iteration 9900 / 12600: loss 1.364962\n",
      "iteration 10000 / 12600: loss 1.063926\n",
      "iteration 10100 / 12600: loss 0.929903\n",
      "iteration 10200 / 12600: loss 1.095083\n",
      "epoch done... acc 0.522\n",
      "iteration 10300 / 12600: loss 1.016533\n",
      "iteration 10400 / 12600: loss 1.183543\n",
      "iteration 10500 / 12600: loss 0.966394\n",
      "iteration 10600 / 12600: loss 1.089161\n",
      "iteration 10700 / 12600: loss 1.010540\n",
      "epoch done... acc 0.516\n",
      "iteration 10800 / 12600: loss 1.190562\n",
      "iteration 10900 / 12600: loss 1.099512\n",
      "iteration 11000 / 12600: loss 1.029276\n",
      "iteration 11100 / 12600: loss 1.118668\n",
      "iteration 11200 / 12600: loss 1.094107\n",
      "epoch done... acc 0.525\n",
      "iteration 11300 / 12600: loss 1.404609\n",
      "iteration 11400 / 12600: loss 0.942396\n",
      "iteration 11500 / 12600: loss 0.975387\n",
      "iteration 11600 / 12600: loss 0.931901\n",
      "iteration 11700 / 12600: loss 1.101835\n",
      "epoch done... acc 0.527\n",
      "iteration 11800 / 12600: loss 0.933478\n",
      "iteration 11900 / 12600: loss 0.986592\n",
      "iteration 12000 / 12600: loss 1.049538\n",
      "iteration 12100 / 12600: loss 1.079057\n",
      "iteration 12200 / 12600: loss 1.114784\n",
      "epoch done... acc 0.521\n",
      "iteration 12300 / 12600: loss 1.208489\n",
      "iteration 12400 / 12600: loss 1.074329\n",
      "iteration 12500 / 12600: loss 1.127008\n",
      "Final training loss:  1.1993648593932675\n",
      "Final validation loss:  1.3587105987106494\n",
      "Final validation accuracy:  0.521\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "66 1 0 385 12600 100 0.001 0.98 0.521\n",
      "iteration 0 / 12600: loss 2.302560\n",
      "epoch done... acc 0.141\n",
      "iteration 100 / 12600: loss 2.094037\n",
      "iteration 200 / 12600: loss 1.837265\n",
      "iteration 300 / 12600: loss 1.787858\n",
      "iteration 400 / 12600: loss 1.763170\n",
      "epoch done... acc 0.396\n",
      "iteration 500 / 12600: loss 1.640092\n",
      "iteration 600 / 12600: loss 1.734206\n",
      "iteration 700 / 12600: loss 1.689372\n",
      "iteration 800 / 12600: loss 1.429106\n",
      "iteration 900 / 12600: loss 1.668774\n",
      "epoch done... acc 0.423\n",
      "iteration 1000 / 12600: loss 1.544430\n",
      "iteration 1100 / 12600: loss 1.637019\n",
      "iteration 1200 / 12600: loss 1.481123\n",
      "iteration 1300 / 12600: loss 1.608476\n",
      "iteration 1400 / 12600: loss 1.617708\n",
      "epoch done... acc 0.443\n",
      "iteration 1500 / 12600: loss 1.700582\n",
      "iteration 1600 / 12600: loss 1.704896\n",
      "iteration 1700 / 12600: loss 1.759630\n",
      "iteration 1800 / 12600: loss 1.552482\n",
      "iteration 1900 / 12600: loss 1.403280\n",
      "epoch done... acc 0.452\n",
      "iteration 2000 / 12600: loss 1.446838\n",
      "iteration 2100 / 12600: loss 1.422454\n",
      "iteration 2200 / 12600: loss 1.583351\n",
      "iteration 2300 / 12600: loss 1.340883\n",
      "iteration 2400 / 12600: loss 1.453944\n",
      "epoch done... acc 0.462\n",
      "iteration 2500 / 12600: loss 1.431833\n",
      "iteration 2600 / 12600: loss 1.426582\n",
      "iteration 2700 / 12600: loss 1.434364\n",
      "iteration 2800 / 12600: loss 1.347534\n",
      "iteration 2900 / 12600: loss 1.446934\n",
      "epoch done... acc 0.469\n",
      "iteration 3000 / 12600: loss 1.315829\n",
      "iteration 3100 / 12600: loss 1.509089\n",
      "iteration 3200 / 12600: loss 1.404291\n",
      "iteration 3300 / 12600: loss 1.524380\n",
      "iteration 3400 / 12600: loss 1.295220\n",
      "epoch done... acc 0.485\n",
      "iteration 3500 / 12600: loss 1.336541\n",
      "iteration 3600 / 12600: loss 1.347289\n",
      "iteration 3700 / 12600: loss 1.437505\n",
      "iteration 3800 / 12600: loss 1.633174\n",
      "iteration 3900 / 12600: loss 1.389191\n",
      "epoch done... acc 0.488\n",
      "iteration 4000 / 12600: loss 1.438058\n",
      "iteration 4100 / 12600: loss 1.441228\n",
      "iteration 4200 / 12600: loss 1.371591\n",
      "iteration 4300 / 12600: loss 1.364950\n",
      "iteration 4400 / 12600: loss 1.334473\n",
      "epoch done... acc 0.491\n",
      "iteration 4500 / 12600: loss 1.379770\n",
      "iteration 4600 / 12600: loss 1.608669\n",
      "iteration 4700 / 12600: loss 1.276528\n",
      "iteration 4800 / 12600: loss 1.314750\n",
      "iteration 4900 / 12600: loss 1.378194\n",
      "epoch done... acc 0.509\n",
      "iteration 5000 / 12600: loss 1.233285\n",
      "iteration 5100 / 12600: loss 1.154434\n",
      "iteration 5200 / 12600: loss 1.304875\n",
      "iteration 5300 / 12600: loss 1.319986\n",
      "epoch done... acc 0.497\n",
      "iteration 5400 / 12600: loss 1.280345\n",
      "iteration 5500 / 12600: loss 1.165807\n",
      "iteration 5600 / 12600: loss 1.467499\n",
      "iteration 5700 / 12600: loss 1.191091\n",
      "iteration 5800 / 12600: loss 1.414929\n",
      "epoch done... acc 0.495\n",
      "iteration 5900 / 12600: loss 1.217262\n",
      "iteration 6000 / 12600: loss 1.313942\n",
      "iteration 6100 / 12600: loss 1.280878\n",
      "iteration 6200 / 12600: loss 1.458917\n",
      "iteration 6300 / 12600: loss 1.293285\n",
      "epoch done... acc 0.499\n",
      "iteration 6400 / 12600: loss 1.334810\n",
      "iteration 6500 / 12600: loss 1.162332\n",
      "iteration 6600 / 12600: loss 1.207360\n",
      "iteration 6700 / 12600: loss 1.221058\n",
      "iteration 6800 / 12600: loss 1.194238\n",
      "epoch done... acc 0.487\n",
      "iteration 6900 / 12600: loss 1.241306\n",
      "iteration 7000 / 12600: loss 1.163093\n",
      "iteration 7100 / 12600: loss 1.330055\n",
      "iteration 7200 / 12600: loss 1.019241\n",
      "iteration 7300 / 12600: loss 1.423021\n",
      "epoch done... acc 0.502\n",
      "iteration 7400 / 12600: loss 1.178000\n",
      "iteration 7500 / 12600: loss 1.191892\n",
      "iteration 7600 / 12600: loss 1.380468\n",
      "iteration 7700 / 12600: loss 1.197275\n",
      "iteration 7800 / 12600: loss 1.298896\n",
      "epoch done... acc 0.508\n",
      "iteration 7900 / 12600: loss 1.079419\n",
      "iteration 8000 / 12600: loss 1.344249\n",
      "iteration 8100 / 12600: loss 1.227660\n",
      "iteration 8200 / 12600: loss 1.342056\n",
      "iteration 8300 / 12600: loss 1.321035\n",
      "epoch done... acc 0.507\n",
      "iteration 8400 / 12600: loss 1.450411\n",
      "iteration 8500 / 12600: loss 1.267171\n",
      "iteration 8600 / 12600: loss 1.114627\n",
      "iteration 8700 / 12600: loss 1.212353\n",
      "iteration 8800 / 12600: loss 1.298385\n",
      "epoch done... acc 0.511\n",
      "iteration 8900 / 12600: loss 1.131388\n",
      "iteration 9000 / 12600: loss 1.222742\n",
      "iteration 9100 / 12600: loss 1.217289\n",
      "iteration 9200 / 12600: loss 1.296247\n",
      "iteration 9300 / 12600: loss 1.124979\n",
      "epoch done... acc 0.517\n",
      "iteration 9400 / 12600: loss 0.952570\n",
      "iteration 9500 / 12600: loss 1.143571\n",
      "iteration 9600 / 12600: loss 1.336353\n",
      "iteration 9700 / 12600: loss 1.201541\n",
      "iteration 9800 / 12600: loss 1.110580\n",
      "epoch done... acc 0.521\n",
      "iteration 9900 / 12600: loss 1.275722\n",
      "iteration 10000 / 12600: loss 1.064288\n",
      "iteration 10100 / 12600: loss 1.144544\n",
      "iteration 10200 / 12600: loss 1.079067\n",
      "epoch done... acc 0.502\n",
      "iteration 10300 / 12600: loss 1.085231\n",
      "iteration 10400 / 12600: loss 1.146896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10500 / 12600: loss 1.295974\n",
      "iteration 10600 / 12600: loss 1.264462\n",
      "iteration 10700 / 12600: loss 1.190515\n",
      "epoch done... acc 0.512\n",
      "iteration 10800 / 12600: loss 0.962960\n",
      "iteration 10900 / 12600: loss 0.941347\n",
      "iteration 11000 / 12600: loss 1.112008\n",
      "iteration 11100 / 12600: loss 1.096233\n",
      "iteration 11200 / 12600: loss 1.233291\n",
      "epoch done... acc 0.522\n",
      "iteration 11300 / 12600: loss 1.198760\n",
      "iteration 11400 / 12600: loss 1.132675\n",
      "iteration 11500 / 12600: loss 1.106096\n",
      "iteration 11600 / 12600: loss 1.193382\n",
      "iteration 11700 / 12600: loss 1.217450\n",
      "epoch done... acc 0.512\n",
      "iteration 11800 / 12600: loss 1.086690\n",
      "iteration 11900 / 12600: loss 1.149690\n",
      "iteration 12000 / 12600: loss 0.997557\n",
      "iteration 12100 / 12600: loss 1.188787\n",
      "iteration 12200 / 12600: loss 1.006731\n",
      "epoch done... acc 0.513\n",
      "iteration 12300 / 12600: loss 1.013224\n",
      "iteration 12400 / 12600: loss 0.992056\n",
      "iteration 12500 / 12600: loss 1.106486\n",
      "Final training loss:  1.0773657359174256\n",
      "Final validation loss:  1.3782534582674466\n",
      "Final validation accuracy:  0.513\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "67 2 0 385 12600 100 0.001 0.98 0.513\n",
      "iteration 0 / 13860: loss 2.302584\n",
      "epoch done... acc 0.089\n",
      "iteration 100 / 13860: loss 2.007406\n",
      "iteration 200 / 13860: loss 1.961423\n",
      "iteration 300 / 13860: loss 1.848920\n",
      "iteration 400 / 13860: loss 1.963520\n",
      "epoch done... acc 0.388\n",
      "iteration 500 / 13860: loss 1.790416\n",
      "iteration 600 / 13860: loss 1.538618\n",
      "iteration 700 / 13860: loss 1.695772\n",
      "iteration 800 / 13860: loss 1.670446\n",
      "iteration 900 / 13860: loss 1.860506\n",
      "epoch done... acc 0.431\n",
      "iteration 1000 / 13860: loss 1.474259\n",
      "iteration 1100 / 13860: loss 1.572208\n",
      "iteration 1200 / 13860: loss 1.444383\n",
      "iteration 1300 / 13860: loss 1.670059\n",
      "iteration 1400 / 13860: loss 1.475138\n",
      "epoch done... acc 0.448\n",
      "iteration 1500 / 13860: loss 1.714178\n",
      "iteration 1600 / 13860: loss 1.644012\n",
      "iteration 1700 / 13860: loss 1.532257\n",
      "iteration 1800 / 13860: loss 1.364174\n",
      "iteration 1900 / 13860: loss 1.558984\n",
      "epoch done... acc 0.47\n",
      "iteration 2000 / 13860: loss 1.338040\n",
      "iteration 2100 / 13860: loss 1.444260\n",
      "iteration 2200 / 13860: loss 1.474152\n",
      "iteration 2300 / 13860: loss 1.624230\n",
      "iteration 2400 / 13860: loss 1.520832\n",
      "epoch done... acc 0.466\n",
      "iteration 2500 / 13860: loss 1.488306\n",
      "iteration 2600 / 13860: loss 1.441139\n",
      "iteration 2700 / 13860: loss 1.507745\n",
      "iteration 2800 / 13860: loss 1.398784\n",
      "iteration 2900 / 13860: loss 1.357765\n",
      "epoch done... acc 0.478\n",
      "iteration 3000 / 13860: loss 1.399515\n",
      "iteration 3100 / 13860: loss 1.497168\n",
      "iteration 3200 / 13860: loss 1.458891\n",
      "iteration 3300 / 13860: loss 1.592816\n",
      "iteration 3400 / 13860: loss 1.263235\n",
      "epoch done... acc 0.49\n",
      "iteration 3500 / 13860: loss 1.336091\n",
      "iteration 3600 / 13860: loss 1.474385\n",
      "iteration 3700 / 13860: loss 1.262633\n",
      "iteration 3800 / 13860: loss 1.412671\n",
      "iteration 3900 / 13860: loss 1.321249\n",
      "epoch done... acc 0.483\n",
      "iteration 4000 / 13860: loss 1.376630\n",
      "iteration 4100 / 13860: loss 1.256388\n",
      "iteration 4200 / 13860: loss 1.289860\n",
      "iteration 4300 / 13860: loss 1.297050\n",
      "iteration 4400 / 13860: loss 1.436312\n",
      "epoch done... acc 0.501\n",
      "iteration 4500 / 13860: loss 1.673480\n",
      "iteration 4600 / 13860: loss 1.291987\n",
      "iteration 4700 / 13860: loss 1.456797\n",
      "iteration 4800 / 13860: loss 1.335129\n",
      "iteration 4900 / 13860: loss 1.180401\n",
      "epoch done... acc 0.507\n",
      "iteration 5000 / 13860: loss 1.362535\n",
      "iteration 5100 / 13860: loss 1.310472\n",
      "iteration 5200 / 13860: loss 1.391662\n",
      "iteration 5300 / 13860: loss 1.423224\n",
      "epoch done... acc 0.513\n",
      "iteration 5400 / 13860: loss 1.204828\n",
      "iteration 5500 / 13860: loss 1.185321\n",
      "iteration 5600 / 13860: loss 1.370389\n",
      "iteration 5700 / 13860: loss 1.152891\n",
      "iteration 5800 / 13860: loss 1.251177\n",
      "epoch done... acc 0.493\n",
      "iteration 5900 / 13860: loss 1.263408\n",
      "iteration 6000 / 13860: loss 1.265285\n",
      "iteration 6100 / 13860: loss 1.226464\n",
      "iteration 6200 / 13860: loss 1.161253\n",
      "iteration 6300 / 13860: loss 1.171061\n",
      "epoch done... acc 0.509\n",
      "iteration 6400 / 13860: loss 1.230724\n",
      "iteration 6500 / 13860: loss 1.223037\n",
      "iteration 6600 / 13860: loss 1.295150\n",
      "iteration 6700 / 13860: loss 1.126546\n",
      "iteration 6800 / 13860: loss 1.367030\n",
      "epoch done... acc 0.505\n",
      "iteration 6900 / 13860: loss 1.123495\n",
      "iteration 7000 / 13860: loss 1.166934\n",
      "iteration 7100 / 13860: loss 1.233669\n",
      "iteration 7200 / 13860: loss 1.254389\n",
      "iteration 7300 / 13860: loss 1.436810\n",
      "epoch done... acc 0.513\n",
      "iteration 7400 / 13860: loss 1.256217\n",
      "iteration 7500 / 13860: loss 1.188677\n",
      "iteration 7600 / 13860: loss 1.235824\n",
      "iteration 7700 / 13860: loss 1.282501\n",
      "iteration 7800 / 13860: loss 1.226628\n",
      "epoch done... acc 0.5\n",
      "iteration 7900 / 13860: loss 1.145817\n",
      "iteration 8000 / 13860: loss 1.403931\n",
      "iteration 8100 / 13860: loss 1.172566\n",
      "iteration 8200 / 13860: loss 1.095727\n",
      "iteration 8300 / 13860: loss 1.199678\n",
      "epoch done... acc 0.497\n",
      "iteration 8400 / 13860: loss 1.128025\n",
      "iteration 8500 / 13860: loss 1.158315\n",
      "iteration 8600 / 13860: loss 1.027880\n",
      "iteration 8700 / 13860: loss 1.090568\n",
      "iteration 8800 / 13860: loss 1.143991\n",
      "epoch done... acc 0.508\n",
      "iteration 8900 / 13860: loss 1.158510\n",
      "iteration 9000 / 13860: loss 1.201817\n",
      "iteration 9100 / 13860: loss 0.957874\n",
      "iteration 9200 / 13860: loss 1.051224\n",
      "iteration 9300 / 13860: loss 0.951417\n",
      "epoch done... acc 0.505\n",
      "iteration 9400 / 13860: loss 1.185135\n",
      "iteration 9500 / 13860: loss 1.193367\n",
      "iteration 9600 / 13860: loss 1.313433\n",
      "iteration 9700 / 13860: loss 1.283214\n",
      "iteration 9800 / 13860: loss 1.167301\n",
      "epoch done... acc 0.516\n",
      "iteration 9900 / 13860: loss 1.387295\n",
      "iteration 10000 / 13860: loss 1.220784\n",
      "iteration 10100 / 13860: loss 1.128278\n",
      "iteration 10200 / 13860: loss 0.884013\n",
      "epoch done... acc 0.51\n",
      "iteration 10300 / 13860: loss 0.988538\n",
      "iteration 10400 / 13860: loss 1.013066\n",
      "iteration 10500 / 13860: loss 1.257950\n",
      "iteration 10600 / 13860: loss 1.206847\n",
      "iteration 10700 / 13860: loss 1.221534\n",
      "epoch done... acc 0.522\n",
      "iteration 10800 / 13860: loss 1.057377\n",
      "iteration 10900 / 13860: loss 1.053607\n",
      "iteration 11000 / 13860: loss 1.040921\n",
      "iteration 11100 / 13860: loss 1.212598\n",
      "iteration 11200 / 13860: loss 1.117763\n",
      "epoch done... acc 0.528\n",
      "iteration 11300 / 13860: loss 1.122630\n",
      "iteration 11400 / 13860: loss 1.178834\n",
      "iteration 11500 / 13860: loss 1.153670\n",
      "iteration 11600 / 13860: loss 1.057814\n",
      "iteration 11700 / 13860: loss 1.240881\n",
      "epoch done... acc 0.523\n",
      "iteration 11800 / 13860: loss 0.957070\n",
      "iteration 11900 / 13860: loss 1.100744\n",
      "iteration 12000 / 13860: loss 1.095718\n",
      "iteration 12100 / 13860: loss 1.008703\n",
      "iteration 12200 / 13860: loss 0.899544\n",
      "epoch done... acc 0.528\n",
      "iteration 12300 / 13860: loss 1.108008\n",
      "iteration 12400 / 13860: loss 0.988277\n",
      "iteration 12500 / 13860: loss 1.049416\n",
      "iteration 12600 / 13860: loss 1.037070\n",
      "iteration 12700 / 13860: loss 1.014857\n",
      "epoch done... acc 0.524\n",
      "iteration 12800 / 13860: loss 0.986650\n",
      "iteration 12900 / 13860: loss 1.021402\n",
      "iteration 13000 / 13860: loss 1.006314\n",
      "iteration 13100 / 13860: loss 1.131700\n",
      "iteration 13200 / 13860: loss 1.111042\n",
      "epoch done... acc 0.521\n",
      "iteration 13300 / 13860: loss 0.979922\n",
      "iteration 13400 / 13860: loss 0.949710\n",
      "iteration 13500 / 13860: loss 1.126112\n",
      "iteration 13600 / 13860: loss 0.876097\n",
      "iteration 13700 / 13860: loss 1.002397\n",
      "epoch done... acc 0.536\n",
      "iteration 13800 / 13860: loss 1.109453\n",
      "Final training loss:  0.9446225856759364\n",
      "Final validation loss:  1.3575344422965363\n",
      "Final validation accuracy:  0.536\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "68 1 1 385 12600 100 0.001 0.98 0.536\n",
      "iteration 0 / 11340: loss 2.302499\n",
      "epoch done... acc 0.142\n",
      "iteration 100 / 11340: loss 1.892426\n",
      "iteration 200 / 11340: loss 1.927856\n",
      "iteration 300 / 11340: loss 1.838068\n",
      "iteration 400 / 11340: loss 1.866724\n",
      "epoch done... acc 0.377\n",
      "iteration 500 / 11340: loss 1.771999\n",
      "iteration 600 / 11340: loss 1.794680\n",
      "iteration 700 / 11340: loss 1.661075\n",
      "iteration 800 / 11340: loss 1.661718\n",
      "iteration 900 / 11340: loss 1.654800\n",
      "epoch done... acc 0.423\n",
      "iteration 1000 / 11340: loss 1.688278\n",
      "iteration 1100 / 11340: loss 1.772448\n",
      "iteration 1200 / 11340: loss 1.482082\n",
      "iteration 1300 / 11340: loss 1.676781\n",
      "iteration 1400 / 11340: loss 1.433682\n",
      "epoch done... acc 0.45\n",
      "iteration 1500 / 11340: loss 1.467989\n",
      "iteration 1600 / 11340: loss 1.289479\n",
      "iteration 1700 / 11340: loss 1.550982\n",
      "iteration 1800 / 11340: loss 1.670915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1900 / 11340: loss 1.606589\n",
      "epoch done... acc 0.455\n",
      "iteration 2000 / 11340: loss 1.307176\n",
      "iteration 2100 / 11340: loss 1.483150\n",
      "iteration 2200 / 11340: loss 1.579352\n",
      "iteration 2300 / 11340: loss 1.464163\n",
      "iteration 2400 / 11340: loss 1.453508\n",
      "epoch done... acc 0.472\n",
      "iteration 2500 / 11340: loss 1.468482\n",
      "iteration 2600 / 11340: loss 1.698903\n",
      "iteration 2700 / 11340: loss 1.427594\n",
      "iteration 2800 / 11340: loss 1.487077\n",
      "iteration 2900 / 11340: loss 1.625196\n",
      "epoch done... acc 0.486\n",
      "iteration 3000 / 11340: loss 1.488055\n",
      "iteration 3100 / 11340: loss 1.339179\n",
      "iteration 3200 / 11340: loss 1.340190\n",
      "iteration 3300 / 11340: loss 1.417607\n",
      "iteration 3400 / 11340: loss 1.580529\n",
      "epoch done... acc 0.498\n",
      "iteration 3500 / 11340: loss 1.425610\n",
      "iteration 3600 / 11340: loss 1.344762\n",
      "iteration 3700 / 11340: loss 1.252173\n",
      "iteration 3800 / 11340: loss 1.264632\n",
      "iteration 3900 / 11340: loss 1.325380\n",
      "epoch done... acc 0.474\n",
      "iteration 4000 / 11340: loss 1.368569\n",
      "iteration 4100 / 11340: loss 1.413615\n",
      "iteration 4200 / 11340: loss 1.401381\n",
      "iteration 4300 / 11340: loss 1.343329\n",
      "iteration 4400 / 11340: loss 1.312410\n",
      "epoch done... acc 0.49\n",
      "iteration 4500 / 11340: loss 1.498922\n",
      "iteration 4600 / 11340: loss 1.204496\n",
      "iteration 4700 / 11340: loss 1.245822\n",
      "iteration 4800 / 11340: loss 1.008864\n",
      "iteration 4900 / 11340: loss 1.258218\n",
      "epoch done... acc 0.502\n",
      "iteration 5000 / 11340: loss 1.300394\n",
      "iteration 5100 / 11340: loss 1.290063\n",
      "iteration 5200 / 11340: loss 1.246247\n",
      "iteration 5300 / 11340: loss 1.324783\n",
      "epoch done... acc 0.503\n",
      "iteration 5400 / 11340: loss 1.131677\n",
      "iteration 5500 / 11340: loss 1.381939\n",
      "iteration 5600 / 11340: loss 1.249333\n",
      "iteration 5700 / 11340: loss 1.196255\n",
      "iteration 5800 / 11340: loss 1.345173\n",
      "epoch done... acc 0.495\n",
      "iteration 5900 / 11340: loss 1.372039\n",
      "iteration 6000 / 11340: loss 1.222477\n",
      "iteration 6100 / 11340: loss 1.226382\n",
      "iteration 6200 / 11340: loss 1.276800\n",
      "iteration 6300 / 11340: loss 1.299823\n",
      "epoch done... acc 0.507\n",
      "iteration 6400 / 11340: loss 1.440516\n",
      "iteration 6500 / 11340: loss 1.146156\n",
      "iteration 6600 / 11340: loss 1.319131\n",
      "iteration 6700 / 11340: loss 1.233367\n",
      "iteration 6800 / 11340: loss 1.290472\n",
      "epoch done... acc 0.511\n",
      "iteration 6900 / 11340: loss 1.209165\n",
      "iteration 7000 / 11340: loss 1.226311\n",
      "iteration 7100 / 11340: loss 1.273397\n",
      "iteration 7200 / 11340: loss 1.171614\n",
      "iteration 7300 / 11340: loss 1.177450\n",
      "epoch done... acc 0.499\n",
      "iteration 7400 / 11340: loss 1.309699\n",
      "iteration 7500 / 11340: loss 1.198472\n",
      "iteration 7600 / 11340: loss 1.188676\n",
      "iteration 7700 / 11340: loss 1.199159\n",
      "iteration 7800 / 11340: loss 1.306683\n",
      "epoch done... acc 0.508\n",
      "iteration 7900 / 11340: loss 0.983423\n",
      "iteration 8000 / 11340: loss 1.198938\n",
      "iteration 8100 / 11340: loss 1.091972\n",
      "iteration 8200 / 11340: loss 1.282294\n",
      "iteration 8300 / 11340: loss 1.131963\n",
      "epoch done... acc 0.522\n",
      "iteration 8400 / 11340: loss 1.073351\n",
      "iteration 8500 / 11340: loss 1.011283\n",
      "iteration 8600 / 11340: loss 1.126151\n",
      "iteration 8700 / 11340: loss 1.178154\n",
      "iteration 8800 / 11340: loss 1.367192\n",
      "epoch done... acc 0.517\n",
      "iteration 8900 / 11340: loss 1.166101\n",
      "iteration 9000 / 11340: loss 1.071821\n",
      "iteration 9100 / 11340: loss 1.041755\n",
      "iteration 9200 / 11340: loss 1.169681\n",
      "iteration 9300 / 11340: loss 1.096144\n",
      "epoch done... acc 0.526\n",
      "iteration 9400 / 11340: loss 1.070144\n",
      "iteration 9500 / 11340: loss 1.372015\n",
      "iteration 9600 / 11340: loss 0.997127\n",
      "iteration 9700 / 11340: loss 1.066233\n",
      "iteration 9800 / 11340: loss 1.067308\n",
      "epoch done... acc 0.515\n",
      "iteration 9900 / 11340: loss 1.167618\n",
      "iteration 10000 / 11340: loss 1.151845\n",
      "iteration 10100 / 11340: loss 1.045358\n",
      "iteration 10200 / 11340: loss 1.314573\n",
      "epoch done... acc 0.513\n",
      "iteration 10300 / 11340: loss 1.091412\n",
      "iteration 10400 / 11340: loss 0.985152\n",
      "iteration 10500 / 11340: loss 1.030959\n",
      "iteration 10600 / 11340: loss 1.210450\n",
      "iteration 10700 / 11340: loss 0.979450\n",
      "epoch done... acc 0.522\n",
      "iteration 10800 / 11340: loss 1.017924\n",
      "iteration 10900 / 11340: loss 1.155549\n",
      "iteration 11000 / 11340: loss 1.028542\n",
      "iteration 11100 / 11340: loss 0.921738\n",
      "iteration 11200 / 11340: loss 1.000825\n",
      "epoch done... acc 0.514\n",
      "iteration 11300 / 11340: loss 1.081572\n",
      "Final training loss:  1.3095000897856701\n",
      "Final validation loss:  1.3986849741164575\n",
      "Final validation accuracy:  0.514\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "69 2 1 385 12600 100 0.001 0.98 0.514\n",
      "iteration 0 / 12600: loss 2.302657\n",
      "epoch done... acc 0.162\n",
      "iteration 100 / 12600: loss 2.108799\n",
      "iteration 200 / 12600: loss 1.777674\n",
      "iteration 300 / 12600: loss 1.819532\n",
      "iteration 400 / 12600: loss 1.735115\n",
      "epoch done... acc 0.375\n",
      "iteration 500 / 12600: loss 1.592642\n",
      "iteration 600 / 12600: loss 1.833850\n",
      "iteration 700 / 12600: loss 1.780317\n",
      "iteration 800 / 12600: loss 1.970801\n",
      "iteration 900 / 12600: loss 1.589631\n",
      "epoch done... acc 0.428\n",
      "iteration 1000 / 12600: loss 1.516681\n",
      "iteration 1100 / 12600: loss 1.594870\n",
      "iteration 1200 / 12600: loss 1.535445\n",
      "iteration 1300 / 12600: loss 1.464744\n",
      "iteration 1400 / 12600: loss 1.714861\n",
      "epoch done... acc 0.437\n",
      "iteration 1500 / 12600: loss 1.638991\n",
      "iteration 1600 / 12600: loss 1.567991\n",
      "iteration 1700 / 12600: loss 1.436651\n",
      "iteration 1800 / 12600: loss 1.538846\n",
      "iteration 1900 / 12600: loss 1.583104\n",
      "epoch done... acc 0.452\n",
      "iteration 2000 / 12600: loss 1.470730\n",
      "iteration 2100 / 12600: loss 1.372393\n",
      "iteration 2200 / 12600: loss 1.501430\n",
      "iteration 2300 / 12600: loss 1.529092\n",
      "iteration 2400 / 12600: loss 1.626868\n",
      "epoch done... acc 0.463\n",
      "iteration 2500 / 12600: loss 1.559577\n",
      "iteration 2600 / 12600: loss 1.495077\n",
      "iteration 2700 / 12600: loss 1.450857\n",
      "iteration 2800 / 12600: loss 1.450690\n",
      "iteration 2900 / 12600: loss 1.293129\n",
      "epoch done... acc 0.48\n",
      "iteration 3000 / 12600: loss 1.499757\n",
      "iteration 3100 / 12600: loss 1.415940\n",
      "iteration 3200 / 12600: loss 1.497230\n",
      "iteration 3300 / 12600: loss 1.495633\n",
      "iteration 3400 / 12600: loss 1.405648\n",
      "epoch done... acc 0.483\n",
      "iteration 3500 / 12600: loss 1.419777\n",
      "iteration 3600 / 12600: loss 1.159631\n",
      "iteration 3700 / 12600: loss 1.436045\n",
      "iteration 3800 / 12600: loss 1.546360\n",
      "iteration 3900 / 12600: loss 1.333712\n",
      "epoch done... acc 0.48\n",
      "iteration 4000 / 12600: loss 1.466348\n",
      "iteration 4100 / 12600: loss 1.365756\n",
      "iteration 4200 / 12600: loss 1.352928\n",
      "iteration 4300 / 12600: loss 1.290915\n",
      "iteration 4400 / 12600: loss 1.339740\n",
      "epoch done... acc 0.496\n",
      "iteration 4500 / 12600: loss 1.285691\n",
      "iteration 4600 / 12600: loss 1.273651\n",
      "iteration 4700 / 12600: loss 1.413305\n",
      "iteration 4800 / 12600: loss 1.392612\n",
      "iteration 4900 / 12600: loss 1.126574\n",
      "epoch done... acc 0.501\n",
      "iteration 5000 / 12600: loss 1.171752\n",
      "iteration 5100 / 12600: loss 1.326945\n",
      "iteration 5200 / 12600: loss 1.362083\n",
      "iteration 5300 / 12600: loss 1.180542\n",
      "epoch done... acc 0.515\n",
      "iteration 5400 / 12600: loss 1.359979\n",
      "iteration 5500 / 12600: loss 1.160409\n",
      "iteration 5600 / 12600: loss 1.282891\n",
      "iteration 5700 / 12600: loss 1.341177\n",
      "iteration 5800 / 12600: loss 1.342861\n",
      "epoch done... acc 0.516\n",
      "iteration 5900 / 12600: loss 1.241188\n",
      "iteration 6000 / 12600: loss 1.198131\n",
      "iteration 6100 / 12600: loss 1.182316\n",
      "iteration 6200 / 12600: loss 1.117041\n",
      "iteration 6300 / 12600: loss 1.219069\n",
      "epoch done... acc 0.514\n",
      "iteration 6400 / 12600: loss 1.128382\n",
      "iteration 6500 / 12600: loss 1.202700\n",
      "iteration 6600 / 12600: loss 1.350443\n",
      "iteration 6700 / 12600: loss 1.127448\n",
      "iteration 6800 / 12600: loss 1.296362\n",
      "epoch done... acc 0.516\n",
      "iteration 6900 / 12600: loss 1.252310\n",
      "iteration 7000 / 12600: loss 1.250664\n",
      "iteration 7100 / 12600: loss 1.187186\n",
      "iteration 7200 / 12600: loss 1.209774\n",
      "iteration 7300 / 12600: loss 1.118151\n",
      "epoch done... acc 0.513\n",
      "iteration 7400 / 12600: loss 1.211762\n",
      "iteration 7500 / 12600: loss 1.221428\n",
      "iteration 7600 / 12600: loss 1.226606\n",
      "iteration 7700 / 12600: loss 1.094758\n",
      "iteration 7800 / 12600: loss 1.217085\n",
      "epoch done... acc 0.508\n",
      "iteration 7900 / 12600: loss 1.160593\n",
      "iteration 8000 / 12600: loss 1.143124\n",
      "iteration 8100 / 12600: loss 1.160004\n",
      "iteration 8200 / 12600: loss 1.228724\n",
      "iteration 8300 / 12600: loss 1.159862\n",
      "epoch done... acc 0.512\n",
      "iteration 8400 / 12600: loss 1.141215\n",
      "iteration 8500 / 12600: loss 1.248722\n",
      "iteration 8600 / 12600: loss 1.207302\n",
      "iteration 8700 / 12600: loss 1.097514\n",
      "iteration 8800 / 12600: loss 1.043103\n",
      "epoch done... acc 0.521\n",
      "iteration 8900 / 12600: loss 1.380781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9000 / 12600: loss 1.065175\n",
      "iteration 9100 / 12600: loss 1.124809\n",
      "iteration 9200 / 12600: loss 1.207891\n",
      "iteration 9300 / 12600: loss 1.002018\n",
      "epoch done... acc 0.526\n",
      "iteration 9400 / 12600: loss 1.250052\n",
      "iteration 9500 / 12600: loss 1.115197\n",
      "iteration 9600 / 12600: loss 1.161951\n",
      "iteration 9700 / 12600: loss 1.101157\n",
      "iteration 9800 / 12600: loss 1.186118\n",
      "epoch done... acc 0.53\n",
      "iteration 9900 / 12600: loss 0.977055\n",
      "iteration 10000 / 12600: loss 1.274053\n",
      "iteration 10100 / 12600: loss 1.084333\n",
      "iteration 10200 / 12600: loss 1.173121\n",
      "epoch done... acc 0.527\n",
      "iteration 10300 / 12600: loss 1.141272\n",
      "iteration 10400 / 12600: loss 1.115710\n",
      "iteration 10500 / 12600: loss 1.212015\n",
      "iteration 10600 / 12600: loss 1.070568\n",
      "iteration 10700 / 12600: loss 1.034014\n",
      "epoch done... acc 0.527\n",
      "iteration 10800 / 12600: loss 1.109440\n",
      "iteration 10900 / 12600: loss 1.142709\n",
      "iteration 11000 / 12600: loss 1.088053\n",
      "iteration 11100 / 12600: loss 1.027314\n",
      "iteration 11200 / 12600: loss 1.031022\n",
      "epoch done... acc 0.53\n",
      "iteration 11300 / 12600: loss 1.160718\n",
      "iteration 11400 / 12600: loss 1.027861\n",
      "iteration 11500 / 12600: loss 1.079483\n",
      "iteration 11600 / 12600: loss 1.119567\n",
      "iteration 11700 / 12600: loss 1.116505\n",
      "epoch done... acc 0.524\n",
      "iteration 11800 / 12600: loss 1.183516\n",
      "iteration 11900 / 12600: loss 1.230062\n",
      "iteration 12000 / 12600: loss 1.036210\n",
      "iteration 12100 / 12600: loss 0.985975\n",
      "iteration 12200 / 12600: loss 1.176206\n",
      "epoch done... acc 0.528\n",
      "iteration 12300 / 12600: loss 1.199501\n",
      "iteration 12400 / 12600: loss 1.022817\n",
      "iteration 12500 / 12600: loss 1.028737\n",
      "Final training loss:  1.102144597128345\n",
      "Final validation loss:  1.3661239134951697\n",
      "Final validation accuracy:  0.528\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "70 1 0 385 12600 100 0.001 0.98 0.528\n",
      "iteration 0 / 12600: loss 2.302627\n",
      "epoch done... acc 0.202\n",
      "iteration 100 / 12600: loss 2.048604\n",
      "iteration 200 / 12600: loss 1.957357\n",
      "iteration 300 / 12600: loss 1.897191\n",
      "iteration 400 / 12600: loss 1.818403\n",
      "epoch done... acc 0.371\n",
      "iteration 500 / 12600: loss 1.726721\n",
      "iteration 600 / 12600: loss 1.746307\n",
      "iteration 700 / 12600: loss 1.695528\n",
      "iteration 800 / 12600: loss 1.740417\n",
      "iteration 900 / 12600: loss 1.508882\n",
      "epoch done... acc 0.423\n",
      "iteration 1000 / 12600: loss 1.589100\n",
      "iteration 1100 / 12600: loss 1.545772\n",
      "iteration 1200 / 12600: loss 1.603957\n",
      "iteration 1300 / 12600: loss 1.708480\n",
      "iteration 1400 / 12600: loss 1.504775\n",
      "epoch done... acc 0.437\n",
      "iteration 1500 / 12600: loss 1.603791\n",
      "iteration 1600 / 12600: loss 1.372299\n",
      "iteration 1700 / 12600: loss 1.454338\n",
      "iteration 1800 / 12600: loss 1.392713\n",
      "iteration 1900 / 12600: loss 1.392038\n",
      "epoch done... acc 0.446\n",
      "iteration 2000 / 12600: loss 1.451021\n",
      "iteration 2100 / 12600: loss 1.588557\n",
      "iteration 2200 / 12600: loss 1.530938\n",
      "iteration 2300 / 12600: loss 1.448399\n",
      "iteration 2400 / 12600: loss 1.462239\n",
      "epoch done... acc 0.465\n",
      "iteration 2500 / 12600: loss 1.240840\n",
      "iteration 2600 / 12600: loss 1.624559\n",
      "iteration 2700 / 12600: loss 1.509735\n",
      "iteration 2800 / 12600: loss 1.448058\n",
      "iteration 2900 / 12600: loss 1.543506\n",
      "epoch done... acc 0.469\n",
      "iteration 3000 / 12600: loss 1.352193\n",
      "iteration 3100 / 12600: loss 1.206537\n",
      "iteration 3200 / 12600: loss 1.405890\n",
      "iteration 3300 / 12600: loss 1.292382\n",
      "iteration 3400 / 12600: loss 1.514250\n",
      "epoch done... acc 0.482\n",
      "iteration 3500 / 12600: loss 1.457303\n",
      "iteration 3600 / 12600: loss 1.488670\n",
      "iteration 3700 / 12600: loss 1.285667\n",
      "iteration 3800 / 12600: loss 1.501740\n",
      "iteration 3900 / 12600: loss 1.349572\n",
      "epoch done... acc 0.489\n",
      "iteration 4000 / 12600: loss 1.454346\n",
      "iteration 4100 / 12600: loss 1.453060\n",
      "iteration 4200 / 12600: loss 1.148492\n",
      "iteration 4300 / 12600: loss 1.480036\n",
      "iteration 4400 / 12600: loss 1.240239\n",
      "epoch done... acc 0.492\n",
      "iteration 4500 / 12600: loss 1.407023\n",
      "iteration 4600 / 12600: loss 1.359292\n",
      "iteration 4700 / 12600: loss 1.266369\n",
      "iteration 4800 / 12600: loss 1.366171\n",
      "iteration 4900 / 12600: loss 1.466990\n",
      "epoch done... acc 0.476\n",
      "iteration 5000 / 12600: loss 1.282440\n",
      "iteration 5100 / 12600: loss 1.197122\n",
      "iteration 5200 / 12600: loss 1.173664\n",
      "iteration 5300 / 12600: loss 1.366397\n",
      "epoch done... acc 0.512\n",
      "iteration 5400 / 12600: loss 1.384473\n",
      "iteration 5500 / 12600: loss 1.351873\n",
      "iteration 5600 / 12600: loss 1.077799\n",
      "iteration 5700 / 12600: loss 1.236710\n",
      "iteration 5800 / 12600: loss 1.296059\n",
      "epoch done... acc 0.483\n",
      "iteration 5900 / 12600: loss 1.297267\n",
      "iteration 6000 / 12600: loss 1.180245\n",
      "iteration 6100 / 12600: loss 1.354079\n",
      "iteration 6200 / 12600: loss 1.115361\n",
      "iteration 6300 / 12600: loss 1.199270\n",
      "epoch done... acc 0.502\n",
      "iteration 6400 / 12600: loss 1.253203\n",
      "iteration 6500 / 12600: loss 1.250137\n",
      "iteration 6600 / 12600: loss 1.157460\n",
      "iteration 6700 / 12600: loss 1.129498\n",
      "iteration 6800 / 12600: loss 1.031666\n",
      "epoch done... acc 0.505\n",
      "iteration 6900 / 12600: loss 1.223868\n",
      "iteration 7000 / 12600: loss 1.221686\n",
      "iteration 7100 / 12600: loss 1.505298\n",
      "iteration 7200 / 12600: loss 0.991241\n",
      "iteration 7300 / 12600: loss 1.273270\n",
      "epoch done... acc 0.502\n",
      "iteration 7400 / 12600: loss 1.221430\n",
      "iteration 7500 / 12600: loss 1.193673\n",
      "iteration 7600 / 12600: loss 1.218083\n",
      "iteration 7700 / 12600: loss 1.287815\n",
      "iteration 7800 / 12600: loss 1.129374\n",
      "epoch done... acc 0.504\n",
      "iteration 7900 / 12600: loss 1.227044\n",
      "iteration 8000 / 12600: loss 1.191908\n",
      "iteration 8100 / 12600: loss 1.386092\n",
      "iteration 8200 / 12600: loss 1.228103\n",
      "iteration 8300 / 12600: loss 1.166053\n",
      "epoch done... acc 0.5\n",
      "iteration 8400 / 12600: loss 1.271497\n",
      "iteration 8500 / 12600: loss 1.156371\n",
      "iteration 8600 / 12600: loss 1.227708\n",
      "iteration 8700 / 12600: loss 1.100000\n",
      "iteration 8800 / 12600: loss 1.271267\n",
      "epoch done... acc 0.51\n",
      "iteration 8900 / 12600: loss 1.082218\n",
      "iteration 9000 / 12600: loss 1.279167\n",
      "iteration 9100 / 12600: loss 1.336295\n",
      "iteration 9200 / 12600: loss 1.313569\n",
      "iteration 9300 / 12600: loss 1.301466\n",
      "epoch done... acc 0.518\n",
      "iteration 9400 / 12600: loss 1.177258\n",
      "iteration 9500 / 12600: loss 1.159716\n",
      "iteration 9600 / 12600: loss 1.202317\n",
      "iteration 9700 / 12600: loss 1.047658\n",
      "iteration 9800 / 12600: loss 1.235195\n",
      "epoch done... acc 0.511\n",
      "iteration 9900 / 12600: loss 1.151927\n",
      "iteration 10000 / 12600: loss 1.342222\n",
      "iteration 10100 / 12600: loss 1.340942\n",
      "iteration 10200 / 12600: loss 1.070406\n",
      "epoch done... acc 0.524\n",
      "iteration 10300 / 12600: loss 1.303711\n",
      "iteration 10400 / 12600: loss 1.029787\n",
      "iteration 10500 / 12600: loss 1.022158\n",
      "iteration 10600 / 12600: loss 1.058865\n",
      "iteration 10700 / 12600: loss 1.001975\n",
      "epoch done... acc 0.52\n",
      "iteration 10800 / 12600: loss 1.196050\n",
      "iteration 10900 / 12600: loss 1.095872\n",
      "iteration 11000 / 12600: loss 1.031235\n",
      "iteration 11100 / 12600: loss 1.008561\n",
      "iteration 11200 / 12600: loss 0.969683\n",
      "epoch done... acc 0.517\n",
      "iteration 11300 / 12600: loss 1.092620\n",
      "iteration 11400 / 12600: loss 1.045938\n",
      "iteration 11500 / 12600: loss 1.121379\n",
      "iteration 11600 / 12600: loss 1.073485\n",
      "iteration 11700 / 12600: loss 0.906922\n",
      "epoch done... acc 0.513\n",
      "iteration 11800 / 12600: loss 0.942828\n",
      "iteration 11900 / 12600: loss 0.981969\n",
      "iteration 12000 / 12600: loss 1.098224\n",
      "iteration 12100 / 12600: loss 0.946385\n",
      "iteration 12200 / 12600: loss 1.147964\n",
      "epoch done... acc 0.523\n",
      "iteration 12300 / 12600: loss 1.012956\n",
      "iteration 12400 / 12600: loss 1.035091\n",
      "iteration 12500 / 12600: loss 0.969218\n",
      "Final training loss:  0.9829157577818706\n",
      "Final validation loss:  1.3807988964089475\n",
      "Final validation accuracy:  0.523\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "71 2 0 385 12600 100 0.001 0.98 0.523\n",
      "iteration 0 / 13860: loss 2.302600\n",
      "epoch done... acc 0.168\n",
      "iteration 100 / 13860: loss 2.084835\n",
      "iteration 200 / 13860: loss 1.837199\n",
      "iteration 300 / 13860: loss 1.870230\n",
      "iteration 400 / 13860: loss 1.567947\n",
      "epoch done... acc 0.386\n",
      "iteration 500 / 13860: loss 1.777270\n",
      "iteration 600 / 13860: loss 1.718126\n",
      "iteration 700 / 13860: loss 1.625710\n",
      "iteration 800 / 13860: loss 1.612465\n",
      "iteration 900 / 13860: loss 1.728687\n",
      "epoch done... acc 0.432\n",
      "iteration 1000 / 13860: loss 1.559610\n",
      "iteration 1100 / 13860: loss 1.519562\n",
      "iteration 1200 / 13860: loss 1.534020\n",
      "iteration 1300 / 13860: loss 1.483193\n",
      "iteration 1400 / 13860: loss 1.723813\n",
      "epoch done... acc 0.444\n",
      "iteration 1500 / 13860: loss 1.515318\n",
      "iteration 1600 / 13860: loss 1.507364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1700 / 13860: loss 1.647560\n",
      "iteration 1800 / 13860: loss 1.608496\n",
      "iteration 1900 / 13860: loss 1.528241\n",
      "epoch done... acc 0.454\n",
      "iteration 2000 / 13860: loss 1.580681\n",
      "iteration 2100 / 13860: loss 1.549059\n",
      "iteration 2200 / 13860: loss 1.464216\n",
      "iteration 2300 / 13860: loss 1.224398\n",
      "iteration 2400 / 13860: loss 1.412142\n",
      "epoch done... acc 0.455\n",
      "iteration 2500 / 13860: loss 1.243795\n",
      "iteration 2600 / 13860: loss 1.374550\n",
      "iteration 2700 / 13860: loss 1.554154\n",
      "iteration 2800 / 13860: loss 1.301363\n",
      "iteration 2900 / 13860: loss 1.442406\n",
      "epoch done... acc 0.468\n",
      "iteration 3000 / 13860: loss 1.396504\n",
      "iteration 3100 / 13860: loss 1.309062\n",
      "iteration 3200 / 13860: loss 1.290575\n",
      "iteration 3300 / 13860: loss 1.534892\n",
      "iteration 3400 / 13860: loss 1.387105\n",
      "epoch done... acc 0.473\n",
      "iteration 3500 / 13860: loss 1.429152\n",
      "iteration 3600 / 13860: loss 1.479260\n",
      "iteration 3700 / 13860: loss 1.277548\n",
      "iteration 3800 / 13860: loss 1.321561\n",
      "iteration 3900 / 13860: loss 1.423694\n",
      "epoch done... acc 0.49\n",
      "iteration 4000 / 13860: loss 1.442750\n",
      "iteration 4100 / 13860: loss 1.339154\n",
      "iteration 4200 / 13860: loss 1.353947\n",
      "iteration 4300 / 13860: loss 1.295225\n",
      "iteration 4400 / 13860: loss 1.219525\n",
      "epoch done... acc 0.479\n",
      "iteration 4500 / 13860: loss 1.279895\n",
      "iteration 4600 / 13860: loss 1.257167\n",
      "iteration 4700 / 13860: loss 1.356444\n",
      "iteration 4800 / 13860: loss 1.319801\n",
      "iteration 4900 / 13860: loss 1.260554\n",
      "epoch done... acc 0.488\n",
      "iteration 5000 / 13860: loss 1.385830\n",
      "iteration 5100 / 13860: loss 1.403987\n",
      "iteration 5200 / 13860: loss 1.305548\n",
      "iteration 5300 / 13860: loss 1.488981\n",
      "epoch done... acc 0.5\n",
      "iteration 5400 / 13860: loss 1.350237\n",
      "iteration 5500 / 13860: loss 1.365625\n",
      "iteration 5600 / 13860: loss 1.253144\n",
      "iteration 5700 / 13860: loss 1.264973\n",
      "iteration 5800 / 13860: loss 1.330303\n",
      "epoch done... acc 0.492\n",
      "iteration 5900 / 13860: loss 1.215291\n",
      "iteration 6000 / 13860: loss 1.267207\n",
      "iteration 6100 / 13860: loss 1.213495\n",
      "iteration 6200 / 13860: loss 1.297074\n",
      "iteration 6300 / 13860: loss 1.266455\n",
      "epoch done... acc 0.506\n",
      "iteration 6400 / 13860: loss 1.334789\n",
      "iteration 6500 / 13860: loss 1.092926\n",
      "iteration 6600 / 13860: loss 1.223191\n",
      "iteration 6700 / 13860: loss 1.142177\n",
      "iteration 6800 / 13860: loss 1.247291\n",
      "epoch done... acc 0.508\n",
      "iteration 6900 / 13860: loss 1.232393\n",
      "iteration 7000 / 13860: loss 1.255166\n",
      "iteration 7100 / 13860: loss 1.221694\n",
      "iteration 7200 / 13860: loss 1.243632\n",
      "iteration 7300 / 13860: loss 1.266537\n",
      "epoch done... acc 0.502\n",
      "iteration 7400 / 13860: loss 1.275429\n",
      "iteration 7500 / 13860: loss 1.091953\n",
      "iteration 7600 / 13860: loss 1.062252\n",
      "iteration 7700 / 13860: loss 1.203550\n",
      "iteration 7800 / 13860: loss 1.239362\n",
      "epoch done... acc 0.505\n",
      "iteration 7900 / 13860: loss 1.128831\n",
      "iteration 8000 / 13860: loss 1.253692\n",
      "iteration 8100 / 13860: loss 1.098711\n",
      "iteration 8200 / 13860: loss 1.157205\n",
      "iteration 8300 / 13860: loss 1.211918\n",
      "epoch done... acc 0.52\n",
      "iteration 8400 / 13860: loss 1.087031\n",
      "iteration 8500 / 13860: loss 1.122689\n",
      "iteration 8600 / 13860: loss 1.161412\n",
      "iteration 8700 / 13860: loss 1.190393\n",
      "iteration 8800 / 13860: loss 1.272188\n",
      "epoch done... acc 0.503\n",
      "iteration 8900 / 13860: loss 1.117353\n",
      "iteration 9000 / 13860: loss 1.322796\n",
      "iteration 9100 / 13860: loss 1.083147\n",
      "iteration 9200 / 13860: loss 1.238357\n",
      "iteration 9300 / 13860: loss 1.114760\n",
      "epoch done... acc 0.527\n",
      "iteration 9400 / 13860: loss 1.051120\n",
      "iteration 9500 / 13860: loss 1.137891\n",
      "iteration 9600 / 13860: loss 1.054893\n",
      "iteration 9700 / 13860: loss 1.117302\n",
      "iteration 9800 / 13860: loss 1.075302\n",
      "epoch done... acc 0.525\n",
      "iteration 9900 / 13860: loss 0.878167\n",
      "iteration 10000 / 13860: loss 1.225508\n",
      "iteration 10100 / 13860: loss 0.919859\n",
      "iteration 10200 / 13860: loss 0.991223\n",
      "epoch done... acc 0.513\n",
      "iteration 10300 / 13860: loss 1.199856\n",
      "iteration 10400 / 13860: loss 1.078331\n",
      "iteration 10500 / 13860: loss 1.037761\n",
      "iteration 10600 / 13860: loss 1.073604\n",
      "iteration 10700 / 13860: loss 0.935948\n",
      "epoch done... acc 0.533\n",
      "iteration 10800 / 13860: loss 0.979987\n",
      "iteration 10900 / 13860: loss 1.167886\n",
      "iteration 11000 / 13860: loss 1.112711\n",
      "iteration 11100 / 13860: loss 1.054507\n",
      "iteration 11200 / 13860: loss 1.056412\n",
      "epoch done... acc 0.5\n",
      "iteration 11300 / 13860: loss 1.076816\n",
      "iteration 11400 / 13860: loss 1.133601\n",
      "iteration 11500 / 13860: loss 1.017603\n",
      "iteration 11600 / 13860: loss 1.047200\n",
      "iteration 11700 / 13860: loss 1.211510\n",
      "epoch done... acc 0.525\n",
      "iteration 11800 / 13860: loss 0.902375\n",
      "iteration 11900 / 13860: loss 1.060174\n",
      "iteration 12000 / 13860: loss 1.131459\n",
      "iteration 12100 / 13860: loss 1.136293\n",
      "iteration 12200 / 13860: loss 1.180815\n",
      "epoch done... acc 0.522\n",
      "iteration 12300 / 13860: loss 1.063559\n",
      "iteration 12400 / 13860: loss 0.948022\n",
      "iteration 12500 / 13860: loss 1.025724\n",
      "iteration 12600 / 13860: loss 0.941294\n",
      "iteration 12700 / 13860: loss 1.021509\n",
      "epoch done... acc 0.528\n",
      "iteration 12800 / 13860: loss 0.959104\n",
      "iteration 12900 / 13860: loss 1.011663\n",
      "iteration 13000 / 13860: loss 1.065883\n",
      "iteration 13100 / 13860: loss 1.018398\n",
      "iteration 13200 / 13860: loss 1.044207\n",
      "epoch done... acc 0.522\n",
      "iteration 13300 / 13860: loss 1.082624\n",
      "iteration 13400 / 13860: loss 1.036948\n",
      "iteration 13500 / 13860: loss 1.009928\n",
      "iteration 13600 / 13860: loss 0.934968\n",
      "iteration 13700 / 13860: loss 1.035072\n",
      "epoch done... acc 0.53\n",
      "iteration 13800 / 13860: loss 0.926363\n",
      "Final training loss:  1.1293517846787906\n",
      "Final validation loss:  1.3733040015449\n",
      "Final validation accuracy:  0.53\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "72 1 1 385 12600 100 0.001 0.98 0.53\n",
      "iteration 0 / 11340: loss 2.302542\n",
      "epoch done... acc 0.119\n",
      "iteration 100 / 11340: loss 1.948113\n",
      "iteration 200 / 11340: loss 1.856227\n",
      "iteration 300 / 11340: loss 1.830533\n",
      "iteration 400 / 11340: loss 1.805662\n",
      "epoch done... acc 0.383\n",
      "iteration 500 / 11340: loss 1.679423\n",
      "iteration 600 / 11340: loss 1.736809\n",
      "iteration 700 / 11340: loss 1.727757\n",
      "iteration 800 / 11340: loss 1.589754\n",
      "iteration 900 / 11340: loss 1.543635\n",
      "epoch done... acc 0.424\n",
      "iteration 1000 / 11340: loss 1.636628\n",
      "iteration 1100 / 11340: loss 1.667614\n",
      "iteration 1200 / 11340: loss 1.522719\n",
      "iteration 1300 / 11340: loss 1.622052\n",
      "iteration 1400 / 11340: loss 1.566253\n",
      "epoch done... acc 0.447\n",
      "iteration 1500 / 11340: loss 1.561831\n",
      "iteration 1600 / 11340: loss 1.517753\n",
      "iteration 1700 / 11340: loss 1.642304\n",
      "iteration 1800 / 11340: loss 1.449903\n",
      "iteration 1900 / 11340: loss 1.416034\n",
      "epoch done... acc 0.468\n",
      "iteration 2000 / 11340: loss 1.407887\n",
      "iteration 2100 / 11340: loss 1.557229\n",
      "iteration 2200 / 11340: loss 1.524926\n",
      "iteration 2300 / 11340: loss 1.562672\n",
      "iteration 2400 / 11340: loss 1.372915\n",
      "epoch done... acc 0.475\n",
      "iteration 2500 / 11340: loss 1.334711\n",
      "iteration 2600 / 11340: loss 1.258396\n",
      "iteration 2700 / 11340: loss 1.524508\n",
      "iteration 2800 / 11340: loss 1.525734\n",
      "iteration 2900 / 11340: loss 1.466141\n",
      "epoch done... acc 0.477\n",
      "iteration 3000 / 11340: loss 1.463690\n",
      "iteration 3100 / 11340: loss 1.334868\n",
      "iteration 3200 / 11340: loss 1.424886\n",
      "iteration 3300 / 11340: loss 1.432015\n",
      "iteration 3400 / 11340: loss 1.333841\n",
      "epoch done... acc 0.478\n",
      "iteration 3500 / 11340: loss 1.416770\n",
      "iteration 3600 / 11340: loss 1.273651\n",
      "iteration 3700 / 11340: loss 1.343628\n",
      "iteration 3800 / 11340: loss 1.410545\n",
      "iteration 3900 / 11340: loss 1.445811\n",
      "epoch done... acc 0.492\n",
      "iteration 4000 / 11340: loss 1.392606\n",
      "iteration 4100 / 11340: loss 1.319680\n",
      "iteration 4200 / 11340: loss 1.313366\n",
      "iteration 4300 / 11340: loss 1.376605\n",
      "iteration 4400 / 11340: loss 1.281242\n",
      "epoch done... acc 0.5\n",
      "iteration 4500 / 11340: loss 1.345807\n",
      "iteration 4600 / 11340: loss 1.509151\n",
      "iteration 4700 / 11340: loss 1.468863\n",
      "iteration 4800 / 11340: loss 1.272382\n",
      "iteration 4900 / 11340: loss 1.341208\n",
      "epoch done... acc 0.509\n",
      "iteration 5000 / 11340: loss 1.429459\n",
      "iteration 5100 / 11340: loss 1.286300\n",
      "iteration 5200 / 11340: loss 1.279972\n",
      "iteration 5300 / 11340: loss 1.362689\n",
      "epoch done... acc 0.502\n",
      "iteration 5400 / 11340: loss 1.417247\n",
      "iteration 5500 / 11340: loss 1.286293\n",
      "iteration 5600 / 11340: loss 1.233686\n",
      "iteration 5700 / 11340: loss 1.376382\n",
      "iteration 5800 / 11340: loss 1.264192\n",
      "epoch done... acc 0.498\n",
      "iteration 5900 / 11340: loss 1.312129\n",
      "iteration 6000 / 11340: loss 1.265346\n",
      "iteration 6100 / 11340: loss 1.145117\n",
      "iteration 6200 / 11340: loss 1.362666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6300 / 11340: loss 1.269485\n",
      "epoch done... acc 0.511\n",
      "iteration 6400 / 11340: loss 1.256007\n",
      "iteration 6500 / 11340: loss 1.223858\n",
      "iteration 6600 / 11340: loss 1.259450\n",
      "iteration 6700 / 11340: loss 1.142220\n",
      "iteration 6800 / 11340: loss 1.232299\n",
      "epoch done... acc 0.502\n",
      "iteration 6900 / 11340: loss 1.134002\n",
      "iteration 7000 / 11340: loss 1.102912\n",
      "iteration 7100 / 11340: loss 1.344874\n",
      "iteration 7200 / 11340: loss 1.180439\n",
      "iteration 7300 / 11340: loss 1.294985\n",
      "epoch done... acc 0.516\n",
      "iteration 7400 / 11340: loss 1.085025\n",
      "iteration 7500 / 11340: loss 1.147117\n",
      "iteration 7600 / 11340: loss 1.191085\n",
      "iteration 7700 / 11340: loss 1.388056\n",
      "iteration 7800 / 11340: loss 1.169826\n",
      "epoch done... acc 0.528\n",
      "iteration 7900 / 11340: loss 1.080311\n",
      "iteration 8000 / 11340: loss 1.160702\n",
      "iteration 8100 / 11340: loss 1.046119\n",
      "iteration 8200 / 11340: loss 1.260401\n",
      "iteration 8300 / 11340: loss 0.991215\n",
      "epoch done... acc 0.521\n",
      "iteration 8400 / 11340: loss 1.353101\n",
      "iteration 8500 / 11340: loss 1.194645\n",
      "iteration 8600 / 11340: loss 1.128281\n",
      "iteration 8700 / 11340: loss 1.146505\n",
      "iteration 8800 / 11340: loss 1.081937\n",
      "epoch done... acc 0.522\n",
      "iteration 8900 / 11340: loss 1.109530\n",
      "iteration 9000 / 11340: loss 1.123557\n",
      "iteration 9100 / 11340: loss 1.005645\n",
      "iteration 9200 / 11340: loss 1.031041\n",
      "iteration 9300 / 11340: loss 1.336454\n",
      "epoch done... acc 0.514\n",
      "iteration 9400 / 11340: loss 1.288682\n",
      "iteration 9500 / 11340: loss 1.124465\n",
      "iteration 9600 / 11340: loss 0.969279\n",
      "iteration 9700 / 11340: loss 1.301800\n",
      "iteration 9800 / 11340: loss 1.142815\n",
      "epoch done... acc 0.524\n",
      "iteration 9900 / 11340: loss 1.111500\n",
      "iteration 10000 / 11340: loss 1.122319\n",
      "iteration 10100 / 11340: loss 1.038169\n",
      "iteration 10200 / 11340: loss 1.059226\n",
      "epoch done... acc 0.533\n",
      "iteration 10300 / 11340: loss 1.012254\n",
      "iteration 10400 / 11340: loss 1.100890\n",
      "iteration 10500 / 11340: loss 1.045456\n",
      "iteration 10600 / 11340: loss 1.194063\n",
      "iteration 10700 / 11340: loss 1.192189\n",
      "epoch done... acc 0.522\n",
      "iteration 10800 / 11340: loss 1.114495\n",
      "iteration 10900 / 11340: loss 1.045409\n",
      "iteration 11000 / 11340: loss 1.174422\n",
      "iteration 11100 / 11340: loss 1.057283\n",
      "iteration 11200 / 11340: loss 1.050354\n",
      "epoch done... acc 0.53\n",
      "iteration 11300 / 11340: loss 1.096500\n",
      "Final training loss:  1.018023745422443\n",
      "Final validation loss:  1.36470555299365\n",
      "Final validation accuracy:  0.53\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "73 2 1 385 12600 100 0.001 0.98 0.53\n",
      "iteration 0 / 12600: loss 2.302569\n",
      "epoch done... acc 0.156\n",
      "iteration 100 / 12600: loss 2.030507\n",
      "iteration 200 / 12600: loss 1.708923\n",
      "iteration 300 / 12600: loss 1.726111\n",
      "iteration 400 / 12600: loss 1.863831\n",
      "epoch done... acc 0.382\n",
      "iteration 500 / 12600: loss 1.819966\n",
      "iteration 600 / 12600: loss 1.728571\n",
      "iteration 700 / 12600: loss 1.620131\n",
      "iteration 800 / 12600: loss 1.640294\n",
      "iteration 900 / 12600: loss 1.659135\n",
      "epoch done... acc 0.421\n",
      "iteration 1000 / 12600: loss 1.617569\n",
      "iteration 1100 / 12600: loss 1.651915\n",
      "iteration 1200 / 12600: loss 1.730082\n",
      "iteration 1300 / 12600: loss 1.430831\n",
      "iteration 1400 / 12600: loss 1.563624\n",
      "epoch done... acc 0.454\n",
      "iteration 1500 / 12600: loss 1.582421\n",
      "iteration 1600 / 12600: loss 1.570489\n",
      "iteration 1700 / 12600: loss 1.570442\n",
      "iteration 1800 / 12600: loss 1.638779\n",
      "iteration 1900 / 12600: loss 1.302812\n",
      "epoch done... acc 0.465\n",
      "iteration 2000 / 12600: loss 1.526452\n",
      "iteration 2100 / 12600: loss 1.699290\n",
      "iteration 2200 / 12600: loss 1.545287\n",
      "iteration 2300 / 12600: loss 1.386394\n",
      "iteration 2400 / 12600: loss 1.297490\n",
      "epoch done... acc 0.468\n",
      "iteration 2500 / 12600: loss 1.319537\n",
      "iteration 2600 / 12600: loss 1.697065\n",
      "iteration 2700 / 12600: loss 1.503615\n",
      "iteration 2800 / 12600: loss 1.556885\n",
      "iteration 2900 / 12600: loss 1.262753\n",
      "epoch done... acc 0.476\n",
      "iteration 3000 / 12600: loss 1.572347\n",
      "iteration 3100 / 12600: loss 1.597019\n",
      "iteration 3200 / 12600: loss 1.529802\n",
      "iteration 3300 / 12600: loss 1.387594\n",
      "iteration 3400 / 12600: loss 1.367982\n",
      "epoch done... acc 0.478\n",
      "iteration 3500 / 12600: loss 1.333682\n",
      "iteration 3600 / 12600: loss 1.342916\n",
      "iteration 3700 / 12600: loss 1.456911\n",
      "iteration 3800 / 12600: loss 1.329956\n",
      "iteration 3900 / 12600: loss 1.363197\n",
      "epoch done... acc 0.475\n",
      "iteration 4000 / 12600: loss 1.257714\n",
      "iteration 4100 / 12600: loss 1.367913\n",
      "iteration 4200 / 12600: loss 1.282433\n",
      "iteration 4300 / 12600: loss 1.403449\n",
      "iteration 4400 / 12600: loss 1.150844\n",
      "epoch done... acc 0.489\n",
      "iteration 4500 / 12600: loss 1.289316\n",
      "iteration 4600 / 12600: loss 1.302022\n",
      "iteration 4700 / 12600: loss 1.291319\n",
      "iteration 4800 / 12600: loss 1.408875\n",
      "iteration 4900 / 12600: loss 1.165027\n",
      "epoch done... acc 0.495\n",
      "iteration 5000 / 12600: loss 1.319011\n",
      "iteration 5100 / 12600: loss 1.428067\n",
      "iteration 5200 / 12600: loss 1.223241\n",
      "iteration 5300 / 12600: loss 1.266731\n",
      "epoch done... acc 0.506\n",
      "iteration 5400 / 12600: loss 1.324566\n",
      "iteration 5500 / 12600: loss 1.195389\n",
      "iteration 5600 / 12600: loss 1.259832\n",
      "iteration 5700 / 12600: loss 1.227238\n",
      "iteration 5800 / 12600: loss 1.374972\n",
      "epoch done... acc 0.491\n",
      "iteration 5900 / 12600: loss 1.021414\n",
      "iteration 6000 / 12600: loss 1.269053\n",
      "iteration 6100 / 12600: loss 1.239854\n",
      "iteration 6200 / 12600: loss 1.288292\n",
      "iteration 6300 / 12600: loss 1.296343\n",
      "epoch done... acc 0.494\n",
      "iteration 6400 / 12600: loss 1.276459\n",
      "iteration 6500 / 12600: loss 1.145446\n",
      "iteration 6600 / 12600: loss 1.208415\n",
      "iteration 6700 / 12600: loss 1.083856\n",
      "iteration 6800 / 12600: loss 1.355365\n",
      "epoch done... acc 0.512\n",
      "iteration 6900 / 12600: loss 1.160326\n",
      "iteration 7000 / 12600: loss 1.025681\n",
      "iteration 7100 / 12600: loss 1.059570\n",
      "iteration 7200 / 12600: loss 1.226140\n",
      "iteration 7300 / 12600: loss 1.253165\n",
      "epoch done... acc 0.504\n",
      "iteration 7400 / 12600: loss 1.109728\n",
      "iteration 7500 / 12600: loss 1.191071\n",
      "iteration 7600 / 12600: loss 1.144405\n",
      "iteration 7700 / 12600: loss 1.138973\n",
      "iteration 7800 / 12600: loss 1.131219\n",
      "epoch done... acc 0.52\n",
      "iteration 7900 / 12600: loss 1.152221\n",
      "iteration 8000 / 12600: loss 1.175056\n",
      "iteration 8100 / 12600: loss 1.128422\n",
      "iteration 8200 / 12600: loss 1.189872\n",
      "iteration 8300 / 12600: loss 0.995709\n",
      "epoch done... acc 0.508\n",
      "iteration 8400 / 12600: loss 1.003819\n",
      "iteration 8500 / 12600: loss 1.178946\n",
      "iteration 8600 / 12600: loss 1.025118\n",
      "iteration 8700 / 12600: loss 1.067317\n",
      "iteration 8800 / 12600: loss 1.139626\n",
      "epoch done... acc 0.532\n",
      "iteration 8900 / 12600: loss 1.097460\n",
      "iteration 9000 / 12600: loss 1.154112\n",
      "iteration 9100 / 12600: loss 1.079100\n",
      "iteration 9200 / 12600: loss 1.017769\n",
      "iteration 9300 / 12600: loss 1.050117\n",
      "epoch done... acc 0.515\n",
      "iteration 9400 / 12600: loss 0.863822\n",
      "iteration 9500 / 12600: loss 1.089000\n",
      "iteration 9600 / 12600: loss 1.204864\n",
      "iteration 9700 / 12600: loss 1.111059\n",
      "iteration 9800 / 12600: loss 1.210333\n",
      "epoch done... acc 0.527\n",
      "iteration 9900 / 12600: loss 1.057925\n",
      "iteration 10000 / 12600: loss 1.035879\n",
      "iteration 10100 / 12600: loss 1.184128\n",
      "iteration 10200 / 12600: loss 1.206766\n",
      "epoch done... acc 0.523\n",
      "iteration 10300 / 12600: loss 1.111921\n",
      "iteration 10400 / 12600: loss 1.052021\n",
      "iteration 10500 / 12600: loss 0.937346\n",
      "iteration 10600 / 12600: loss 1.284728\n",
      "iteration 10700 / 12600: loss 1.083969\n",
      "epoch done... acc 0.524\n",
      "iteration 10800 / 12600: loss 1.208621\n",
      "iteration 10900 / 12600: loss 1.010608\n",
      "iteration 11000 / 12600: loss 1.206022\n",
      "iteration 11100 / 12600: loss 0.997075\n",
      "iteration 11200 / 12600: loss 1.096523\n",
      "epoch done... acc 0.514\n",
      "iteration 11300 / 12600: loss 0.914326\n",
      "iteration 11400 / 12600: loss 1.051382\n",
      "iteration 11500 / 12600: loss 1.233391\n",
      "iteration 11600 / 12600: loss 1.083013\n",
      "iteration 11700 / 12600: loss 1.057704\n",
      "epoch done... acc 0.516\n",
      "iteration 11800 / 12600: loss 0.978423\n",
      "iteration 11900 / 12600: loss 1.034723\n",
      "iteration 12000 / 12600: loss 1.115095\n",
      "iteration 12100 / 12600: loss 1.162144\n",
      "iteration 12200 / 12600: loss 1.014047\n",
      "epoch done... acc 0.515\n",
      "iteration 12300 / 12600: loss 0.918721\n",
      "iteration 12400 / 12600: loss 1.033789\n",
      "iteration 12500 / 12600: loss 1.021291\n",
      "Final training loss:  1.0002372296378454\n",
      "Final validation loss:  1.3872470042762777\n",
      "Final validation accuracy:  0.515\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "74 1 0 385 12600 100 0.001 0.98 0.515\n",
      "iteration 0 / 12600: loss 2.302537\n",
      "epoch done... acc 0.166\n",
      "iteration 100 / 12600: loss 1.992220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 200 / 12600: loss 1.897131\n",
      "iteration 300 / 12600: loss 1.862098\n",
      "iteration 400 / 12600: loss 1.800671\n",
      "epoch done... acc 0.387\n",
      "iteration 500 / 12600: loss 1.701451\n",
      "iteration 600 / 12600: loss 1.763533\n",
      "iteration 700 / 12600: loss 1.734378\n",
      "iteration 800 / 12600: loss 1.855762\n",
      "iteration 900 / 12600: loss 1.647030\n",
      "epoch done... acc 0.414\n",
      "iteration 1000 / 12600: loss 1.573474\n",
      "iteration 1100 / 12600: loss 1.699122\n",
      "iteration 1200 / 12600: loss 1.523400\n",
      "iteration 1300 / 12600: loss 1.709730\n",
      "iteration 1400 / 12600: loss 1.408265\n",
      "epoch done... acc 0.437\n",
      "iteration 1500 / 12600: loss 1.504447\n",
      "iteration 1600 / 12600: loss 1.786995\n",
      "iteration 1700 / 12600: loss 1.490081\n",
      "iteration 1800 / 12600: loss 1.480354\n",
      "iteration 1900 / 12600: loss 1.488295\n",
      "epoch done... acc 0.456\n",
      "iteration 2000 / 12600: loss 1.621624\n",
      "iteration 2100 / 12600: loss 1.301971\n",
      "iteration 2200 / 12600: loss 1.384841\n",
      "iteration 2300 / 12600: loss 1.397373\n",
      "iteration 2400 / 12600: loss 1.556959\n",
      "epoch done... acc 0.456\n",
      "iteration 2500 / 12600: loss 1.225385\n",
      "iteration 2600 / 12600: loss 1.415991\n",
      "iteration 2700 / 12600: loss 1.407414\n",
      "iteration 2800 / 12600: loss 1.482238\n",
      "iteration 2900 / 12600: loss 1.369181\n",
      "epoch done... acc 0.462\n",
      "iteration 3000 / 12600: loss 1.556544\n",
      "iteration 3100 / 12600: loss 1.236599\n",
      "iteration 3200 / 12600: loss 1.412751\n",
      "iteration 3300 / 12600: loss 1.269276\n",
      "iteration 3400 / 12600: loss 1.463239\n",
      "epoch done... acc 0.466\n",
      "iteration 3500 / 12600: loss 1.311117\n",
      "iteration 3600 / 12600: loss 1.363294\n",
      "iteration 3700 / 12600: loss 1.363265\n",
      "iteration 3800 / 12600: loss 1.512597\n",
      "iteration 3900 / 12600: loss 1.508832\n",
      "epoch done... acc 0.479\n",
      "iteration 4000 / 12600: loss 1.485519\n",
      "iteration 4100 / 12600: loss 1.612186\n",
      "iteration 4200 / 12600: loss 1.350681\n",
      "iteration 4300 / 12600: loss 1.478055\n",
      "iteration 4400 / 12600: loss 1.476350\n",
      "epoch done... acc 0.482\n",
      "iteration 4500 / 12600: loss 1.321579\n",
      "iteration 4600 / 12600: loss 1.410863\n",
      "iteration 4700 / 12600: loss 1.377659\n",
      "iteration 4800 / 12600: loss 1.379414\n",
      "iteration 4900 / 12600: loss 1.236071\n",
      "epoch done... acc 0.49\n",
      "iteration 5000 / 12600: loss 1.230566\n",
      "iteration 5100 / 12600: loss 1.312647\n",
      "iteration 5200 / 12600: loss 1.350337\n",
      "iteration 5300 / 12600: loss 1.547299\n",
      "epoch done... acc 0.485\n",
      "iteration 5400 / 12600: loss 1.153371\n",
      "iteration 5500 / 12600: loss 1.271781\n",
      "iteration 5600 / 12600: loss 1.252255\n",
      "iteration 5700 / 12600: loss 1.470818\n",
      "iteration 5800 / 12600: loss 1.276256\n",
      "epoch done... acc 0.495\n",
      "iteration 5900 / 12600: loss 1.281643\n",
      "iteration 6000 / 12600: loss 1.394934\n",
      "iteration 6100 / 12600: loss 1.332142\n",
      "iteration 6200 / 12600: loss 1.238077\n",
      "iteration 6300 / 12600: loss 1.214994\n",
      "epoch done... acc 0.509\n",
      "iteration 6400 / 12600: loss 1.163166\n",
      "iteration 6500 / 12600: loss 1.245712\n",
      "iteration 6600 / 12600: loss 1.238896\n",
      "iteration 6700 / 12600: loss 1.180049\n",
      "iteration 6800 / 12600: loss 1.155931\n",
      "epoch done... acc 0.499\n",
      "iteration 6900 / 12600: loss 1.276611\n",
      "iteration 7000 / 12600: loss 1.218853\n",
      "iteration 7100 / 12600: loss 1.399525\n",
      "iteration 7200 / 12600: loss 1.298795\n",
      "iteration 7300 / 12600: loss 1.281771\n",
      "epoch done... acc 0.514\n",
      "iteration 7400 / 12600: loss 1.244759\n",
      "iteration 7500 / 12600: loss 1.260510\n",
      "iteration 7600 / 12600: loss 1.261722\n",
      "iteration 7700 / 12600: loss 1.443240\n",
      "iteration 7800 / 12600: loss 1.256348\n",
      "epoch done... acc 0.511\n",
      "iteration 7900 / 12600: loss 1.071030\n",
      "iteration 8000 / 12600: loss 1.432994\n",
      "iteration 8100 / 12600: loss 1.171477\n",
      "iteration 8200 / 12600: loss 1.280685\n",
      "iteration 8300 / 12600: loss 1.240410\n",
      "epoch done... acc 0.505\n",
      "iteration 8400 / 12600: loss 1.190593\n",
      "iteration 8500 / 12600: loss 1.085492\n",
      "iteration 8600 / 12600: loss 1.028017\n",
      "iteration 8700 / 12600: loss 1.247118\n",
      "iteration 8800 / 12600: loss 1.322058\n",
      "epoch done... acc 0.519\n",
      "iteration 8900 / 12600: loss 1.150495\n",
      "iteration 9000 / 12600: loss 1.159871\n",
      "iteration 9100 / 12600: loss 1.375794\n",
      "iteration 9200 / 12600: loss 1.231099\n",
      "iteration 9300 / 12600: loss 1.008162\n",
      "epoch done... acc 0.519\n",
      "iteration 9400 / 12600: loss 1.105660\n",
      "iteration 9500 / 12600: loss 1.216049\n",
      "iteration 9600 / 12600: loss 1.216095\n",
      "iteration 9700 / 12600: loss 1.129454\n",
      "iteration 9800 / 12600: loss 1.163203\n",
      "epoch done... acc 0.52\n",
      "iteration 9900 / 12600: loss 1.004496\n",
      "iteration 10000 / 12600: loss 0.995951\n",
      "iteration 10100 / 12600: loss 1.234608\n",
      "iteration 10200 / 12600: loss 1.142930\n",
      "epoch done... acc 0.526\n",
      "iteration 10300 / 12600: loss 1.153426\n",
      "iteration 10400 / 12600: loss 1.248441\n",
      "iteration 10500 / 12600: loss 1.213694\n",
      "iteration 10600 / 12600: loss 0.980985\n",
      "iteration 10700 / 12600: loss 1.268503\n",
      "epoch done... acc 0.524\n",
      "iteration 10800 / 12600: loss 1.228169\n",
      "iteration 10900 / 12600: loss 0.973368\n",
      "iteration 11000 / 12600: loss 1.080065\n",
      "iteration 11100 / 12600: loss 1.237330\n",
      "iteration 11200 / 12600: loss 1.031815\n",
      "epoch done... acc 0.516\n",
      "iteration 11300 / 12600: loss 1.113191\n",
      "iteration 11400 / 12600: loss 0.864821\n",
      "iteration 11500 / 12600: loss 0.993959\n",
      "iteration 11600 / 12600: loss 1.065854\n",
      "iteration 11700 / 12600: loss 0.960975\n",
      "epoch done... acc 0.511\n",
      "iteration 11800 / 12600: loss 0.981908\n",
      "iteration 11900 / 12600: loss 1.164658\n",
      "iteration 12000 / 12600: loss 1.304111\n",
      "iteration 12100 / 12600: loss 1.086759\n",
      "iteration 12200 / 12600: loss 1.008346\n",
      "epoch done... acc 0.533\n",
      "iteration 12300 / 12600: loss 1.010655\n",
      "iteration 12400 / 12600: loss 1.112995\n",
      "iteration 12500 / 12600: loss 1.194239\n",
      "Final training loss:  0.984888236551379\n",
      "Final validation loss:  1.3666898840628319\n",
      "Final validation accuracy:  0.533\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "75 2 0 385 12600 100 0.001 0.98 0.533\n",
      "iteration 0 / 13860: loss 2.302544\n",
      "epoch done... acc 0.122\n",
      "iteration 100 / 13860: loss 2.052918\n",
      "iteration 200 / 13860: loss 1.810062\n",
      "iteration 300 / 13860: loss 1.913315\n",
      "iteration 400 / 13860: loss 1.786230\n",
      "epoch done... acc 0.382\n",
      "iteration 500 / 13860: loss 1.882073\n",
      "iteration 600 / 13860: loss 1.715161\n",
      "iteration 700 / 13860: loss 1.696026\n",
      "iteration 800 / 13860: loss 1.857258\n",
      "iteration 900 / 13860: loss 1.607793\n",
      "epoch done... acc 0.426\n",
      "iteration 1000 / 13860: loss 1.521057\n",
      "iteration 1100 / 13860: loss 1.644929\n",
      "iteration 1200 / 13860: loss 1.544076\n",
      "iteration 1300 / 13860: loss 1.624763\n",
      "iteration 1400 / 13860: loss 1.549934\n",
      "epoch done... acc 0.439\n",
      "iteration 1500 / 13860: loss 1.274554\n",
      "iteration 1600 / 13860: loss 1.363722\n",
      "iteration 1700 / 13860: loss 1.366702\n",
      "iteration 1800 / 13860: loss 1.608995\n",
      "iteration 1900 / 13860: loss 1.336558\n",
      "epoch done... acc 0.453\n",
      "iteration 2000 / 13860: loss 1.631163\n",
      "iteration 2100 / 13860: loss 1.578059\n",
      "iteration 2200 / 13860: loss 1.443638\n",
      "iteration 2300 / 13860: loss 1.614770\n",
      "iteration 2400 / 13860: loss 1.482603\n",
      "epoch done... acc 0.475\n",
      "iteration 2500 / 13860: loss 1.385590\n",
      "iteration 2600 / 13860: loss 1.412775\n",
      "iteration 2700 / 13860: loss 1.312495\n",
      "iteration 2800 / 13860: loss 1.696046\n",
      "iteration 2900 / 13860: loss 1.384870\n",
      "epoch done... acc 0.473\n",
      "iteration 3000 / 13860: loss 1.359831\n",
      "iteration 3100 / 13860: loss 1.420250\n",
      "iteration 3200 / 13860: loss 1.304550\n",
      "iteration 3300 / 13860: loss 1.428507\n",
      "iteration 3400 / 13860: loss 1.499896\n",
      "epoch done... acc 0.49\n",
      "iteration 3500 / 13860: loss 1.396454\n",
      "iteration 3600 / 13860: loss 1.240526\n",
      "iteration 3700 / 13860: loss 1.263448\n",
      "iteration 3800 / 13860: loss 1.214271\n",
      "iteration 3900 / 13860: loss 1.429707\n",
      "epoch done... acc 0.482\n",
      "iteration 4000 / 13860: loss 1.241867\n",
      "iteration 4100 / 13860: loss 1.280135\n",
      "iteration 4200 / 13860: loss 1.412370\n",
      "iteration 4300 / 13860: loss 1.176197\n",
      "iteration 4400 / 13860: loss 1.399652\n",
      "epoch done... acc 0.492\n",
      "iteration 4500 / 13860: loss 1.246872\n",
      "iteration 4600 / 13860: loss 1.240940\n",
      "iteration 4700 / 13860: loss 1.300368\n",
      "iteration 4800 / 13860: loss 1.337115\n",
      "iteration 4900 / 13860: loss 1.250151\n",
      "epoch done... acc 0.489\n",
      "iteration 5000 / 13860: loss 1.330867\n",
      "iteration 5100 / 13860: loss 1.356466\n",
      "iteration 5200 / 13860: loss 1.134835\n",
      "iteration 5300 / 13860: loss 1.558079\n",
      "epoch done... acc 0.487\n",
      "iteration 5400 / 13860: loss 1.345698\n",
      "iteration 5500 / 13860: loss 1.101000\n",
      "iteration 5600 / 13860: loss 1.241554\n",
      "iteration 5700 / 13860: loss 1.306723\n",
      "iteration 5800 / 13860: loss 1.355762\n",
      "epoch done... acc 0.519\n",
      "iteration 5900 / 13860: loss 1.172819\n",
      "iteration 6000 / 13860: loss 1.332891\n",
      "iteration 6100 / 13860: loss 1.070260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6200 / 13860: loss 1.269160\n",
      "iteration 6300 / 13860: loss 1.254036\n",
      "epoch done... acc 0.509\n",
      "iteration 6400 / 13860: loss 1.089959\n",
      "iteration 6500 / 13860: loss 1.162474\n",
      "iteration 6600 / 13860: loss 1.264200\n",
      "iteration 6700 / 13860: loss 1.318614\n",
      "iteration 6800 / 13860: loss 1.102437\n",
      "epoch done... acc 0.523\n",
      "iteration 6900 / 13860: loss 1.209709\n",
      "iteration 7000 / 13860: loss 1.380982\n",
      "iteration 7100 / 13860: loss 1.080351\n",
      "iteration 7200 / 13860: loss 1.235828\n",
      "iteration 7300 / 13860: loss 1.186245\n",
      "epoch done... acc 0.512\n",
      "iteration 7400 / 13860: loss 1.219942\n",
      "iteration 7500 / 13860: loss 1.299121\n",
      "iteration 7600 / 13860: loss 1.127443\n",
      "iteration 7700 / 13860: loss 1.121295\n",
      "iteration 7800 / 13860: loss 1.055692\n",
      "epoch done... acc 0.524\n",
      "iteration 7900 / 13860: loss 1.308705\n",
      "iteration 8000 / 13860: loss 1.296634\n",
      "iteration 8100 / 13860: loss 1.131398\n",
      "iteration 8200 / 13860: loss 1.237004\n",
      "iteration 8300 / 13860: loss 1.279836\n",
      "epoch done... acc 0.524\n",
      "iteration 8400 / 13860: loss 1.073876\n",
      "iteration 8500 / 13860: loss 1.113514\n",
      "iteration 8600 / 13860: loss 1.080704\n",
      "iteration 8700 / 13860: loss 1.324915\n",
      "iteration 8800 / 13860: loss 1.141463\n",
      "epoch done... acc 0.532\n",
      "iteration 8900 / 13860: loss 1.084743\n",
      "iteration 9000 / 13860: loss 1.162573\n",
      "iteration 9100 / 13860: loss 1.122153\n",
      "iteration 9200 / 13860: loss 1.192491\n",
      "iteration 9300 / 13860: loss 1.129604\n",
      "epoch done... acc 0.522\n",
      "iteration 9400 / 13860: loss 1.268230\n",
      "iteration 9500 / 13860: loss 0.970649\n",
      "iteration 9600 / 13860: loss 1.256563\n",
      "iteration 9700 / 13860: loss 1.144683\n",
      "iteration 9800 / 13860: loss 0.996027\n",
      "epoch done... acc 0.542\n",
      "iteration 9900 / 13860: loss 1.124186\n",
      "iteration 10000 / 13860: loss 1.067635\n",
      "iteration 10100 / 13860: loss 1.031022\n",
      "iteration 10200 / 13860: loss 1.089110\n",
      "epoch done... acc 0.525\n",
      "iteration 10300 / 13860: loss 1.102563\n",
      "iteration 10400 / 13860: loss 1.174382\n",
      "iteration 10500 / 13860: loss 1.207950\n",
      "iteration 10600 / 13860: loss 1.117132\n",
      "iteration 10700 / 13860: loss 0.913266\n",
      "epoch done... acc 0.529\n",
      "iteration 10800 / 13860: loss 1.079649\n",
      "iteration 10900 / 13860: loss 1.079513\n",
      "iteration 11000 / 13860: loss 1.092106\n",
      "iteration 11100 / 13860: loss 0.953122\n",
      "iteration 11200 / 13860: loss 1.070267\n",
      "epoch done... acc 0.543\n",
      "iteration 11300 / 13860: loss 1.111441\n",
      "iteration 11400 / 13860: loss 0.912648\n",
      "iteration 11500 / 13860: loss 1.013605\n",
      "iteration 11600 / 13860: loss 1.077041\n",
      "iteration 11700 / 13860: loss 1.256797\n",
      "epoch done... acc 0.529\n",
      "iteration 11800 / 13860: loss 1.231998\n",
      "iteration 11900 / 13860: loss 1.274612\n",
      "iteration 12000 / 13860: loss 1.070699\n",
      "iteration 12100 / 13860: loss 1.236883\n",
      "iteration 12200 / 13860: loss 1.274645\n",
      "epoch done... acc 0.534\n",
      "iteration 12300 / 13860: loss 1.131712\n",
      "iteration 12400 / 13860: loss 1.140856\n",
      "iteration 12500 / 13860: loss 0.918027\n",
      "iteration 12600 / 13860: loss 0.935060\n",
      "iteration 12700 / 13860: loss 1.159332\n",
      "epoch done... acc 0.545\n",
      "iteration 12800 / 13860: loss 1.262211\n",
      "iteration 12900 / 13860: loss 1.059417\n",
      "iteration 13000 / 13860: loss 1.135671\n",
      "iteration 13100 / 13860: loss 1.135177\n",
      "iteration 13200 / 13860: loss 1.044539\n",
      "epoch done... acc 0.531\n",
      "iteration 13300 / 13860: loss 1.177689\n",
      "iteration 13400 / 13860: loss 1.122353\n",
      "iteration 13500 / 13860: loss 0.918017\n",
      "iteration 13600 / 13860: loss 1.104170\n",
      "iteration 13700 / 13860: loss 1.039757\n",
      "epoch done... acc 0.519\n",
      "iteration 13800 / 13860: loss 0.974029\n",
      "Final training loss:  0.9704623222585846\n",
      "Final validation loss:  1.3457169782967768\n",
      "Final validation accuracy:  0.519\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "76 1 1 385 12600 100 0.001 0.98 0.519\n",
      "iteration 0 / 11340: loss 2.302501\n",
      "epoch done... acc 0.18\n",
      "iteration 100 / 11340: loss 2.079376\n",
      "iteration 200 / 11340: loss 1.853914\n",
      "iteration 300 / 11340: loss 1.898925\n",
      "iteration 400 / 11340: loss 1.750752\n",
      "epoch done... acc 0.389\n",
      "iteration 500 / 11340: loss 1.784115\n",
      "iteration 600 / 11340: loss 1.638131\n",
      "iteration 700 / 11340: loss 1.660728\n",
      "iteration 800 / 11340: loss 1.628435\n",
      "iteration 900 / 11340: loss 1.594651\n",
      "epoch done... acc 0.426\n",
      "iteration 1000 / 11340: loss 1.564473\n",
      "iteration 1100 / 11340: loss 1.490307\n",
      "iteration 1200 / 11340: loss 1.524969\n",
      "iteration 1300 / 11340: loss 1.370677\n",
      "iteration 1400 / 11340: loss 1.752831\n",
      "epoch done... acc 0.446\n",
      "iteration 1500 / 11340: loss 1.336187\n",
      "iteration 1600 / 11340: loss 1.500301\n",
      "iteration 1700 / 11340: loss 1.464689\n",
      "iteration 1800 / 11340: loss 1.570897\n",
      "iteration 1900 / 11340: loss 1.378690\n",
      "epoch done... acc 0.451\n",
      "iteration 2000 / 11340: loss 1.498519\n",
      "iteration 2100 / 11340: loss 1.449454\n",
      "iteration 2200 / 11340: loss 1.533805\n",
      "iteration 2300 / 11340: loss 1.614066\n",
      "iteration 2400 / 11340: loss 1.284685\n",
      "epoch done... acc 0.463\n",
      "iteration 2500 / 11340: loss 1.321683\n",
      "iteration 2600 / 11340: loss 1.447671\n",
      "iteration 2700 / 11340: loss 1.530795\n",
      "iteration 2800 / 11340: loss 1.373234\n",
      "iteration 2900 / 11340: loss 1.368647\n",
      "epoch done... acc 0.479\n",
      "iteration 3000 / 11340: loss 1.499861\n",
      "iteration 3100 / 11340: loss 1.341064\n",
      "iteration 3200 / 11340: loss 1.487182\n",
      "iteration 3300 / 11340: loss 1.466700\n",
      "iteration 3400 / 11340: loss 1.436878\n",
      "epoch done... acc 0.487\n",
      "iteration 3500 / 11340: loss 1.369308\n",
      "iteration 3600 / 11340: loss 1.251472\n",
      "iteration 3700 / 11340: loss 1.428543\n",
      "iteration 3800 / 11340: loss 1.310524\n",
      "iteration 3900 / 11340: loss 1.377365\n",
      "epoch done... acc 0.483\n",
      "iteration 4000 / 11340: loss 1.433713\n",
      "iteration 4100 / 11340: loss 1.403278\n",
      "iteration 4200 / 11340: loss 1.270375\n",
      "iteration 4300 / 11340: loss 1.372184\n",
      "iteration 4400 / 11340: loss 1.166345\n",
      "epoch done... acc 0.474\n",
      "iteration 4500 / 11340: loss 1.121873\n",
      "iteration 4600 / 11340: loss 1.318836\n",
      "iteration 4700 / 11340: loss 1.330699\n",
      "iteration 4800 / 11340: loss 1.193359\n",
      "iteration 4900 / 11340: loss 1.288225\n",
      "epoch done... acc 0.5\n",
      "iteration 5000 / 11340: loss 1.225000\n",
      "iteration 5100 / 11340: loss 1.465838\n",
      "iteration 5200 / 11340: loss 1.115572\n",
      "iteration 5300 / 11340: loss 1.388782\n",
      "epoch done... acc 0.496\n",
      "iteration 5400 / 11340: loss 1.423630\n",
      "iteration 5500 / 11340: loss 1.153513\n",
      "iteration 5600 / 11340: loss 1.104903\n",
      "iteration 5700 / 11340: loss 1.329561\n",
      "iteration 5800 / 11340: loss 1.194582\n",
      "epoch done... acc 0.503\n",
      "iteration 5900 / 11340: loss 1.380965\n",
      "iteration 6000 / 11340: loss 1.507898\n",
      "iteration 6100 / 11340: loss 1.204451\n",
      "iteration 6200 / 11340: loss 1.330117\n",
      "iteration 6300 / 11340: loss 1.339099\n",
      "epoch done... acc 0.506\n",
      "iteration 6400 / 11340: loss 1.244357\n",
      "iteration 6500 / 11340: loss 1.243404\n",
      "iteration 6600 / 11340: loss 1.215888\n",
      "iteration 6700 / 11340: loss 1.224258\n",
      "iteration 6800 / 11340: loss 1.136693\n",
      "epoch done... acc 0.51\n",
      "iteration 6900 / 11340: loss 1.261591\n",
      "iteration 7000 / 11340: loss 1.265812\n",
      "iteration 7100 / 11340: loss 1.356300\n",
      "iteration 7200 / 11340: loss 1.270570\n",
      "iteration 7300 / 11340: loss 1.424603\n",
      "epoch done... acc 0.532\n",
      "iteration 7400 / 11340: loss 1.282224\n",
      "iteration 7500 / 11340: loss 1.158662\n",
      "iteration 7600 / 11340: loss 1.303122\n",
      "iteration 7700 / 11340: loss 1.285475\n",
      "iteration 7800 / 11340: loss 1.286781\n",
      "epoch done... acc 0.515\n",
      "iteration 7900 / 11340: loss 1.161883\n",
      "iteration 8000 / 11340: loss 1.169394\n",
      "iteration 8100 / 11340: loss 1.346205\n",
      "iteration 8200 / 11340: loss 1.068510\n",
      "iteration 8300 / 11340: loss 1.156733\n",
      "epoch done... acc 0.531\n",
      "iteration 8400 / 11340: loss 1.036980\n",
      "iteration 8500 / 11340: loss 1.048861\n",
      "iteration 8600 / 11340: loss 1.083074\n",
      "iteration 8700 / 11340: loss 1.141767\n",
      "iteration 8800 / 11340: loss 1.249624\n",
      "epoch done... acc 0.533\n",
      "iteration 8900 / 11340: loss 1.288821\n",
      "iteration 9000 / 11340: loss 0.918008\n",
      "iteration 9100 / 11340: loss 1.118272\n",
      "iteration 9200 / 11340: loss 1.105549\n",
      "iteration 9300 / 11340: loss 1.092250\n",
      "epoch done... acc 0.511\n",
      "iteration 9400 / 11340: loss 1.045956\n",
      "iteration 9500 / 11340: loss 1.210895\n",
      "iteration 9600 / 11340: loss 1.096560\n",
      "iteration 9700 / 11340: loss 1.033386\n",
      "iteration 9800 / 11340: loss 1.070367\n",
      "epoch done... acc 0.518\n",
      "iteration 9900 / 11340: loss 1.268596\n",
      "iteration 10000 / 11340: loss 1.124695\n",
      "iteration 10100 / 11340: loss 1.064256\n",
      "iteration 10200 / 11340: loss 0.953425\n",
      "epoch done... acc 0.521\n",
      "iteration 10300 / 11340: loss 1.413601\n",
      "iteration 10400 / 11340: loss 1.039281\n",
      "iteration 10500 / 11340: loss 1.084421\n",
      "iteration 10600 / 11340: loss 1.064636\n",
      "iteration 10700 / 11340: loss 1.003697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch done... acc 0.53\n",
      "iteration 10800 / 11340: loss 1.078697\n",
      "iteration 10900 / 11340: loss 1.159791\n",
      "iteration 11000 / 11340: loss 1.117291\n",
      "iteration 11100 / 11340: loss 1.184448\n",
      "iteration 11200 / 11340: loss 1.241294\n",
      "epoch done... acc 0.522\n",
      "iteration 11300 / 11340: loss 1.149098\n",
      "Final training loss:  1.0778331028069372\n",
      "Final validation loss:  1.3659128918757484\n",
      "Final validation accuracy:  0.522\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "77 2 1 385 12600 100 0.001 0.98 0.522\n",
      "iteration 0 / 12600: loss 2.302584\n",
      "epoch done... acc 0.148\n",
      "iteration 100 / 12600: loss 2.097794\n",
      "iteration 200 / 12600: loss 1.802897\n",
      "iteration 300 / 12600: loss 1.746125\n",
      "iteration 400 / 12600: loss 1.850782\n",
      "epoch done... acc 0.372\n",
      "iteration 500 / 12600: loss 1.729122\n",
      "iteration 600 / 12600: loss 1.718670\n",
      "iteration 700 / 12600: loss 1.712995\n",
      "iteration 800 / 12600: loss 1.653917\n",
      "iteration 900 / 12600: loss 1.619038\n",
      "epoch done... acc 0.426\n",
      "iteration 1000 / 12600: loss 1.691072\n",
      "iteration 1100 / 12600: loss 1.635531\n",
      "iteration 1200 / 12600: loss 1.584245\n",
      "iteration 1300 / 12600: loss 1.529624\n",
      "iteration 1400 / 12600: loss 1.547185\n",
      "epoch done... acc 0.451\n",
      "iteration 1500 / 12600: loss 1.529588\n",
      "iteration 1600 / 12600: loss 1.603418\n",
      "iteration 1700 / 12600: loss 1.591524\n",
      "iteration 1800 / 12600: loss 1.428892\n",
      "iteration 1900 / 12600: loss 1.467029\n",
      "epoch done... acc 0.454\n",
      "iteration 2000 / 12600: loss 1.490016\n",
      "iteration 2100 / 12600: loss 1.514655\n",
      "iteration 2200 / 12600: loss 1.485467\n",
      "iteration 2300 / 12600: loss 1.290756\n",
      "iteration 2400 / 12600: loss 1.596459\n",
      "epoch done... acc 0.471\n",
      "iteration 2500 / 12600: loss 1.335660\n",
      "iteration 2600 / 12600: loss 1.394661\n",
      "iteration 2700 / 12600: loss 1.397330\n",
      "iteration 2800 / 12600: loss 1.344255\n",
      "iteration 2900 / 12600: loss 1.445090\n",
      "epoch done... acc 0.475\n",
      "iteration 3000 / 12600: loss 1.491560\n",
      "iteration 3100 / 12600: loss 1.437683\n",
      "iteration 3200 / 12600: loss 1.234799\n",
      "iteration 3300 / 12600: loss 1.320045\n",
      "iteration 3400 / 12600: loss 1.399838\n",
      "epoch done... acc 0.483\n",
      "iteration 3500 / 12600: loss 1.468930\n",
      "iteration 3600 / 12600: loss 1.298558\n",
      "iteration 3700 / 12600: loss 1.615875\n",
      "iteration 3800 / 12600: loss 1.302763\n",
      "iteration 3900 / 12600: loss 1.548348\n",
      "epoch done... acc 0.474\n",
      "iteration 4000 / 12600: loss 1.687201\n",
      "iteration 4100 / 12600: loss 1.435363\n",
      "iteration 4200 / 12600: loss 1.257865\n",
      "iteration 4300 / 12600: loss 1.371782\n",
      "iteration 4400 / 12600: loss 1.428337\n",
      "epoch done... acc 0.489\n",
      "iteration 4500 / 12600: loss 1.316506\n",
      "iteration 4600 / 12600: loss 1.336036\n",
      "iteration 4700 / 12600: loss 1.345319\n",
      "iteration 4800 / 12600: loss 1.273057\n",
      "iteration 4900 / 12600: loss 1.421349\n",
      "epoch done... acc 0.502\n",
      "iteration 5000 / 12600: loss 1.382175\n",
      "iteration 5100 / 12600: loss 1.332581\n",
      "iteration 5200 / 12600: loss 1.395759\n",
      "iteration 5300 / 12600: loss 1.386136\n",
      "epoch done... acc 0.49\n",
      "iteration 5400 / 12600: loss 1.198319\n",
      "iteration 5500 / 12600: loss 1.177328\n",
      "iteration 5600 / 12600: loss 1.223742\n",
      "iteration 5700 / 12600: loss 1.190624\n",
      "iteration 5800 / 12600: loss 1.187304\n",
      "epoch done... acc 0.519\n",
      "iteration 5900 / 12600: loss 1.302035\n",
      "iteration 6000 / 12600: loss 1.234360\n",
      "iteration 6100 / 12600: loss 1.085097\n",
      "iteration 6200 / 12600: loss 1.225472\n",
      "iteration 6300 / 12600: loss 1.173054\n",
      "epoch done... acc 0.5\n",
      "iteration 6400 / 12600: loss 1.376874\n",
      "iteration 6500 / 12600: loss 1.247229\n",
      "iteration 6600 / 12600: loss 1.328081\n",
      "iteration 6700 / 12600: loss 1.268680\n",
      "iteration 6800 / 12600: loss 1.246535\n",
      "epoch done... acc 0.499\n",
      "iteration 6900 / 12600: loss 1.241098\n",
      "iteration 7000 / 12600: loss 1.207161\n",
      "iteration 7100 / 12600: loss 1.266707\n",
      "iteration 7200 / 12600: loss 1.406956\n",
      "iteration 7300 / 12600: loss 1.123777\n",
      "epoch done... acc 0.516\n",
      "iteration 7400 / 12600: loss 1.223623\n",
      "iteration 7500 / 12600: loss 1.230202\n",
      "iteration 7600 / 12600: loss 1.137207\n",
      "iteration 7700 / 12600: loss 1.181656\n",
      "iteration 7800 / 12600: loss 1.266759\n",
      "epoch done... acc 0.525\n",
      "iteration 7900 / 12600: loss 1.168051\n",
      "iteration 8000 / 12600: loss 1.259103\n",
      "iteration 8100 / 12600: loss 1.103726\n",
      "iteration 8200 / 12600: loss 1.059195\n",
      "iteration 8300 / 12600: loss 1.261088\n",
      "epoch done... acc 0.51\n",
      "iteration 8400 / 12600: loss 1.325850\n",
      "iteration 8500 / 12600: loss 1.154924\n",
      "iteration 8600 / 12600: loss 0.925840\n",
      "iteration 8700 / 12600: loss 1.177398\n",
      "iteration 8800 / 12600: loss 1.085809\n",
      "epoch done... acc 0.508\n",
      "iteration 8900 / 12600: loss 1.331616\n",
      "iteration 9000 / 12600: loss 1.125484\n",
      "iteration 9100 / 12600: loss 1.279041\n",
      "iteration 9200 / 12600: loss 1.065167\n",
      "iteration 9300 / 12600: loss 0.977371\n",
      "epoch done... acc 0.522\n",
      "iteration 9400 / 12600: loss 1.210403\n",
      "iteration 9500 / 12600: loss 1.134605\n",
      "iteration 9600 / 12600: loss 1.134443\n",
      "iteration 9700 / 12600: loss 0.999211\n",
      "iteration 9800 / 12600: loss 1.165010\n",
      "epoch done... acc 0.518\n",
      "iteration 9900 / 12600: loss 1.187795\n",
      "iteration 10000 / 12600: loss 1.180822\n",
      "iteration 10100 / 12600: loss 0.968888\n",
      "iteration 10200 / 12600: loss 1.023091\n",
      "epoch done... acc 0.517\n",
      "iteration 10300 / 12600: loss 1.093800\n",
      "iteration 10400 / 12600: loss 1.153758\n",
      "iteration 10500 / 12600: loss 0.998418\n",
      "iteration 10600 / 12600: loss 1.198322\n",
      "iteration 10700 / 12600: loss 1.243053\n",
      "epoch done... acc 0.519\n",
      "iteration 10800 / 12600: loss 1.088291\n",
      "iteration 10900 / 12600: loss 1.102393\n",
      "iteration 11000 / 12600: loss 1.024233\n",
      "iteration 11100 / 12600: loss 1.115516\n",
      "iteration 11200 / 12600: loss 1.076395\n",
      "epoch done... acc 0.541\n",
      "iteration 11300 / 12600: loss 0.921494\n",
      "iteration 11400 / 12600: loss 1.050141\n",
      "iteration 11500 / 12600: loss 1.039676\n",
      "iteration 11600 / 12600: loss 1.082039\n",
      "iteration 11700 / 12600: loss 0.988173\n",
      "epoch done... acc 0.535\n",
      "iteration 11800 / 12600: loss 1.134140\n",
      "iteration 11900 / 12600: loss 0.918428\n",
      "iteration 12000 / 12600: loss 1.180611\n",
      "iteration 12100 / 12600: loss 1.056451\n",
      "iteration 12200 / 12600: loss 1.052376\n",
      "epoch done... acc 0.526\n",
      "iteration 12300 / 12600: loss 1.024960\n",
      "iteration 12400 / 12600: loss 1.008346\n",
      "iteration 12500 / 12600: loss 0.879976\n",
      "Final training loss:  1.1640920541009805\n",
      "Final validation loss:  1.3669060491742588\n",
      "Final validation accuracy:  0.526\n",
      "i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc\n",
      "78 1 0 385 12600 100 0.001 0.98 0.526\n",
      "iteration 0 / 12600: loss 2.302553\n",
      "epoch done... acc 0.192\n",
      "iteration 100 / 12600: loss 2.043362\n",
      "iteration 200 / 12600: loss 1.858771\n",
      "iteration 300 / 12600: loss 1.836122\n",
      "iteration 400 / 12600: loss 1.727295\n",
      "epoch done... acc 0.373\n",
      "iteration 500 / 12600: loss 1.709195\n",
      "iteration 600 / 12600: loss 1.809359\n",
      "iteration 700 / 12600: loss 1.774040\n",
      "iteration 800 / 12600: loss 1.647965\n",
      "iteration 900 / 12600: loss 1.649804\n",
      "epoch done... acc 0.428\n",
      "iteration 1000 / 12600: loss 1.638696\n",
      "iteration 1100 / 12600: loss 1.533313\n",
      "iteration 1200 / 12600: loss 1.491101\n",
      "iteration 1300 / 12600: loss 1.572747\n",
      "iteration 1400 / 12600: loss 1.595269\n",
      "epoch done... acc 0.44\n",
      "iteration 1500 / 12600: loss 1.561562\n",
      "iteration 1600 / 12600: loss 1.392851\n",
      "iteration 1700 / 12600: loss 1.686636\n",
      "iteration 1800 / 12600: loss 1.465667\n",
      "iteration 1900 / 12600: loss 1.538220\n",
      "epoch done... acc 0.453\n",
      "iteration 2000 / 12600: loss 1.623147\n",
      "iteration 2100 / 12600: loss 1.542354\n",
      "iteration 2200 / 12600: loss 1.576401\n",
      "iteration 2300 / 12600: loss 1.380617\n",
      "iteration 2400 / 12600: loss 1.340207\n",
      "epoch done... acc 0.457\n",
      "iteration 2500 / 12600: loss 1.454014\n",
      "iteration 2600 / 12600: loss 1.395269\n",
      "iteration 2700 / 12600: loss 1.587441\n",
      "iteration 2800 / 12600: loss 1.435635\n",
      "iteration 2900 / 12600: loss 1.615268\n",
      "epoch done... acc 0.473\n",
      "iteration 3000 / 12600: loss 1.295641\n",
      "iteration 3100 / 12600: loss 1.511458\n",
      "iteration 3200 / 12600: loss 1.322789\n",
      "iteration 3300 / 12600: loss 1.369600\n",
      "iteration 3400 / 12600: loss 1.517393\n",
      "epoch done... acc 0.486\n",
      "iteration 3500 / 12600: loss 1.463950\n",
      "iteration 3600 / 12600: loss 1.239060\n",
      "iteration 3700 / 12600: loss 1.238727\n",
      "iteration 3800 / 12600: loss 1.319616\n",
      "iteration 3900 / 12600: loss 1.463581\n",
      "epoch done... acc 0.483\n",
      "iteration 4000 / 12600: loss 1.506562\n",
      "iteration 4100 / 12600: loss 1.386833\n",
      "iteration 4200 / 12600: loss 1.364286\n",
      "iteration 4300 / 12600: loss 1.282489\n",
      "iteration 4400 / 12600: loss 1.295119\n",
      "epoch done... acc 0.474\n",
      "iteration 4500 / 12600: loss 1.244977\n",
      "iteration 4600 / 12600: loss 1.281624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4700 / 12600: loss 1.397115\n",
      "iteration 4800 / 12600: loss 1.364435\n",
      "iteration 4900 / 12600: loss 1.316922\n",
      "epoch done... acc 0.501\n",
      "iteration 5000 / 12600: loss 1.243886\n",
      "iteration 5100 / 12600: loss 1.417682\n",
      "iteration 5200 / 12600: loss 1.151995\n",
      "iteration 5300 / 12600: loss 1.426096\n",
      "epoch done... acc 0.494\n",
      "iteration 5400 / 12600: loss 1.307826\n",
      "iteration 5500 / 12600: loss 1.423592\n",
      "iteration 5600 / 12600: loss 1.397645\n",
      "iteration 5700 / 12600: loss 1.270136\n",
      "iteration 5800 / 12600: loss 1.247701\n",
      "epoch done... acc 0.493\n",
      "iteration 5900 / 12600: loss 1.262029\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-27e31a551fe8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    405\u001b[0m         stats = net.train(X_train, y_train, X_val, y_val,\n\u001b[1;32m    406\u001b[0m                             \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate_decay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m                           num_iters=num_iter, batch_size=batch_size, verbose=True)\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Final training loss: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss_history'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-27e31a551fe8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, X_val, y_val, learning_rate, learning_rate_decay, num_iters, batch_size, verbose)\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0;31m# Berechnung des Fehlers mit den aktuellen Parametern (W, b)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;31m# mit dem Testset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m             \u001b[0mloss_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma1_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma2_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m             \u001b[0mloss_val_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-27e31a551fe8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# Nutzen Sie die ReLU Aktivierungsfunktion im ersten Layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# Berechnen Sie die Klassenwahrscheinlichkeiten unter Nutzung der softmax Funktion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mm1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;31m#print('forward m1.shape:',m1.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import helper\n",
    "import os\n",
    "import random\n",
    "\n",
    "# show plot first run\n",
    "%matplotlib inline \n",
    "# test autocompletion with tab or tab+shift\n",
    "%config IPCompleter.greedy=True \n",
    "\n",
    "random.seed(30)\n",
    "\n",
    "class TwoLayerNeuralNetwork:\n",
    "    \"\"\"\n",
    "    Ein 2-Layer 'fully-connected' neural network, d.h. alle Neuronen sind mit allen anderen\n",
    "    verbunden. Die Anzahl der Eingabevektoren ist N mit einer Dimension D, einem 'Hidden'-Layer mit\n",
    "    H Neuronen. Es soll eine Klassifikation ber 10 Klassen (C) durchgefhrt werden.\n",
    "    Wir trainieren das Netzwerk mit einer 'Softmax'-Loss Funktion und einer L2 Regularisierung\n",
    "    auf den Gewichtungsmatrizen (W1 und W2). Das Netzwerk nutzt ReLU Aktivierungsfunktionen nach\n",
    "    dem ersten Layer.\n",
    "    Die Architektur des Netzwerkes lt sich abstrakt so darstellen:\n",
    "    Eingabe - 'fully connected'-Layer - ReLU - 'fully connected'-Layer - Softmax\n",
    "\n",
    "    Die Ausgabe aus dem 2.Layer sind die 'Scores' (Wahrscheinlichkeiten) fr jede Klasse.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, std=1e-4):\n",
    "        \"\"\"\n",
    "        Intitialisierung des Netzes - Die Gewichtungsmatrizen und die Bias-Vektoren werden mit\n",
    "        Zufallswerten initialisiert.\n",
    "        W1: 1.Layer Gewichte (D, H)\n",
    "        b1: 1.Layer Bias (H,)\n",
    "        W2: 2.Layer Gewichte (H, C)\n",
    "        b2: 2.Layer Bias (C,)\n",
    "\n",
    "        :param input_size: Die CIFAR-10 Bilder haben die Dimension D (32*32*3).\n",
    "        :param hidden_size: Anzahl der Neuronen im Hidden-Layer H.\n",
    "        :param output_size: Anzahl der Klassen C.\n",
    "        :param std: Skalierungsfaktoren fr die Initialisierung (muss klein sein)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.W1 = std * np.random.randn(input_size, hidden_size)\n",
    "        self.b1 = std * np.random.randn(1, hidden_size)\n",
    "        self.W2 = std * np.random.randn(hidden_size, output_size)\n",
    "        self.b2 = std * np.random.randn(1, output_size)\n",
    "        #print('W1 shape:', self.W1.shape)\n",
    "        #print('b1 shape:', self.b1.shape)\n",
    "        #print('W2 shape:', self.W2.shape)\n",
    "        #print('b2 shape:', self.b2.shape)\n",
    "\n",
    "    def softmax(self, z):\n",
    "        z -= np.max(z)\n",
    "        return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0.0, x)\n",
    "\n",
    "    def relu_derivative(self, output):\n",
    "        output[output <= 0] = 0\n",
    "        output[output>0] = 1\n",
    "        return output\n",
    "\n",
    "    def loss_deriv_softmax(self, activation, y_batch):\n",
    "        batch_size = y_batch.shape[0]\n",
    "        dCda2 = activation\n",
    "        dCda2[range(batch_size), y_batch] -= 1\n",
    "        dCda2 /= batch_size\n",
    "        return dCda2\n",
    "\n",
    "    def loss_crossentropy(self, activation, y_batch):\n",
    "        \"\"\"\n",
    "        Berechnet den loss (Fehler) des 2-Layer-Netzes\n",
    "\n",
    "        :param batch_size: Anzahl der Eingabebilder in einem Batch ber den der Fehler normalisiert werden muss\n",
    "        :param y: Vektor mit den Trainingslabels y[i] enthlt ein Label aus X[i] und jedes y[i] ist ein\n",
    "                  Integer zwischen 0 <= y[i] < C (Anzahl der Klassen)\n",
    "        :param reg: Regulasierungsstrke\n",
    "        :return: loss - normalisierter Fehler des Batches\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = y_batch.shape[0]\n",
    "        correct_logprobs = -np.log(activation[range(batch_size), y_batch])\n",
    "        loss = np.sum(correct_logprobs) / batch_size\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def forward(self, X, y):\n",
    "        \"\"\"\n",
    "        Fhrt den gesamten Forward Prozess durch und berechnet den Fehler (loss) und die Aktivierungen a1 und\n",
    "        a2 und gibt diese Werte zuruck\n",
    "        :param X: Trainings bzw. Testset\n",
    "        :param y: Labels des Trainings- bzw. Testsets\n",
    "        :return: loss, a1, a2\n",
    "        \"\"\"\n",
    "\n",
    "        # Berechen Sie den score\n",
    "        #N, D = X.shape\n",
    "        # TODO: Berechnen Sie den Forward-Schritt und geben Sie den Vektor mit Scores zurueck\n",
    "        # Nutzen Sie die ReLU Aktivierungsfunktion im ersten Layer\n",
    "        # Berechnen Sie die Klassenwahrscheinlichkeiten unter Nutzung der softmax Funktion\n",
    "        m1 = (X @ self.W1) + self.b1\n",
    "        #print('forward m1.shape:',m1.shape)\n",
    "        a1 = self.relu(m1)\n",
    "        \n",
    "        m2 = (a1 @ self.W2) + self.b2\n",
    "        #print('forward m2.shape:',m2.shape)\n",
    "        a2 = self.softmax(m2)\n",
    "        \n",
    "        # TODO: Berechnen Sie den Fehler mit der cross-entropy Funktion\n",
    "        loss = self.loss_crossentropy(a2, y)\n",
    "\n",
    "        return loss, a1, a2\n",
    "\n",
    "    def backward(self, a1, a2, X, y):\n",
    "        \"\"\"\n",
    "        Backward pass- dabei wird der Gradient der Gewichte W1, W2 und der Biases b1, b2 aus den Ausgaben des Netzes\n",
    "        berechnet und die Gradienten der einzelnen Layer als ein Dictionary zurckgegeben.\n",
    "        Zum Beispiel sollte grads['W1'] die Gradienten von self.W1 enthalten (das ist eine Matrix der gleichen Gre\n",
    "        wie self.W1.\n",
    "        :param a1: Aktivierung aus dem 1.Layer\n",
    "        :param a2: Aktivierung aus dem 2.Layer -> Output des Netzes\n",
    "        :param X:\n",
    "        :param y:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Nutzen Sie dabei die Notizen aus der Vorlesung und die gegebenen Ableitungsfunktionen\n",
    "        # please use the appropriate loss functions \n",
    "        # YOUR CODE HERE\n",
    "        # Backward pass: Berechnen Sie die Gradienten\n",
    "        N, D = X.shape\n",
    "        \n",
    "        # W1' = X.T * ( ( (2*(a2-yc)*relu(a2)) * W2.T ) * relu(a1) ) ## use relu(a2) or not??? no neg val can pass through\n",
    "        # b1' = 1 * ( ( (2*(a2-yc)*relu(a2)) * W2.T ) * relu(a1) )\n",
    "        # W2' = a1.T * 2*(a2 - yc)\n",
    "        # v2' = 1 * 2*(a2 - yc)\n",
    "\n",
    "        # dCostda2\n",
    "        #print('backw a2 shape:',a2.shape)\n",
    "        dCda2  = self.loss_deriv_softmax(a2,y)\n",
    "        #print('backw dCda2 shape:',dCda2.shape)\n",
    "        #dm2dW2 = a1 # only for W2\n",
    "        #dm2da1 = W2 # only for W1\n",
    "        da1dm1 = self.relu_derivative(a1) # only for W1\n",
    "        #print('backw da1dm1 shape:',da1dm1.shape)\n",
    "        #dm1db1 = 1 # ignore\n",
    "        #dm1dw1 = X_batch # just use X_batch\n",
    "\n",
    "        # dCdW2 = (W2') = dCda2 * da2dm2 * dm2dW2\n",
    "        # calc in order: dCdW2 = dm2dW2 * (dCda2 * da2dm2)  \n",
    "        # fit dimension: dCdW2 = dm2dW2.T * (dCda2 * da2dm2)\n",
    "\n",
    "        # b2' = dm1db1 * dCda2 * da2dm2  # first term is 1*\n",
    "        # b2' = dCda2 * da2dm2 # mean over axis=0\n",
    "\n",
    "        # dCdW1 = (W1') = dCda2 * da2dm2 * dm2da1 * da1dm1 * dm1dw1\n",
    "        # calc in order: dCdW1 = X_batch * [ ((dCda2 * da2dm2)*dm2da1) * da1dm1 ]\n",
    "        # fit dimension: dCdW1 = X_batch.T * [ ((dCda2 * da2dm2)*dm2da1.T) * da1dm1 ]\n",
    "\n",
    "        # replace: dCdW2 = a1.T * (dCda2 * da2dm2)\n",
    "        #    dCdb2 = b2' = mean_over_axis0( dCda2 * da2dm2 )\n",
    "        #          dCdW1 = X_batch.T * [ ((dCda2 * da2dm2)*W2.T) * relu_derivative(a1) ]\n",
    "        #    dCdb1 = b1' = mean_over_axis0( ((dCda2 * da2dm2)*W2.T) * relu_derivative(a1) )\n",
    "        \n",
    "        # implementation\n",
    "        #tmp1 = dCda2 \n",
    "        #tmp1 = torch.mul(dCda2,da2dm2) # *relu_deriv element wise mult\n",
    "        #print(dCda2.shape, self.W2.shape)\n",
    "        tmp2 = dCda2 @ self.W2.T\n",
    "        #print(tmp2.shape, da1dm1.shape)\n",
    "        \n",
    "        ### mult!!!\n",
    "        tmp3 = tmp2 * da1dm1 # *relu_deriv element wise mult\n",
    "        \n",
    "        #dCdW2 = a1.T @ dCda2       print('dCdW2 shape:',dCdW2.shape)Cdb2 = np.mean( dCda2, axis=0 )\n",
    "        dCdW1 = X.T @ tmp3\n",
    "        dCdb1 = np.mean( tmp3, axis=0 )\n",
    "\n",
    "        dCdW2 = a1.T @ dCda2\n",
    "        dCdb2 = np.mean( dCda2, axis=0 )\n",
    "\n",
    "        # function should return 4 derivatives with respect to\n",
    "        # W1, W2, b1, b2        \n",
    "        # Fllen Sie das Dictionary grads['W2'], grads['b2'], grads['W1'], grads['b1']\n",
    "        grads = {}\n",
    "        grads['W1'] = dCdW1\n",
    "        grads['b1'] = dCdb1\n",
    "        grads['W2'] = dCdW2\n",
    "        grads['b2'] = dCdb2\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def train(self, X, y, X_val, y_val,\n",
    "              learning_rate=1e-3, learning_rate_decay=0.95, num_iters=100,\n",
    "              batch_size=200, verbose=False):\n",
    "        \"\"\"\n",
    "        Training des Neuronalen Netzwerkes unter der Nutzung des iterativen\n",
    "        Optimierungsverfahrens Stochastic Gradient Descent\n",
    "        Train this neural network using stochastic gradient descent.\n",
    "\n",
    "        :param X: Numpy Array der Gre (N,D)\n",
    "        :param y: Numpy Array der Gre (N,) mit den jeweiligen Labels y[i] = c. Das bedeutet, dass X[i] das label c hat\n",
    "                  mit 0 <= c < C\n",
    "        :param X_val: Numpy Array der Gre (N_val,D) mit den Validierungs-/Testdaten\n",
    "        :param y_val: Numpy Array der Gre (N_val,) mit den Labels fr die Validierungs-/Testdaten\n",
    "        :param learning_rate: Faktor der Lernrate fr den Optimierungsprozess\n",
    "        :param learning_rate_decay: gibt an, inwieweit die Lernrate in jeder Epoche angepasst werden soll\n",
    "        :param reg: Strke der Regularisierung\n",
    "        :param num_iters: Anzahl der Iterationen der Optimierung\n",
    "        :param batch_size: Anzahl der Trainingseingabebilder, die in jedem forward-Schritt mitgegeben werden sollen\n",
    "        :param verbose: boolean, ob etwas ausgegeben werden soll\n",
    "        :return: dict (fuer die Auswertung) - enthlt Fehler und Genauigkeit der Klassifizierung fr jede Iteration bzw. Epoche\n",
    "        \"\"\"\n",
    "        num_train = X.shape[0]\n",
    "        iterations_per_epoch = int(max(num_train / batch_size, 1))\n",
    "\n",
    "        # Wir nutzen einen Stochastischen Gradient Decent (SGD) Optimierer um unsere\n",
    "        # Parameter W1,W2,b1,b2 zu optimieren\n",
    "        loss_history = []\n",
    "        loss_val_history = []\n",
    "        train_acc_history = []\n",
    "        val_acc_history = []\n",
    "\n",
    "        sample_propabilities = np.ones(X.shape[0])\n",
    "        for it in range(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "\n",
    "            ############################\n",
    "            # TODO: Erzeugen Sie einen zuflligen Batch der Gre batch_size\n",
    "            # aus den Trainingsdaten und speichern diese in X_batch und y_batch\n",
    "            # X_batch\n",
    "            # y_batch \n",
    "            idx = random.sample(range(0, num_train), batch_size)\n",
    "            X_batch = X[idx]\n",
    "            y_batch = y[idx]\n",
    "            \n",
    "            ############################\n",
    "\n",
    "            # TODO: Berechnung von loss und gradient fr den aktuellen Batch\n",
    "            #print('X_batch shape:',X_batch.shape)\n",
    "            #print('y_batch shape:',y_batch.shape)\n",
    "            loss, a1, a2 = self.forward(X_batch, y_batch)\n",
    "            # Merken des Fehlers fr den Plot\n",
    "            loss_history.append(loss)\n",
    "            \n",
    "            # Jetzt backward pass (Gradienten berechnen).\n",
    "            # backward pass \n",
    "            grads = self.backward(a1, a2, X_batch, y_batch)\n",
    "            \n",
    "            # Berechnung des Fehlers mit den aktuellen Parametern (W, b)\n",
    "            # mit dem Testset\n",
    "            loss_val, a1_val, a2_val = self.forward(X_val, y_val)\n",
    "            loss_val_history.append(loss_val)\n",
    "\n",
    "            ############################\n",
    "            # TODO: Nutzen Sie die Gradienten aus der Backward-Funktion und passen\n",
    "            # Sie die Parameter an (self.W1, self.b1 etc). Diese werden mit der Lernrate\n",
    "            # gewichtet\n",
    "\n",
    "            self.W1 -= learning_rate * grads['W1']\n",
    "            self.W2 -= learning_rate * grads['W2']\n",
    "            self.b1 -= learning_rate * grads['b1']\n",
    "            self.b2 -= learning_rate * grads['b2']\n",
    "            \n",
    "            ############################\n",
    "\n",
    "            # Ausgabe des aktuellen Fehlers. Diese sollte am Anfang erstmal nach unten gehen\n",
    "            # kann aber immer etwas schwanken.\n",
    "            if verbose and it % 100 == 0:\n",
    "                print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
    "\n",
    "            # Wir berprfen jede Epoche die Genauigkeit (von Trainingsset und Testset)\n",
    "            # und dmpfen die Lernrate\n",
    "            if it % iterations_per_epoch == 0:\n",
    "                # berprfung der Klassifikationsgenauigkeit\n",
    "                train_acc = (self.predict(X_batch) == y_batch).mean()\n",
    "                val_acc = (self.predict(X_val) == y_val).mean()\n",
    "                train_acc_history.append(train_acc)\n",
    "                val_acc_history.append(val_acc)\n",
    "                print('epoch done... acc', val_acc)\n",
    "\n",
    "                # Dmpfung der Lernrate\n",
    "                learning_rate *= learning_rate_decay\n",
    "\n",
    "        # Zum Plotten der Genauigkeiten geben wir die\n",
    "        # gesammelten Daten zurck\n",
    "        return {\n",
    "            'loss_history': loss_history,\n",
    "            'loss_val_history': loss_val_history,\n",
    "            'train_acc_history': train_acc_history,\n",
    "            'val_acc_history': val_acc_history,\n",
    "        }\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Benutzen Sie die trainierten Gewichte des 2-Layer-Netzes um die Klassen fr das\n",
    "        Validierungsset vorherzusagen. Dafr mssen Sie fr das/die Eingabebilder X nur\n",
    "        die Scores berechnen. Der hchste Score ist die vorhergesagte Klasse. Dieser wird in y_pred\n",
    "        geschrieben und zurckgegeben.\n",
    "\n",
    "        :param X: Numpy Array der Gre (N,D)\n",
    "        :return: y_pred Numpy Array der Gre (N,) die die jeweiligen Labels fr alle Elemente in X enthaelt.\n",
    "        y_pred[i] = c bedeutet, das fuer X[i] die Klasse c mit 0<=c<C vorhergesagt wurde\n",
    "        \"\"\"\n",
    "        #y_pred = None\n",
    "        m1 = (X @ self.W1) + self.b1\n",
    "        #print('forward m1.shape:',m1.shape)\n",
    "        a1 = self.relu(m1)\n",
    "        \n",
    "        a2 = (a1 @ self.W2) + self.b2\n",
    "        #print('forward m2.shape:',m2.shape)\n",
    "        #a2 = self.softmax(m2)\n",
    "        \n",
    "        ############################\n",
    "        # TODO: Implementieren Sie die Vorhersage. D.h. fr ein/mehrere Bild/er mit den gelernten\n",
    "        # Parametern den Wahrscheinlichkeit berechnen.\n",
    "        # np.argmax in dem Wahrscheinlichkeitsvektor ist die wahrscheinlichste Klasse\n",
    "        ############################\n",
    "        # Implementieren Sie nochmals den Forward pass um die Wahrscheinlichkeiten\n",
    "        # vorherzusagen\n",
    "        y_pred = np.argmax(a2, axis=1)\n",
    "        #print('y_pred:',y_pred)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    X_train, y_train, X_val, y_val = helper.prepare_CIFAR10_images()\n",
    "    # TODO: Laden der Bilder. Hinweis - wir nutzen nur die Trainigsbilder zum Trainieren und die\n",
    "    # Validierungsbilder zum Testen.\n",
    "    print('Train data shape: ', X_train.shape)\n",
    "    print('Train labels shape: ', y_train.shape)\n",
    "    print('Validation data shape: ', X_val.shape)\n",
    "    print('Validation labels shape: ', y_val.shape)\n",
    "\n",
    "    # Grsse der Bilder\n",
    "    input_size = 32 * 32 * 3\n",
    "    # Anzahl der Klassen\n",
    "    num_classes = 10\n",
    "\n",
    "    #############################################\n",
    "    # Hyperparameter\n",
    "    #############################################\n",
    "\n",
    "    # TODO: mit diesen Parametern sollten Sie in etwa auf eine\n",
    "    # Klassifikationsgenauigkeit von 43% kommen. Optimieren Sie die\n",
    "    # Hyperparameter um die Genauigkeit zu erhhen (bitte tun sie das\n",
    "    # systematisch und nicht einfach durch probieren - also z.B. in einem\n",
    "    # for-loop eine Reihe von Parametern testen und die Einzelbilder abspeichern)\n",
    "    \n",
    "    hidden_size = 240 #50   # Anzahl der Neuronen im Hidden Layer\n",
    "    num_iter = 14000 #3000  # Anzahl der Optimierungsiterationen\n",
    "    batch_size = 100  # Eingabeanzahl der Bilder\n",
    "    learning_rate = 0.001  # Lernrate\n",
    "    learning_rate_decay = 0.98  # Lernratenabschwchung\n",
    "    \n",
    "    ##\n",
    "    #opt_mult = np.array([5,400,25,0.0003,0.01])\n",
    "    scale = 0.1\n",
    "    \n",
    "    store_best0 = hidden_size \n",
    "    store_best1 = num_iter\n",
    "    #store_best2 = batch_size\n",
    "    #store_best3 = learning_rate\n",
    "    #store_best4 = learning_rate_decay\n",
    "    \n",
    "    # score\n",
    "    acc = 0.0\n",
    "    acc_tmp = 0.0\n",
    "    \n",
    "    sign = 1\n",
    "    step = 0 # 0,1,2\n",
    "    opt_param = 0 # param edited at moment\n",
    "    \n",
    "    for i in range(20000):\n",
    "        if step == 2:\n",
    "            opt_param += 1\n",
    "            opt_param %=2\n",
    "            step = 0\n",
    "        \n",
    "        if step == 0:\n",
    "            scale = np.abs(scale)\n",
    "        if step == 1:\n",
    "            scale = -1*np.abs(scale)\n",
    "            \n",
    "        # set new params\n",
    "        if opt_param == 0:\n",
    "            # update hidden_size\n",
    "            #opt_param = hidden_size\n",
    "            hidden_size += int(scale*hidden_size)\n",
    "            if hidden_size < 10:\n",
    "                hidden_size = 10\n",
    "        elif opt_param == 1:    \n",
    "            # update num_iter\n",
    "            #opt_param = num_iter\n",
    "            num_iter += int(scale*num_iter)\n",
    "            if num_iter < 500:\n",
    "                num_iter = 500\n",
    "        \n",
    "        \n",
    "       # print(input_size, hidden_size,num_classes)\n",
    "        net = TwoLayerNeuralNetwork(input_size, hidden_size, num_classes)\n",
    "\n",
    "        # Train the network\n",
    "        stats = net.train(X_train, y_train, X_val, y_val,\n",
    "                            learning_rate=learning_rate, learning_rate_decay=learning_rate_decay, \n",
    "                          num_iters=num_iter, batch_size=batch_size, verbose=True)\n",
    "\n",
    "        print('Final training loss: ', stats['loss_history'][-1])\n",
    "        print('Final validation loss: ', stats['loss_val_history'][-1])\n",
    "\n",
    "        print('Final validation accuracy: ', stats['val_acc_history'][-1])\n",
    "        \n",
    "        acc = stats['val_acc_history'][-1]\n",
    "        \n",
    "        if acc > acc_tmp:\n",
    "            if i == 0:\n",
    "                acc_tmp = acc\n",
    "        \n",
    "            if acc > acc_tmp+0.01:\n",
    "                # increase only very small amount\n",
    "                acc_tmp+=0.01\n",
    "                acc = acc_tmp\n",
    "            else:\n",
    "                acc_tmp = acc\n",
    "            \n",
    "            # print only if improvement!\n",
    "            #helper.plot_net_weights(net)\n",
    "            name = 'nr_'+str(i)+'_acc_hsz_'+str(hidden_size)+'_numi_'+str(num_iter)+'_bat_'+str(batch_size)+'_lrate_'+str(learning_rate)+'_ldec_'+str(learning_rate_decay)+'_acc_'+str(acc)+'.png'\n",
    "            helper.plot_accuracy(stats,name)\n",
    "            name = 'nr_'+str(i)+'_loss_hsz_'+str(hidden_size)+'_numi_'+str(num_iter)+'_bat_'+str(batch_size)+'_lrate_'+str(learning_rate)+'_ldec_'+str(learning_rate_decay)+'_acc_'+str(acc)+'.png'\n",
    "            helper.plot_loss(stats,name)\n",
    "            \n",
    "            # keep value and continue\n",
    "            if opt_param == 0:\n",
    "                # update hidden_size\n",
    "                store_best0 = hidden_size\n",
    "            elif opt_param == 1:    \n",
    "                # update num_iter\n",
    "                #opt_param = num_iter\n",
    "                store_best1 = num_iter\n",
    "        else:\n",
    "            step +=1\n",
    "            if opt_param == 0:\n",
    "                # reset value\n",
    "                hidden_size = int(store_best0)\n",
    "            elif opt_param == 1:\n",
    "                num_iter = int(store_best1)\n",
    "\n",
    "        print('i step opt_param: hidden_size num_iter batch_size learning_rate learning_rate_decay acc')\n",
    "        print(i,step, opt_param,hidden_size, num_iter, batch_size,  learning_rate, learning_rate_decay, acc)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
