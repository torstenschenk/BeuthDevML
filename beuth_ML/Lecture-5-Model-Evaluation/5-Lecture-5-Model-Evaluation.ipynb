{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning \n",
    "\n",
    "Classification Metrics and Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning Pipelines\n",
    "\n",
    "![ml-pipeline-2.png](figures/ml-pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model Evaluation\n",
    "    \n",
    "- Machine Learning models are evaluated by comparing the predictions $f(x)=\\hat{y}$ and the target values $y$\n",
    "\n",
    "\n",
    "- Useful Metrics\n",
    "    - Regression (for continuous predictions - covered later)\n",
    "    - Classification\n",
    "    \n",
    "\n",
    "- As ML models can memorize any training data set, **all metrics must always be computed using cross-validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification Metrics\n",
    "\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Accuracy\n",
    "\n",
    "Defined as \n",
    "\n",
    "$\\frac{\\text{number of correct assignments}}{\\text{number of data points}}$\n",
    "\n",
    "Problem: Imbalanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_predicted = np.array([1, 0, 0, 0, 0])\n",
    "y_true = np.array([0, 1, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy_score(y_true, y_predicted)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Precision\n",
    "\n",
    "$${\\text{precision}}={\\frac {|\\{{\\text{relevant instances}}\\}\\cap \\{{\\text{predicted instances}}\\}|}{|\\{{\\text{predicted instances}}\\}|}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "y_predicted = np.array([1, 0, 0, 0, 0])\n",
    "y_true = np.array([0, 1, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6\n",
      "Precision: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy_score(y_true, y_predicted)}\")\n",
    "print(f\"Precision: {precision_score(y_true, y_predicted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_predicted = np.array([1, 1, 0, 0, 0])\n",
    "y_true = np.array([0, 1, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8\n",
      "Precision: 0.5\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy_score(y_true, y_predicted)}\")\n",
    "print(f\"Precision: {precision_score(y_true, y_predicted)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recall\n",
    "\n",
    "\n",
    "$${\\text{recall}}={\\frac {|\\{{\\text{relevant instances}}\\}\\cap \\{{\\text{predicted instances}}\\}|}{|\\{{\\text{relevant instances}}\\}|}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "y_predicted = np.array([1, 0, 0, 0, 0])\n",
    "y_true = np.array([0, 1, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy_score(y_true, y_predicted)}\")\n",
    "print(f\"Precision: {precision_score(y_true, y_predicted)}\")\n",
    "print(f\"Recall: {recall_score(y_true, y_predicted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_predicted = np.array([1, 1, 0, 0, 0])\n",
    "y_true = np.array([0, 1, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8\n",
      "Precision: 0.5\n",
      "Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy_score(y_true, y_predicted)}\")\n",
    "print(f\"Precision: {precision_score(y_true, y_predicted)}\")\n",
    "print(f\"Recall: {recall_score(y_true, y_predicted)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Precision and Recall\n",
    "<center>\n",
    "<img src=\"figures/Precisionrecall.png\" width=300px>\n",
    "</center>\n",
    "\n",
    "Source Wikipedia page on [Precision and Recall](https://en.wikipedia.org/wiki/Precision_and_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## F1 Score\n",
    "\n",
    "$$F=2\\cdot {\\frac {\\mathrm {precision} \\cdot \\mathrm {recall} }{\\mathrm {precision} +\\mathrm {recall} }}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_predicted = np.array([1, 0, 0, 0, 0])\n",
    "y_true = np.array([0, 1, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Precision: {precision_score(y_true, y_predicted)}\")\n",
    "print(f\"Recall: {recall_score(y_true, y_predicted)}\")\n",
    "print(f\"F1: {f1_score(y_true, y_predicted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_predicted = np.array([1, 1, 0, 0, 0])\n",
    "y_true = np.array([0, 1, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5\n",
      "Recall: 1.0\n",
      "F1: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "print(f\"Precision: {precision_score(y_true, y_predicted)}\")\n",
    "print(f\"Recall: {recall_score(y_true, y_predicted)}\")\n",
    "print(f\"F1: {f1_score(y_true, y_predicted)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model Evaluation\n",
    "\n",
    "![model_complexity](figures/overfitting_underfitting_cartoon.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-Validation\n",
    "\n",
    "- ML models can memorize any data set (**overfitting**)\n",
    "    \n",
    "- But we want our models to perform well on new data (**generalization of learned rules**)\n",
    "\n",
    "- Cross-validation emulates the setting of new unseen data:\n",
    "\n",
    "    - Split data in training and test\n",
    "    - Train model on training set\n",
    "    - Test model on test set\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simplest Cross-Validation for Model Evaluation\n",
    "\n",
    "Split data into **two folds** and train / test **just once**\n",
    "\n",
    "|$~$ |$~$|\n",
    "|--|--|\n",
    "|Fold 1|Test|\n",
    "|Fold 2|Train|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 3-Fold Cross-Validation for Model Evaluation:\n",
    "\n",
    "Split data into three folds:\n",
    "\n",
    "|$~$ |\n",
    "|--|\n",
    "|Fold 1|\n",
    "|Fold 2|\n",
    "|Fold 3|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fold 1 is Test Data\n",
    "\n",
    "|$~$ |$~$|\n",
    "|--|--|\n",
    "|Fold 1|Test|\n",
    "|Fold 2|Train|\n",
    "|Fold 3|Train|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fold 2 is Test Data\n",
    "\n",
    "|$~$ |$~$|\n",
    "|--|--|\n",
    "|Fold 1|Train|\n",
    "|Fold 2|Test|\n",
    "|Fold 3|Train|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fold 3 is Test Data\n",
    "\n",
    "|$~$ |$~$|\n",
    "|--|--|\n",
    "|Fold 1|Train|\n",
    "|Fold 2|Train|\n",
    "|Fold 3|Test|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap: Cross-Validation for Model Evaluation\n",
    "\n",
    "- Split dataset in training and test set\n",
    "- Requires **at least two** different folds / partitions / data sets (train and test)\n",
    "- Fast Cross-validation:\n",
    "    - Make two partitions, train and test\n",
    "- Cross-validation with best generalization performance estimates:\n",
    "    - k-fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example Application: Classifying Parliament Speeches\n",
    "\n",
    "- Task: Political bias prediction from text data\n",
    "- Data: [parliament speeches in German Bundestag](https://github.com/Datenschule/plpr-scraper)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning Pipeline\n",
    "\n",
    "![ml-pipeline-2.png](figures/ml-pipeline-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import os, gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATADIR = \"data\"\n",
    "\n",
    "if not os.path.exists(DATADIR): \n",
    "    os.mkdir(DATADIR)\n",
    "\n",
    "file_name = os.path.join(DATADIR, 'bundestags_parlamentsprotokolle.csv.gzip')\n",
    "if not os.path.exists(file_name):\n",
    "    url_data = 'https://www.dropbox.com/s/1nlbfehnrwwa2zj/bundestags_parlamentsprotokolle.csv.gzip?dl=1'\n",
    "    urllib.request.urlretrieve(url_data, file_name)\n",
    "\n",
    "df = pd.read_csv(gzip.open(file_name), index_col=0).sample(frac=1)\n",
    "\n",
    "alle_sprecher = df.sprecher.unique()\n",
    "parteien = df.partei.unique()\n",
    "partei_farben = {'cducsu':'black', 'linke':'purple', 'spd':'red', 'gruene':'green', 'fdp':'yellow'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sitzung</th>\n",
       "      <th>wahlperiode</th>\n",
       "      <th>sprecher</th>\n",
       "      <th>text</th>\n",
       "      <th>partei</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30289</th>\n",
       "      <td>76</td>\n",
       "      <td>18</td>\n",
       "      <td>Dr. Konstantin von Notz</td>\n",
       "      <td>Ich kann Ihnen schon sagen, wie das Ihre Unabh...</td>\n",
       "      <td>gruene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11201</th>\n",
       "      <td>133</td>\n",
       "      <td>17</td>\n",
       "      <td>Peter Wichtel</td>\n",
       "      <td>Denn damit haben Sie am Ende genau das, was hi...</td>\n",
       "      <td>cducsu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24223</th>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>Dr. Eva Högl</td>\n",
       "      <td>Unser Koalitionsvertrag ist voller guter Ideen...</td>\n",
       "      <td>spd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34597</th>\n",
       "      <td>127</td>\n",
       "      <td>18</td>\n",
       "      <td>Dr. Joachim Pfeiffer</td>\n",
       "      <td>Frau Präsidentin! Liebe Kolleginnen und Kolleg...</td>\n",
       "      <td>cducsu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7715</th>\n",
       "      <td>96</td>\n",
       "      <td>17</td>\n",
       "      <td>Bettina Kudla</td>\n",
       "      <td>Zur Gläubigerbeteiligung - auch dieses Thema w...</td>\n",
       "      <td>cducsu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sitzung  wahlperiode                 sprecher  \\\n",
       "30289       76           18  Dr. Konstantin von Notz   \n",
       "11201      133           17            Peter Wichtel   \n",
       "24223       10           18             Dr. Eva Högl   \n",
       "34597      127           18     Dr. Joachim Pfeiffer   \n",
       "7715        96           17            Bettina Kudla   \n",
       "\n",
       "                                                    text  partei  \n",
       "30289  Ich kann Ihnen schon sagen, wie das Ihre Unabh...  gruene  \n",
       "11201  Denn damit haben Sie am Ende genau das, was hi...  cducsu  \n",
       "24223  Unser Koalitionsvertrag ist voller guter Ideen...     spd  \n",
       "34597  Frau Präsidentin! Liebe Kolleginnen und Kolleg...  cducsu  \n",
       "7715   Zur Gläubigerbeteiligung - auch dieses Thema w...  cducsu  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Put some data aside for model evaluation\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(df['text'], df['partei'], test_size=0.2)\n",
    "\n",
    "ncc_clf = Pipeline([('vect', TfidfVectorizer(max_features=int(1e8))),\n",
    "                            ('clf', NearestCentroid())]).fit(train_data, train_labels)\n",
    "\n",
    "logreg_clf = Pipeline([('vect', TfidfVectorizer(max_features=int(1e8))),\n",
    "                        ('clf', SGDClassifier())]).fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluating NCC on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      cducsu       0.49      0.65      0.56      9612\n",
      "         fdp       0.42      0.21      0.29      5275\n",
      "      gruene       0.43      0.34      0.38      6323\n",
      "       linke       0.62      0.37      0.46      8222\n",
      "         spd       0.28      0.48      0.35      5511\n",
      "\n",
      "   micro avg       0.44      0.44      0.44     34943\n",
      "   macro avg       0.45      0.41      0.41     34943\n",
      "weighted avg       0.47      0.44      0.43     34943\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ncc_predictions = ncc_clf.predict(train_data)\n",
    "print(classification_report(ncc_predictions, train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluating NCC on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      cducsu       0.48      0.61      0.54      2498\n",
      "         fdp       0.35      0.18      0.23      1365\n",
      "      gruene       0.40      0.32      0.35      1569\n",
      "       linke       0.54      0.34      0.42      1894\n",
      "         spd       0.26      0.45      0.33      1410\n",
      "\n",
      "   micro avg       0.41      0.41      0.41      8736\n",
      "   macro avg       0.41      0.38      0.38      8736\n",
      "weighted avg       0.42      0.41      0.40      8736\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ncc_predictions_test = ncc_clf.predict(test_data)\n",
    "print(classification_report(ncc_predictions_test, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluating Logistic Regression on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      cducsu       0.96      0.66      0.78     18852\n",
      "         fdp       0.27      0.98      0.42       738\n",
      "      gruene       0.69      0.93      0.79      3708\n",
      "       linke       0.85      0.87      0.86      4768\n",
      "         spd       0.62      0.85      0.72      6877\n",
      "\n",
      "   micro avg       0.76      0.76      0.76     34943\n",
      "   macro avg       0.68      0.86      0.71     34943\n",
      "weighted avg       0.84      0.76      0.77     34943\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg_predictions = logreg_clf.predict(train_data)\n",
    "print(classification_report(logreg_predictions, train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluating Logistic Regression on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      cducsu       0.91      0.55      0.69      5273\n",
      "         fdp       0.04      0.62      0.08        50\n",
      "      gruene       0.33      0.62      0.43       658\n",
      "       linke       0.63      0.64      0.63      1177\n",
      "         spd       0.37      0.57      0.45      1578\n",
      "\n",
      "   micro avg       0.57      0.57      0.57      8736\n",
      "   macro avg       0.46      0.60      0.46      8736\n",
      "weighted avg       0.72      0.57      0.61      8736\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg_predictions_test = logreg_clf.predict(test_data)\n",
    "print(classification_report(logreg_predictions_test, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary Comparison NCC and Logistic Regression\n",
    "\n",
    "Nearest Centroid Classifiers\n",
    "- NCC is a simple model\n",
    "- Overall performance not great\n",
    "- But not a lot of overfitting\n",
    "\n",
    "Logistic Regression\n",
    "- More powerful model\n",
    "- Better prediction performance\n",
    "- More overfitting"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
